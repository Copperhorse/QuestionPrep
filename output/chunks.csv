chunk_id,file_id,chunk_index,content,section_header,section_level,parent_section,content_type,estimated_tokens,prev_chunk_id,next_chunk_id,created_at,quality_score,eval_content_type,eval_reason,should_use
bf657377-5257-425f-a2b1-6602e680bb5d,5fd22b0a-4f4d-46ec-97d2-0107a8747661,0,"Publish AI, ML &amp; data-science insights to a global community of data professionals.

LATEST EDITOR'S PICKS DEEP DIVES NEWSLETTER WRITE FOR TDS

DATA SCIENCE",,0,,prose,41,,57e5ea11-636c-4866-bcac-dcefa8014d57,2025-12-07T21:51:50.102983,,,,True
57e5ea11-636c-4866-bcac-dcefa8014d57,5fd22b0a-4f4d-46ec-97d2-0107a8747661,1,"## Building a Random Forest by Hand in Python

A deep dive on a powerful and popular algorithm

Matt Sosna 20 min read Jan 30, 2024

Photo by FlyD on Unsplash

<!-- image -->

<!-- image -->",Building a Random Forest by Hand in Python,2,,prose,50,bf657377-5257-425f-a2b1-6602e680bb5d,bfcdc2a2-1397-4215-945a-4cf5bbab85d2,2025-12-07T21:51:50.102992,,,,True
bfcdc2a2-1397-4215-945a-4cf5bbab85d2,5fd22b0a-4f4d-46ec-97d2-0107a8747661,2,"Submit an Article

<!-- image -->

<!-- image -->

From drug discovery to species classi fi cation, credit scoring to cybersecurity and more, the random forest is a popular and powerful algorithm for modeling our complex world. Its versatility and predictive prowess would seem to require cutting-edge complexity, but if we dig into what a random forest actually is, we see a shockingly simple set of repeating steps.",Building a Random Forest by Hand in Python,2,,prose,79,57e5ea11-636c-4866-bcac-dcefa8014d57,c16f13fd-69ab-4661-8f89-1e2d8c3a3e97,2025-12-07T21:51:50.102996,,,,True
c16f13fd-69ab-4661-8f89-1e2d8c3a3e97,5fd22b0a-4f4d-46ec-97d2-0107a8747661,3,"I fi nd that the best way to learn something is to play with it. So to gain an intuition on how random forests work, let's build one by hand in Python, starting with a decision tree and expanding to the full forest. We'll see fi rst-hand how fl exible and interpretable this algorithm is for both classi fi cation and regression. And while this project may sound complicated, there are really only a few core concepts we'll need to learn: 1) how to iteratively partition data, and 2) how to quantify how well data is partitioned.",Building a Random Forest by Hand in Python,2,,prose,120,bfcdc2a2-1397-4215-945a-4cf5bbab85d2,ec700c3b-a459-4f1c-b13f-b1b2b1e1b6fa,2025-12-07T21:51:50.103000,,,,True
ec700c3b-a459-4f1c-b13f-b1b2b1e1b6fa,5fd22b0a-4f4d-46ec-97d2-0107a8747661,4,## Background,Background,2,,prose,2,c16f13fd-69ab-4661-8f89-1e2d8c3a3e97,117e1c26-9945-4c70-b38c-bd85206ab609,2025-12-07T21:51:50.103004,,,,True
117e1c26-9945-4c70-b38c-bd85206ab609,5fd22b0a-4f4d-46ec-97d2-0107a8747661,5,"## Decision tree inference

A decision tree is a supervised learning algorithm that identi fi es a branching set of binary rules that map features to labels in a dataset. Unlike algorithms like logistic regression where the output is an equation, the decision tree algorithm is nonparametric, meaning it doesn't make strong assumptions on the relationship between features and labels. This means that decision trees are free to grow in whatever way best partitions their training data, so the resulting structure will vary between datasets.",Decision tree inference,2,,prose,96,ec700c3b-a459-4f1c-b13f-b1b2b1e1b6fa,7e1d2598-1704-41e6-b5f7-4e51494649f2,2025-12-07T21:51:50.103008,,,,True
7e1d2598-1704-41e6-b5f7-4e51494649f2,5fd22b0a-4f4d-46ec-97d2-0107a8747661,6,"One major bene fi t of decision trees is their explainability: each step the tree takes in deciding how to predict a category (for classi fi cation) or continuous value (for regression) can be seen in the tree nodes. A model that predicts whether a shopper will buy a product they viewed online, for example, might look like this.",Decision tree inference,2,,prose,71,117e1c26-9945-4c70-b38c-bd85206ab609,4c988455-b468-406f-9eeb-6c1d861b431e,2025-12-07T21:51:50.103011,,,,True
4c988455-b468-406f-9eeb-6c1d861b431e,5fd22b0a-4f4d-46ec-97d2-0107a8747661,7,"<!-- image -->

Starting with the root , each node in the tree asks a binary question (e.g., ""Was the session length longer than 5 minutes?"" ) and passes the feature vector to one of two child nodes depending on the answer. If there are no children - i.e., we're at a leaf node - then the tree returns a response.",Decision tree inference,2,,prose,72,7e1d2598-1704-41e6-b5f7-4e51494649f2,faef9a27-1a13-4cb9-98e3-e0273ae6e2d8,2025-12-07T21:51:50.103020,,,,True
faef9a27-1a13-4cb9-98e3-e0273ae6e2d8,5fd22b0a-4f4d-46ec-97d2-0107a8747661,8,"<!-- image -->

(We'll focus on classi fi cation in this blog post, but a decision tree regressor would look identical but with continuous values returned rather than class labels.)",Decision tree inference,2,,prose,37,4c988455-b468-406f-9eeb-6c1d861b431e,84e63793-14ce-42eb-a3fa-c016e43302bb,2025-12-07T21:51:50.103024,,,,True
84e63793-14ce-42eb-a3fa-c016e43302bb,5fd22b0a-4f4d-46ec-97d2-0107a8747661,9,"## Decision tree training

Inference , or this prediction process, is pretty straightforward. But building this tree is much less obvious. How is the binary rule in each node determined? Which features are used in the tree, and in what order? Where does a threshold like 0.5 or 1 come from?",Decision tree training,2,,prose,64,faef9a27-1a13-4cb9-98e3-e0273ae6e2d8,08328d2e-8c54-4f77-8494-c165b747abd6,2025-12-07T21:51:50.103028,,,,True
08328d2e-8c54-4f77-8494-c165b747abd6,5fd22b0a-4f4d-46ec-97d2-0107a8747661,10,"To understand how decision trees are built, let's imagine we're trying to partition a large dataset of shapes (squares and triangles) into smaller datasets of only squares or only triangles based on their features. In the ideal case, there's some categorical feature that perfectly separates the shapes.",Decision tree training,2,,prose,57,84e63793-14ce-42eb-a3fa-c016e43302bb,2ae6d2b2-54c6-4e06-8f13-68f6fe4800d0,2025-12-07T21:51:50.103032,,,,True
2ae6d2b2-54c6-4e06-8f13-68f6fe4800d0,5fd22b0a-4f4d-46ec-97d2-0107a8747661,11,"<!-- image -->

But it's never that easy. If you're lucky, maybe there's a continuous feature that has some threshold that perfectly separates the shapes instead. It takes a couple tries to fi nd the exact threshold, but then you have your perfect split. (Phew!)",Decision tree training,2,,prose,56,08328d2e-8c54-4f77-8494-c165b747abd6,37a38698-a166-41ee-9dba-77096c60b0b8,2025-12-07T21:51:50.103034,,,,True
37a38698-a166-41ee-9dba-77096c60b0b8,5fd22b0a-4f4d-46ec-97d2-0107a8747661,12,"<!-- image -->

Well… it's never really that easy, either. In this toy example, all triangles and squares are identical, meaning it's trivial to separate their feature vectors. (Find one rule that works for one triangle and it works for all triangles!)",Decision tree training,2,,prose,52,2ae6d2b2-54c6-4e06-8f13-68f6fe4800d0,0e63f3bd-6fb7-4ce5-bfb8-8c944e43517c,2025-12-07T21:51:50.103037,,,,True
0e63f3bd-6fb7-4ce5-bfb8-8c944e43517c,5fd22b0a-4f4d-46ec-97d2-0107a8747661,13,"## But in the real world, features don't map so neatly to labels.

Going back to our e-commerce example, a feature like time spent on the site in a session might not be able to perfectly partition the classes at any threshold.

<!-- image -->","But in the real world, features don't map so neatly to labels.",2,,prose,51,37a38698-a166-41ee-9dba-77096c60b0b8,a9e245a6-aeb2-431a-85af-a4356b67168e,2025-12-07T21:51:50.103041,,,,True
a9e245a6-aeb2-431a-85af-a4356b67168e,5fd22b0a-4f4d-46ec-97d2-0107a8747661,14,"So what do we do if no feature at any threshold can perfectly split our data? In this case, we need a way to quantify how ""mixed"" a set of labels is. One common metric is Gini impurity , which is calculated with the following equation:","But in the real world, features don't map so neatly to labels.",2,,prose,54,0e63f3bd-6fb7-4ce5-bfb8-8c944e43517c,9a36bfde-2232-4a97-8257-43b0384435c4,2025-12-07T21:51:50.103044,,,,True
9a36bfde-2232-4a97-8257-43b0384435c4,5fd22b0a-4f4d-46ec-97d2-0107a8747661,15,"<!-- image -->

k=1

Here, pk is the probability of a randomly-drawn sample belonging to class k among our m classes. For our squares and triangles, since we only have two classes and the probabilities must sum to 1, we can just de fi ne the whole equation in terms of pk .","But in the real world, features don't map so neatly to labels.",2,,prose,63,a9e245a6-aeb2-431a-85af-a4356b67168e,2bd54704-64e3-4c84-b2a1-37ae6daff0de,2025-12-07T21:51:50.103047,,,,True
2bd54704-64e3-4c84-b2a1-37ae6daff0de,5fd22b0a-4f4d-46ec-97d2-0107a8747661,16,"$$G = 1 - p _ { k } { ^ { 2 } } - ( 1 - p _ { k } ) ^ { 2 }$$

Below is a visual representation of the Gini impurity as a function of p ✓ , the probability of randomly selecting a positive label from the set. [1] (We've just replaced pk with p ✓ to indicate that the checkmarks are the positive class.) The lowest impurity is when the elements in the set are either all not checkmarks (i.e., x's) or all checkmarks. The impurity peaks when we have equal numbers of x's and checkmarks.","But in the real world, features don't map so neatly to labels.",2,,prose,134,9a36bfde-2232-4a97-8257-43b0384435c4,e61aac82-efa6-422d-bc74-98aa0b876be4,2025-12-07T21:51:50.103050,,,,True
e61aac82-efa6-422d-bc74-98aa0b876be4,5fd22b0a-4f4d-46ec-97d2-0107a8747661,17,"Image adapted from Data Science for Business: What You Need to Know about Data Mining and Data-Analytic Thinking

<!-- image -->

When identifying rules to partition our classes, then, we can simply select a split such that we minimize the weighted Gini impurity of the subsets. (Each subset has its own impurity, so we take the average weighted by the number of samples in each subset.) For a given feature, we can split the data on all possible values of that feature, record the weighted Gini impurity of the subsets, and then select the feature value that resulted in the lowest impurity.","But in the real world, features don't map so neatly to labels.",2,,prose,123,2bd54704-64e3-4c84-b2a1-37ae6daff0de,e32e148e-dae8-4ceb-963f-fcd9eecb9ddd,2025-12-07T21:51:50.103053,,,,True
e32e148e-dae8-4ceb-963f-fcd9eecb9ddd,5fd22b0a-4f4d-46ec-97d2-0107a8747661,18,"Below, splitting the feature Age of account on 35 days best separates users who buy a product from those who don't (in an imaginary dataset).

Image by author

<!-- image -->

We can repeat this process for all features and select the feature whose optimal split resulted in the lowest impurity. Below, we see that the optimal split for Session length results in a lower Gini impurity than the best splits for Age of account and Is frequent shopper . Is frequent shopper is a binary feature, so there's only one value to split with.","But in the real world, features don't map so neatly to labels.",2,,prose,109,e61aac82-efa6-422d-bc74-98aa0b876be4,c55267cd-187c-49c2-bebb-5479636d56db,2025-12-07T21:51:50.103056,,,,True
c55267cd-187c-49c2-bebb-5479636d56db,5fd22b0a-4f4d-46ec-97d2-0107a8747661,19,"Image by author

<!-- image -->","But in the real world, features don't map so neatly to labels.",2,,prose,7,e32e148e-dae8-4ceb-963f-fcd9eecb9ddd,54ad0dbf-e7ad-491c-9320-4da13478afc5,2025-12-07T21:51:50.103059,,,,True
54ad0dbf-e7ad-491c-9320-4da13478afc5,5fd22b0a-4f4d-46ec-97d2-0107a8747661,20,"## Splitting on Session length &gt; 5 min therefore becomes the fi rst

fork in our decision tree. We then repeat our process of iterating through features and values and choosing the feature that best partitions the data for each subset, then their subsets, and so on until we either have perfectly partitioned data or our tree reaches a maximum allowed depth. (More on that in the next section.)",Splitting on Session length &gt; 5 min therefore becomes the fi rst,2,,prose,81,c55267cd-187c-49c2-bebb-5479636d56db,0bace98a-9063-4409-be6d-f6d6cebf7aad,2025-12-07T21:51:50.103062,,,,True
0bace98a-9063-4409-be6d-f6d6cebf7aad,5fd22b0a-4f4d-46ec-97d2-0107a8747661,21,"Below is the tree we saw earlier but with the training data displayed in each node. Notice how the positive and negative classes become progressively separated as we move down the tree. Once we reach the bottom of the tree, the leaf nodes output the majority class - the only class, in our case - in their data subset.",Splitting on Session length &gt; 5 min therefore becomes the fi rst,2,,prose,64,54ad0dbf-e7ad-491c-9320-4da13478afc5,91a82013-064e-4179-8c10-860943d32912,2025-12-07T21:51:50.103065,,,,True
91a82013-064e-4179-8c10-860943d32912,5fd22b0a-4f4d-46ec-97d2-0107a8747661,22,"<!-- image -->

<!-- image -->",Splitting on Session length &gt; 5 min therefore becomes the fi rst,2,,prose,6,0bace98a-9063-4409-be6d-f6d6cebf7aad,f123ef9a-4ae2-4e36-a5a0-d5014b158923,2025-12-07T21:51:50.103068,,,,True
f123ef9a-4ae2-4e36-a5a0-d5014b158923,5fd22b0a-4f4d-46ec-97d2-0107a8747661,23,"## Random forests

The decision tree above partitions the data until the subsets contain only labels of one class (i.e., Gini impurity = 0). While this maximizes our model's ability to explain its training data, we risk over fi tting our model to our data. Think of it like the model memorizing every feature-label combination rather than learning the underlying patterns . An over fi t model struggles to generalize to new data, which is usually our goal with modeling in the fi rst place.",Random forests,2,,prose,102,91a82013-064e-4179-8c10-860943d32912,38e2d5df-0dbd-4408-9653-7bd62c26a4f6,2025-12-07T21:51:50.103071,,,,True
38e2d5df-0dbd-4408-9653-7bd62c26a4f6,5fd22b0a-4f4d-46ec-97d2-0107a8747661,24,"There are a few ways to combat over fi tting. One option is to limit the depth of the tree . If we limited the above tree to only two levels, for example, we would end the left branch at the Is frequent shopper split.",Random forests,2,,prose,50,f123ef9a-4ae2-4e36-a5a0-d5014b158923,0641890c-09d7-4f58-8cac-c0391eeaffc2,2025-12-07T21:51:50.103074,,,,True
0641890c-09d7-4f58-8cac-c0391eeaffc2,5fd22b0a-4f4d-46ec-97d2-0107a8747661,25,"<!-- image -->

<!-- image -->

The leaf nodes on the left branch now have mixed labels in their subsets. Allowing for this ""impurity"" might seem suboptimal, but it's a strong defense against noisy features: if Time idle and Age of account had predictive power in our training data just due to chance, a model that excluded those features would be better at generalizing to new data.",Random forests,2,,prose,80,38e2d5df-0dbd-4408-9653-7bd62c26a4f6,26830363-8066-46c4-b04a-302aa6b3c8bf,2025-12-07T21:51:50.103077,,,,True
26830363-8066-46c4-b04a-302aa6b3c8bf,5fd22b0a-4f4d-46ec-97d2-0107a8747661,26,"Limiting tree depth works well, but we can pair it with an even stronger strategy: ensemble learning . In machine learning - and in animal collectives aggregating a set of predictions often achieves higher accuracy than any individual prediction. Errors in individual models cancel out, allowing a clearer look at the underlying patterns in the data being modeled.",Random forests,2,,prose,66,0641890c-09d7-4f58-8cac-c0391eeaffc2,72e90ff9-d8fa-4a7b-a469-4c00ed72c3fe,2025-12-07T21:51:50.103080,,,,True
72e90ff9-d8fa-4a7b-a469-4c00ed72c3fe,5fd22b0a-4f4d-46ec-97d2-0107a8747661,27,"<!-- image -->

This sounds great, but there needs to be variation in model predictions for an ensemble to be useful. The algorithm we described in the last section - splitting on all values of all features to get the lowest Gini impurity - is deterministic. For a given dataset, our algorithm always outputs the same decision tree [2], so training 10 or 100 trees as an ensemble wouldn't actually accomplish anything. So how is a forest any better than an individual tree?",Random forests,2,,prose,96,26830363-8066-46c4-b04a-302aa6b3c8bf,1392d2f0-50e0-4f6e-bc96-f37e1154f771,2025-12-07T21:51:50.103084,,,,True
1392d2f0-50e0-4f6e-bc96-f37e1154f771,5fd22b0a-4f4d-46ec-97d2-0107a8747661,28,"This is where randomness comes in. Both the way our data is split and the data itself varies between trees in a random forest , allowing for variation in model predictions and greater protection against over fi tting.

Let's start with the data. We can protect against outliers hijacking our model with meaningless correlations by bootstrapping our data, or sampling with replacement . The idea is that outliers are rare, so they're less likely to be randomly selected than samples re fl ecting genuine relationships between features and labels. Bootstrapping lets us give each decision tree in our forest a slightly di � erent dataset that should still contain the same general trends.",Random forests,2,,prose,130,72e90ff9-d8fa-4a7b-a469-4c00ed72c3fe,4e5ffda2-1e8a-4258-acc5-8962abceafa8,2025-12-07T21:51:50.103087,,,,True
4e5ffda2-1e8a-4258-acc5-8962abceafa8,5fd22b0a-4f4d-46ec-97d2-0107a8747661,29,"<!-- image -->

The second way is that random forests randomly select only a subset of the features when evaluating how to split the data. scikit-learn's RandomForestClassifier , for example, only considers the square root of the number of features when searching for the thresholds that minimize Gini impurity.",Random forests,2,,prose,59,1392d2f0-50e0-4f6e-bc96-f37e1154f771,641bac9e-cba3-4ec8-b21e-a682adb6955f,2025-12-07T21:51:50.103090,,,,True
641bac9e-cba3-4ec8-b21e-a682adb6955f,5fd22b0a-4f4d-46ec-97d2-0107a8747661,30,"These methods might seem strange - why wouldn't we use all our features, and why would we purposely duplicate and drop rows in our data? And indeed, each individual tree we produce this way often has signi fi cantly worse predictive power than a normal decision tree. But when we aggregate dozens of these Swiss-cheese trees, a surprising result emerges: a forest that is collectively more accurate than our original decision trees.",Random forests,2,,prose,84,4e5ffda2-1e8a-4258-acc5-8962abceafa8,af2595d4-1577-410f-a230-c8385a777ff9,2025-12-07T21:51:50.103093,,,,True
af2595d4-1577-410f-a230-c8385a777ff9,5fd22b0a-4f4d-46ec-97d2-0107a8747661,31,<!-- image -->,Random forests,2,,prose,3,641bac9e-cba3-4ec8-b21e-a682adb6955f,10f2e4ae-25e8-46c5-b3c0-4a385f732134,2025-12-07T21:51:50.103096,,,,True
10f2e4ae-25e8-46c5-b3c0-4a385f732134,5fd22b0a-4f4d-46ec-97d2-0107a8747661,32,"## Implementation

Let's now implement a random forest in Python to see for ourselves. We'll start with the nodes of a tree, followed by a decision tree and fi nally a random forest.",Implementation,2,,prose,40,af2595d4-1577-410f-a230-c8385a777ff9,34860cba-a7e1-4c50-a031-b54650e49ad7,2025-12-07T21:51:50.103099,,,,True
34860cba-a7e1-4c50-a031-b54650e49ad7,5fd22b0a-4f4d-46ec-97d2-0107a8747661,33,"## Node

Let's start with a class that will serve as a node in our decision tree. The class will have the following attributes used for training :

- A subset of data (or entire dataset for the root node)
- The proportion of positive labels and Gini impurity of this subset
- Pointers to left and right child nodes, set to None if the node is a leaf",Node,2,,prose,79,10f2e4ae-25e8-46c5-b3c0-4a385f732134,91266ab8-916c-4293-9198-486bc01d844e,2025-12-07T21:51:50.103102,,,,True
91266ab8-916c-4293-9198-486bc01d844e,5fd22b0a-4f4d-46ec-97d2-0107a8747661,34,"The class will also have the following attributes for classifying new data :

- A feature name and threshold, used to point the input towards the left or right child node (if the node is not a leaf)
- Which label to return (if the node is a leaf)",Node,2,,prose,55,34860cba-a7e1-4c50-a031-b54650e49ad7,2f103871-fac8-4d60-9c2a-e6ab82b2a111,2025-12-07T21:51:50.103105,,,,True
2f103871-fac8-4d60-9c2a-e6ab82b2a111,5fd22b0a-4f4d-46ec-97d2-0107a8747661,35,"We can construct a Node class that meets these criteria with the below code. While the source code in GitHub has more thorough docstrings and input validation, I'll share a slimmed down version here for readability. Note that we'll call this fi le node.py and reference it in later fi les.",Node,2,,prose,60,91266ab8-916c-4293-9198-486bc01d844e,ea94b78f-05b6-473d-a653-1e3795b01d26,2025-12-07T21:51:50.103108,,,,True
ea94b78f-05b6-473d-a653-1e3795b01d26,5fd22b0a-4f4d-46ec-97d2-0107a8747661,36,"```
import numpy as np import pandas as pd from typing_extensions import Self class Node: """""" Node in a decision tree. """""" def __init__( self, df: pd.DataFrame, target_col: str ) -> None: # For training self.df = df self.target_col = target_col self.pk = self._set_pk() self.gini = self._set_gini() # For training/inference self.left = None self.right = None
```",Node,2,,code,91,2f103871-fac8-4d60-9c2a-e6ab82b2a111,238b8f6e-cfd9-4c79-877b-f694f1259d26,2025-12-07T21:51:50.103111,,,,True
238b8f6e-cfd9-4c79-877b-f694f1259d26,5fd22b0a-4f4d-46ec-97d2-0107a8747661,37,"# For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature.","For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",1,,prose,265,ea94b78f-05b6-473d-a653-1e3795b01d26,b8dc5542-0a1d-4e7b-9db5-15015c904a77,2025-12-07T21:51:50.103115,,,,True
b8dc5542-0a1d-4e7b-9db5-15015c904a77,5fd22b0a-4f4d-46ec-97d2-0107a8747661,38,"If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None","For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",1,,prose,187,238b8f6e-cfd9-4c79-877b-f694f1259d26,12331bc5-83e2-4500-a6d9-3a6772e62d91,2025-12-07T21:51:50.103118,,,,True
12331bc5-83e2-4500-a6d9-3a6772e62d91,5fd22b0a-4f4d-46ec-97d2-0107a8747661,39,"Here's \_process\_split , which does the actual work. We split self.df on the feature threshold, end early if either child dataset is empty, create child nodes with the subsets, and fi nally calculate the weighted Gini impurity.","For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",1,,prose,51,b8dc5542-0a1d-4e7b-9db5-15015c904a77,69ce6294-2561-4aad-9202-f2ea47bb4ecf,2025-12-07T21:51:50.103121,,,,True
69ce6294-2561-4aad-9202-f2ea47bb4ecf,5fd22b0a-4f4d-46ec-97d2-0107a8747661,40,"```
class Node: ... def _process_split( self, feature: str, threshold: int|float ) -> tuple[float, int|float, Self|None, Self|None]: """""" Splits df on the feature threshold. Returns weighted Gini impurity, inputted threshold, and child nodes. If split results in empty subset, returns Gini impurity and None's. """""" df_lower = self.df[self.df[feature] <= threshold] df_upper = self.df[self.df[feature] > threshold] # If threshold doesn't split the data at all, end early if len(df_lower) == 0 or len(df_upper) == 0: return self.gini, None, None, None node_lower = Node(df_lower, self.target_col) node_upper = Node(df_upper, self.target_col) prop_lower = len(df_lower) / len(self.df) prop_upper = len(df_upper) / len(self.df) weighted_gini = node_lower.gini * prop_lower + node_upper.gini * prop_upper return weighted_gini, threshold, node_lower, node_upper
```","For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",1,,code,225,12331bc5-83e2-4500-a6d9-3a6772e62d91,33f3ddbf-dc2e-4d29-9b3b-702acd6c2267,2025-12-07T21:51:50.103123,,,,True
33f3ddbf-dc2e-4d29-9b3b-702acd6c2267,5fd22b0a-4f4d-46ec-97d2-0107a8747661,41,"Let's quickly test it out. Below, we instantiate a node and have it fi nd the optimal split for the data. split\_on\_feature returns the weighted Gini impurity of 0.0 in the child nodes because we can perfectly partition the labels at a value of 2 (the second returned value). The third and fourth values are the left and right child nodes from the partitioning.","For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",1,,prose,82,69ce6294-2561-4aad-9202-f2ea47bb4ecf,ddc7bcad-209e-452b-83bf-907a5a2f0ff6,2025-12-07T21:51:50.103127,,,,True
ddc7bcad-209e-452b-83bf-907a5a2f0ff6,5fd22b0a-4f4d-46ec-97d2-0107a8747661,42,"```
import pandas as pd df = pd.DataFrame({'feature': [1, 2, 3], 'label': [0, 0, 1]}) node = Node(df, 'label') print(f""pk: {round(node.pk, 2)}, gini: {round(node.gini, 2)}"") # pk: 0.33, gini: 0.44 print(node.split_on_feature('feature')) # (0.0, 2, # <__main__.Node object at 0x137c279d0>, # <__main__.Node object at 0x137c24160>)
```","For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",1,,code,135,33f3ddbf-dc2e-4d29-9b3b-702acd6c2267,678ad0b7-123f-4c12-a131-87b6dcb18a86,2025-12-07T21:51:50.103130,,,,True
678ad0b7-123f-4c12-a131-87b6dcb18a86,5fd22b0a-4f4d-46ec-97d2-0107a8747661,43,"## Decision tree

The next step is to arrange nodes in a tree to best partition training data and most accurately classify new data. Let's start with the basic structure, then the ability to train the classi fi er (i.e., build the tree) and generate predictions. We store our decision\_tree.py fi le in the same directory as node.py and import Node from the fi le.",Decision tree,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,80,ddc7bcad-209e-452b-83bf-907a5a2f0ff6,eccea802-2a8c-4b49-b088-a9c3237c5012,2025-12-07T21:51:50.103133,,,,True
eccea802-2a8c-4b49-b088-a9c3237c5012,5fd22b0a-4f4d-46ec-97d2-0107a8747661,44,"Our DecisionTree class begins with the root node, which is a Node we instantiate with df and target\_col . feature\_select controls the proportion of features we use when training the trees of a random forest; we'll default to 100% of features for the base decision tree class. max\_depth speci fi es the maximum depth our tree can grow, which helps prevent over fi tting.",Decision tree,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,81,678ad0b7-123f-4c12-a131-87b6dcb18a86,47df3aff-2549-4eb1-bd04-05f9eb09ce62,2025-12-07T21:51:50.103136,,,,True
47df3aff-2549-4eb1-bd04-05f9eb09ce62,5fd22b0a-4f4d-46ec-97d2-0107a8747661,45,"```
import numpy as np import pandas as pd from .node import Node class DecisionTree: """""" Decision tree classifier, constructed with Nodes """""" def __init__( self, df: pd.DataFrame, target_col: str, feature_select: float = 1.0, max_depth: int = 4 ) -> None: self.root = Node(df, target_col) self.features = list(df) self.features.remove(target_col) self.feature_select = feature_select self.max_depth = max_depth
```",Decision tree,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",code,100,eccea802-2a8c-4b49-b088-a9c3237c5012,a0c41e5f-57f0-49f1-9303-5c1ec9806ac0,2025-12-07T21:51:50.103139,,,,True
a0c41e5f-57f0-49f1-9303-5c1ec9806ac0,5fd22b0a-4f4d-46ec-97d2-0107a8747661,46,"Now let's write the logic to build the tree. Proving that all that Leetcode I studied is useful outside of just coding interviews, we use a stack to perform a depthfi rst traversal, iteratively calling \_process\_node on each node and appending its children to the stack. We also keep an eye on our current depth to ensure we don't exceed self.max\_depth . Once we've processed all nodes, we return our DecisionTree instance.",Decision tree,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,93,47df3aff-2549-4eb1-bd04-05f9eb09ce62,d311d255-1d92-4180-b488-8c45711a164f,2025-12-07T21:51:50.103142,,,,True
d311d255-1d92-4180-b488-8c45711a164f,5fd22b0a-4f4d-46ec-97d2-0107a8747661,47,"```
class DecisionTree: ... def build_tree(self) -> None: """""" Builds tree using depth-first traversal """""" stack = [(self.root, 0)] while stack: current_node, depth = stack.pop() if depth <= self.max_depth: left, right = self._process_node(current_node) if left and right: current_node.left = left current_node.right = right stack.append((right, depth+1)) stack.append((left, depth+1))
```",Decision tree,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",code,95,a0c41e5f-57f0-49f1-9303-5c1ec9806ac0,6ea7470f-6554-4441-95ab-163b38e6441d,2025-12-07T21:51:50.103144,,,,True
6ea7470f-6554-4441-95ab-163b38e6441d,5fd22b0a-4f4d-46ec-97d2-0107a8747661,48,"What actually happens in \_process\_node ? We start by randomly selecting a subset of features (or just all of them if self.feature\_select is 1.0), followed by iterating through those features and calling the node's split\_on\_feature method to fi nd that feature's optimal split. We then fi nd the feature whose split resulted in the lowest Gini impurity in the child nodes and compare it to our current node's impurity. If the best split resulted in a lower impurity, we return the child nodes; if not, we return None to indicate we should stop traversing.",Decision tree,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,124,d311d255-1d92-4180-b488-8c45711a164f,4b23de6f-7076-4e85-8b8a-51bbc33c55ad,2025-12-07T21:51:50.103147,,,,True
4b23de6f-7076-4e85-8b8a-51bbc33c55ad,5fd22b0a-4f4d-46ec-97d2-0107a8747661,49,"```
class DecisionTree:
```

```
... def _process_node( self, node: Node ) -> tuple[Node|None, Node|None]: """""" Iterates through features, identifies split that minimizes Gini impurity in child nodes, and identifies feature whose split minimizes Gini impurity the most. Then returns child nodes split on that feature. """""" # Randomly select features. No randomness if # self.feature_select = 1.0 (default). features = list( np.random.choice( self.features, int(self.feature_select*len(self.features)), replace=False ) ) # Get Gini impurity for best split for each column d = {} for col in features: feature_info = node.split_on_feature(col) if feature_info[0] is not None: d[col] = feature_info # Select best column to split on min_gini = np.inf best_feature = None for col, tup in d.items(): if tup[0] < min_gini: min_gini = tup[0] best_feature = col # Only update if the best split reduces Gini impurity if min_gini < node.gini: # Update node node.feature = best_feature node.threshold = d[col][1] return d[col][2:] return None, None
```",Decision tree,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",code,262,6ea7470f-6554-4441-95ab-163b38e6441d,d4126f10-e370-45eb-a7ed-835b3cf820e9,2025-12-07T21:51:50.103150,,,,True
d4126f10-e370-45eb-a7ed-835b3cf820e9,5fd22b0a-4f4d-46ec-97d2-0107a8747661,50,"The above code allows us to build a tree. Now let's write the code for generating predictions. Our classify function is just a wrapper around the helper \_classify and lets us generate predictions on a dataframe of feature vectors (rather than just one at a time). The real work is done in \_classify , which recursively moves to the left or right child node depending on how the feature vector compares to the node's threshold. A leaf node has None for its feature and threshold attributes, so we then return a label of 1 if the node's pk is greater than 0.5, else 0 .",Decision tree,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,124,4b23de6f-7076-4e85-8b8a-51bbc33c55ad,2e77e40d-5c42-4ad4-8780-a256b244e9eb,2025-12-07T21:51:50.103153,,,,True
2e77e40d-5c42-4ad4-8780-a256b244e9eb,5fd22b0a-4f4d-46ec-97d2-0107a8747661,51,"```
class DecisionTree: ... def classify(self, feature_df: pd.DataFrame) -> list[int]: """""" Given a dataframe where each row is a feature vector, traverses the tree to generate a predicted label. """""" return [ self._classify(self.root, f) for i, f in feature_df.iterrows( ] def _classify(self, node: Node, features: pd.Series) -> int: """""" Given a vector of features, traverse the node's children until a leaf is reached, then return the most frequent class in the node. If there are an equal number of positive and negative labels, predicts the negative class. """""" # Child node if node.feature is None or node.threshold is None: return int(node.pk > 0.5) if features[node.feature] < node.threshold: return self._classify(node.left, features) return self._classify(node.right, features)
```",Decision tree,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",code,180,d4126f10-e370-45eb-a7ed-835b3cf820e9,ec923de8-d480-4cca-9dad-fb7cf2e860e5,2025-12-07T21:51:50.103156,,,,True
ec923de8-d480-4cca-9dad-fb7cf2e860e5,5fd22b0a-4f4d-46ec-97d2-0107a8747661,52,"## Random Forest

We have everything we need for a decision tree classi fi er! The hardest work - by far - is behind us. Extending our classi fi er to a random forest just requires generating multiple trees on bootstrapped data, since we've already implemented randomized feature selection in \_process\_node .",Random Forest,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,66,2e77e40d-5c42-4ad4-8780-a256b244e9eb,b74c4ab2-3ca1-455d-b4ce-08c3414f4a16,2025-12-07T21:51:50.103159,,,,True
b74c4ab2-3ca1-455d-b4ce-08c3414f4a16,5fd22b0a-4f4d-46ec-97d2-0107a8747661,53,"So let's create a random\_forest.py fi le that contains a RandomForest class. As always, we instantiate the class with a dataframe ( df ) and target column, and like the DecisionTree class we have a feature\_select and max\_depth parameter. Instantiation now also takes an n\_trees parameter.",Random Forest,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,63,ec923de8-d480-4cca-9dad-fb7cf2e860e5,78dce3ac-7fd0-4402-96d7-58662740d363,2025-12-07T21:51:50.103162,,,,True
78dce3ac-7fd0-4402-96d7-58662740d363,5fd22b0a-4f4d-46ec-97d2-0107a8747661,54,"```
import pandas as pd from .decision_tree import DecisionTree class RandomForest: """""" Forest of decision trees. """""" def __init__(
```

```
self, df: pd.DataFrame, target_col: str, n_trees: int = 100, feature_select: float = 0.5, max_depth: int = 4 ) -> None: self.df = df self.target_col = target_col self.n_trees = n_trees self.feature_select = feature_select self.max_depth = max_depth self.forest = []
```",Random Forest,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",code,106,b74c4ab2-3ca1-455d-b4ce-08c3414f4a16,d3b73e4f-c3ee-4883-8684-b51744d50aa8,2025-12-07T21:51:50.103166,,,,True
d3b73e4f-c3ee-4883-8684-b51744d50aa8,5fd22b0a-4f4d-46ec-97d2-0107a8747661,55,"Training is straightforward: we create n\_trees bootstrapped dataframes, instantiate a DecisionTree for each one, then call .build\_tree to train each decision tree.

```
class RandomForest: ... def train(self) -> None: """""" Fit the forest to self.df """""" bootstrap_dfs = [self._bootstrap() for _ in range(self.n_trees) self.forest = [ DecisionTree( bdf, self.target_col, self.feature_select, self.max_depth ) for bdf in bootstrap_dfs ] self.forest = [tree.build_tree() for tree in self.forest] return None def _bootstrap(self) -> pd.DataFrame: """""" Sample rows from self.df with replacement """""" return self.df.sample(len(self.df), replace=True)
```",Random Forest,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",code,152,78dce3ac-7fd0-4402-96d7-58662740d363,7829def4-a1c6-4d7c-9dcc-8c3eba7168b0,2025-12-07T21:51:50.103169,,,,True
7829def4-a1c6-4d7c-9dcc-8c3eba7168b0,5fd22b0a-4f4d-46ec-97d2-0107a8747661,56,"Classi fi cation is also simple: we have each tree in the forest classify the inputted feature\_df , then return the most common predicted label for each row. I found that the easiest way to do this was to cast the preds list to a dataframe and then take the mode.",Random Forest,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,59,d3b73e4f-c3ee-4883-8684-b51744d50aa8,5b8e10ef-b4fc-47df-9cb4-d96e91499f63,2025-12-07T21:51:50.103172,,,,True
5b8e10ef-b4fc-47df-9cb4-d96e91499f63,5fd22b0a-4f4d-46ec-97d2-0107a8747661,57,"```
class RandomForest ... def classify(self, feature_df: pd.DataFrame) -> int: """""" Classify inputted feature vectors. Each tree in the forest generates a predicted label and the most common label for each feature vector is returned. """""" preds = pd.DataFrame( [tree.classify(feature_df) for tree in self.forest] ) # Return most common predicted label return list(preds.mode().iloc[0])
```",Random Forest,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",code,85,7829def4-a1c6-4d7c-9dcc-8c3eba7168b0,84aa45d4-47f0-4aff-8bb2-8216dcebcee7,2025-12-07T21:51:50.103175,,,,True
84aa45d4-47f0-4aff-8bb2-8216dcebcee7,5fd22b0a-4f4d-46ec-97d2-0107a8747661,58,"And… that's it! If we run the script run.py, we can compare the accuracies of our DecisionTree classi fi er, the average tree in a RandomForest , the full RandomForest , and scikit-learn's RandomForestClassifier for good measure. The results will vary by dataset, number of trees per forest, etc., but we can see that our lone decision tree has higher accuracy than the average tree in the forest, and that the full random forest is substantially stronger than individual trees. Unsurprisingly, scikit-learn has the highest accuracy… but we didn't do too badly!",Random Forest,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,121,5b8e10ef-b4fc-47df-9cb4-d96e91499f63,3b00abe3-f764-45e2-b629-a16e56ff44bb,2025-12-07T21:51:50.103177,,,,True
3b00abe3-f764-45e2-b629-a16e56ff44bb,5fd22b0a-4f4d-46ec-97d2-0107a8747661,59,"## Accuracy

* Single decision tree:   0.61

* Avg random forest tree: 0.595

* Full random forest:     0.81

* Scikit-learn forest:    0.89

Photo by Todd Quackenbush on Unsplash",Accuracy,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,60,84aa45d4-47f0-4aff-8bb2-8216dcebcee7,440c78e0-8082-4abc-9034-fa423039b5e9,2025-12-07T21:51:50.103181,,,,True
440c78e0-8082-4abc-9034-fa423039b5e9,5fd22b0a-4f4d-46ec-97d2-0107a8747661,60,<!-- image -->,Accuracy,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,3,3b00abe3-f764-45e2-b629-a16e56ff44bb,b9cfbbbb-d879-49a8-a03e-e2cc7de818a5,2025-12-07T21:51:50.103184,,,,True
b9cfbbbb-d879-49a8-a03e-e2cc7de818a5,5fd22b0a-4f4d-46ec-97d2-0107a8747661,61,"## Conclusions

In this post, we covered one of the most popular and powerful algorithms in machine learning: the random forest. We started with the theory, explaining how decision trees use metrics such as Gini impurity to identify the value of a feature that best splits a dataset. We then showed how we can build a tree by iteratively identifying the feature whose optimal split most reduces Gini impurity until the classes are either perfectly separated or we reach a maximum accepted depth. We explained how a random forest is constructed with dozens of decision trees, random feature selection, and bootstrapped datasets. Finally, we wrote some code to test the theory.",Conclusions,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,131,440c78e0-8082-4abc-9034-fa423039b5e9,bfc14a84-bd3d-4b00-8288-d678d0f088eb,2025-12-07T21:51:50.103186,,,,True
bfc14a84-bd3d-4b00-8288-d678d0f088eb,5fd22b0a-4f4d-46ec-97d2-0107a8747661,62,"If you're hungry for more, there are plenty of mini projects to try. You could visualize how the same feature can be used multiple times in a tree if it has a highly non-linear relationship with the target variable. We didn't cover regression in this post, but it wouldn't take much e � ort to modify our code to handle a continuous target. It'd be interesting to see how the predictive power of the vanilla DecisionTree , the average random forest tree, and the full forest diverge as a function of the amount of data and number of columns. Finally, if anyone wants to parallelize how RandomForest trains trees, feel free to submit a PR.",Conclusions,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,133,b9cfbbbb-d879-49a8-a03e-e2cc7de818a5,d530dbe0-cc9f-41f2-a8f2-987661a42823,2025-12-07T21:51:50.103189,,,,True
d530dbe0-cc9f-41f2-a8f2-987661a42823,5fd22b0a-4f4d-46ec-97d2-0107a8747661,63,Thanks for reading! Matt,Conclusions,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,5,bfc14a84-bd3d-4b00-8288-d678d0f088eb,ad103c92-8902-4845-8e38-963284e6233c,2025-12-07T21:51:50.103192,,,,True
ad103c92-8902-4845-8e38-963284e6233c,5fd22b0a-4f4d-46ec-97d2-0107a8747661,64,"## 1. Decision tree training

This fi gure was a serious labor of love. Figuring out how to get the curve (and circles) to transition from one color to another was tricky, but surprisingly the hardest part was dealing with the spacing of the x's and checkmarks to keep them cleanly within the circles but not too bunched up. I changed the icons and colors of this fi gure so many times that I ended up writing a script to generate it.",1. Decision tree training,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,97,d530dbe0-cc9f-41f2-a8f2-987661a42823,8ba9df57-adf2-4d11-bcc1-ae1d1a455af2,2025-12-07T21:51:50.103195,,,,True
8ba9df57-adf2-4d11-bcc1-ae1d1a455af2,5fd22b0a-4f4d-46ec-97d2-0107a8747661,65,"Here's a version of the fi gure I had for a while before I thought I needed a clearer visual distinction from the toy example (partitioning shapes) and the main example in the post (ecommerce).

<!-- image -->",1. Decision tree training,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,46,ad103c92-8902-4845-8e38-963284e6233c,2b96f9cd-d78d-4ccd-9c0a-d6abe42af207,2025-12-07T21:51:50.103198,,,,True
2b96f9cd-d78d-4ccd-9c0a-d6abe42af207,5fd22b0a-4f4d-46ec-97d2-0107a8747661,66,"## 2. Random forests

You can see for yourself how sklearn outputs identical decision trees for a given dataset with the below code. Note that we need to specify the same random\_state .

```
import numpy as np import pandas as pd from sklearn.tree import DecisionTreeClassifier # Params random_state = 42 # Generate data df = pd.DataFrame( { 'feature_1': np.random.normal(0, 1, 100), 'feature_2': np.random.normal(1, 1, 100), 'label': np.random.choice([0, 1], 100) } ) mod1 = DecisionTreeClassifier(random_state=random_state) mod1.fit(df[['feature_1', 'feature_2']], df['label']) mod2 = DecisionTreeClassifier(random_state=42) mod2.fit(df[['feature_1', 'feature_2']], df['label']) comparison = (mod1.tree_.value == mod2.tree_.value) print(comparison.all()) # True
```",2. Random forests,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",code,205,8ba9df57-adf2-4d11-bcc1-ae1d1a455af2,717bea27-2353-49ad-95cf-41675c730bc0,2025-12-07T21:51:50.103201,,,,True
717bea27-2353-49ad-95cf-41675c730bc0,5fd22b0a-4f4d-46ec-97d2-0107a8747661,67,"·

·

·

WRITTEN BY",2. Random forests,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,10,2b96f9cd-d78d-4ccd-9c0a-d6abe42af207,73cef01a-6ae8-470c-bd1a-b5503b4bb0b5,2025-12-07T21:51:50.103204,,,,True
73cef01a-6ae8-470c-bd1a-b5503b4bb0b5,5fd22b0a-4f4d-46ec-97d2-0107a8747661,68,"## Matt Sosna

<!-- image -->

Towards Data Science is a community publication. Submit your insights to reach our global audience and earn through the TDS Author Payment Program.

<!-- image -->",Matt Sosna,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,38,717bea27-2353-49ad-95cf-41675c730bc0,434280d1-73fe-4a86-844a-935c49a69d84,2025-12-07T21:51:50.103207,,,,True
434280d1-73fe-4a86-844a-935c49a69d84,5fd22b0a-4f4d-46ec-97d2-0107a8747661,69,"## Related Articles

<!-- image -->

<!-- image -->

<!-- image -->

ARTIFICIAL INTELLIGENCE",Related Articles,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,20,73cef01a-6ae8-470c-bd1a-b5503b4bb0b5,88ddc866-6ce6-4d50-bc4d-d5eef4ab9b5b,2025-12-07T21:51:50.103210,,,,True
88ddc866-6ce6-4d50-bc4d-d5eef4ab9b5b,5fd22b0a-4f4d-46ec-97d2-0107a8747661,70,"## Implementing Convolutional Neural Networks in TensorFlow

Step-by-step code guide to building a Convolutional Neural Network

Shreya Rao

August 20, 2024

6 min read

<!-- image -->

DATA SCIENCE",Implementing Convolutional Neural Networks in TensorFlow,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,48,434280d1-73fe-4a86-844a-935c49a69d84,5daaf88b-9d22-4051-a366-f2d38a482686,2025-12-07T21:51:50.103213,,,,True
5daaf88b-9d22-4051-a366-f2d38a482686,5fd22b0a-4f4d-46ec-97d2-0107a8747661,71,"## Hands-on Time Series Anomaly Detection using Autoencoders, with Python

Here's how to use Autoencoders to detect signals with anomalies in a few lines of…

Piero Paialunga

August 21, 2024 12 min read","Hands-on Time Series Anomaly Detection using Autoencoders, with Python",2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,52,88ddc866-6ce6-4d50-bc4d-d5eef4ab9b5b,59445c81-c674-418a-a0b5-ba22b5b4a046,2025-12-07T21:51:50.103215,,,,True
59445c81-c674-418a-a0b5-ba22b5b4a046,5fd22b0a-4f4d-46ec-97d2-0107a8747661,72,DATA SCIENCE,"Hands-on Time Series Anomaly Detection using Autoencoders, with Python",2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,3,5daaf88b-9d22-4051-a366-f2d38a482686,3c7e23bb-ba14-4ec0-80a0-03db1980affd,2025-12-07T21:51:50.103218,,,,True
3c7e23bb-ba14-4ec0-80a0-03db1980affd,5fd22b0a-4f4d-46ec-97d2-0107a8747661,73,"## Solving a Constrained Project Scheduling Problem with Quantum Annealing

Solving the resource constrained project scheduling problem (RCPSP) with DWave's hybrid constrained quadratic model (CQM)

Luis Fernando PÉREZ ARMAS, Ph.D.",Solving a Constrained Project Scheduling Problem with Quantum Annealing,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,53,59445c81-c674-418a-a0b5-ba22b5b4a046,e1b190f5-9761-46d5-ba2d-1e2db78099f9,2025-12-07T21:51:50.103221,,,,True
e1b190f5-9761-46d5-ba2d-1e2db78099f9,5fd22b0a-4f4d-46ec-97d2-0107a8747661,74,"August 20, 2024

29 min read

ARTIFICIAL INTELLIGENCE

<!-- image -->",Solving a Constrained Project Scheduling Problem with Quantum Annealing,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,23,3c7e23bb-ba14-4ec0-80a0-03db1980affd,d0535548-02a8-4aca-91e7-8b8a5d1e9378,2025-12-07T21:51:50.103224,,,,True
d0535548-02a8-4aca-91e7-8b8a5d1e9378,5fd22b0a-4f4d-46ec-97d2-0107a8747661,75,"## How to Forecast Hierarchical Time Series

A beginner's guide to forecast reconciliation

Dr. Robert Kübler

August 20, 2024 13 min read

MACHINE LEARNING

<!-- image -->",How to Forecast Hierarchical Time Series,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,43,e1b190f5-9761-46d5-ba2d-1e2db78099f9,7b3003f2-503b-4128-9cc9-2ccf3d8c2bb6,2025-12-07T21:51:50.103227,,,,True
7b3003f2-503b-4128-9cc9-2ccf3d8c2bb6,5fd22b0a-4f4d-46ec-97d2-0107a8747661,76,"## 3 AI Use Cases (That Are Not a Chatbot)

Feature engineering, structuring unstructured data, and lead scoring

Shaw Talebi

August 21, 2024 7 min read

DATA SCIENCE

<!-- image -->",3 AI Use Cases (That Are Not a Chatbot),2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,51,d0535548-02a8-4aca-91e7-8b8a5d1e9378,f64844e3-35ea-4548-94b9-86ab1c7a87b4,2025-12-07T21:51:50.103230,,,,True
f64844e3-35ea-4548-94b9-86ab1c7a87b4,5fd22b0a-4f4d-46ec-97d2-0107a8747661,77,"## Back To Basics, Part Uno: Linear Regression and Cost Function

An illustrated guide on essential machine learning concepts

Shreya Rao

February 3, 2023 6 min read

DATA SCIENCE","Back To Basics, Part Uno: Linear Regression and Cost Function",2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,43,7b3003f2-503b-4128-9cc9-2ccf3d8c2bb6,6ff9ad24-5b01-40a3-a21d-d56d00706948,2025-12-07T21:51:50.103233,,,,True
6ff9ad24-5b01-40a3-a21d-d56d00706948,5fd22b0a-4f4d-46ec-97d2-0107a8747661,78,"## Must-Know in Statistics: The Bivariate Normal Projection Explained

Derivation and practical examples of this powerful concept

Luigi Battistoni August 14, 2024 7 min read

<!-- image -->

<!-- image -->

Your home for data science and Al. The world's leading publication for data science, data analytics, data engineering, machine learning, and arti fi cial intelligence professionals.",Must-Know in Statistics: The Bivariate Normal Projection Explained,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,83,f64844e3-35ea-4548-94b9-86ab1c7a87b4,3d31c6de-dbbc-4651-a966-4086580aeb4d,2025-12-07T21:51:50.103236,,,,True
3d31c6de-dbbc-4651-a966-4086580aeb4d,5fd22b0a-4f4d-46ec-97d2-0107a8747661,79,"© Insight Media Group, LLC 2025

<!-- image -->

<!-- image -->

<!-- image -->

Subscribe to Our Newsletter

COOKIES SETTINGS WRITE FOR TDS · ABOUT · ADVERTISE PRIVACY POLICY · TERMS OF USE

•",Must-Know in Statistics: The Bivariate Normal Projection Explained,2,"For inference self.feature = None self.threshold = None def \_set\_pk(self) -&gt; float: """""" Sets pk, the prop samples that are the positive class. Assumes samples are an array of ints, where 1 is the positive class and 0 is the negative class. """""" return np.mean(self.df[self.target\_col].values) def \_set\_gini(self) -&gt; float: """""" Sets the Gini impurity. """""" return 1 - self.pk**2 -(1 - self.pk)**2 So far the code is fairly lightweight. We instantiate the node by specifying a dataframe ( df ) and the column containing labels ( target\_col ). We create empty attributes for the left and right child nodes ( self.left , self.right ) and the feature and threshold values used for inference. Finally, we calculate pk (the proportion of 1's in target column) and the Gini impurity with the \_set\_pk and \_set\_gini methods, respectively. Now let's add the logic for iterating through the values of a feature and identifying the threshold that minimizes the Gini impurity in the child nodes. The function split\_on\_feature runs the helper function \_process\_split on each unique value in a feature. If any values are left after removing nulls (for leaf nodes, the list will be empty), we return the Gini impurity, feature threshold, and left and right child nodes for the split that resulted in the lowest impurity. class Node: ... def split\_on\_feature( self, feature: str ) -&gt; tuple[float, int|float, Self, Self]: """""" Iterate through values of a feature and identify split that minimizes weighted Gini impurity in child nodes. Returns tuple of weighted Gini impurity, feature threshold, and left and right child nodes. """""" values = [] for thresh in self.df[feature].unique(): values.append(self.\_process\_split(feature, thresh)) values = [v for v in values if v[1] is not None] if values: return min(values, key=lambda x: x[0]) return None, None, None, None",prose,47,6ff9ad24-5b01-40a3-a21d-d56d00706948,,2025-12-07T21:51:50.103239,,,,True
