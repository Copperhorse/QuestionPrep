chunk_id,file_id,chunk_index,content,section_header,section_level,parent_section,top_header,content_type,estimated_tokens,prev_chunk_id,next_chunk_id,created_at,quality_score,eval_content_type,eval_reason,should_use
6686c2c5-4ac1-45d1-b591-b9c4792058bf,312060dd-8950-458a-9680-47ea55c0345e,0,"## Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup  
Luyu Gao 1 , Yunyi Zhang 2 , Jiawei Han 2 , Jamie Callan 1  
1 Language Technologies Institute, Carnegie Mellon University  
2 Department of Computer Science, University of Illinois Urbana-Champaign  
1 { luyug, callan } @cs.cmu.edu 2 { yzhan238, hanj } @illionis.edu",Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup,2,,,prose,91,,bfe49823-a361-486a-9fd2-faad744f2c34,2025-12-18T02:00:21.653226,95,main_content,OK,True
bfe49823-a361-486a-9fd2-faad744f2c34,312060dd-8950-458a-9680-47ea55c0345e,1,"## Abstract  
Contrastive learning has been applied successfully to learn vector representations of text. Previous research demonstrated that learning high-quality representations benefits from batch-wise contrastive loss with a large number of negatives. In practice, the technique of in-batch negative is used, where for each example in a batch, other batch examples' positives will be taken as its negatives, avoiding encoding extra negatives. This, however, still conditions each example's loss on all batch examples and requires fitting the entire large batch into GPU memory. This paper introduces a gradient caching technique that decouples backpropagation between contrastive loss and the encoder, removing encoder backward pass data dependency along the batch dimension. As a result, gradients can be computed for one subset of the batch at a time, leading to almost constant memory usage. 1",Abstract,2,,,prose,162,6686c2c5-4ac1-45d1-b591-b9c4792058bf,c2da299e-99ce-4101-b5e5-8560d12124b6,2025-12-18T02:00:21.653236,95,main_content,OK,True
c2da299e-99ce-4101-b5e5-8560d12124b6,312060dd-8950-458a-9680-47ea55c0345e,2,"## 1 Introduction  
Contrastive learning learns to encode data into an embedding space such that related data points have closer representations and unrelated ones have further apart ones. Recent works in NLP adopt deep neural nets as encoders and use unsupervised contrastive learning on sentence representation (Giorgi et al., 2020), text retrieval (Lee et al., 2019), and language model pre-training tasks (Wu et al., 2020). Supervised contrastive learning (Khosla et al., 2020) has also been shown effective in training dense retrievers (Karpukhin et al., 2020; Qu et al., 2020). These works typically use batch-wise contrastive loss, sharing target texts as in-batch negatives. With such a technique, previous works have empirically shown that larger batches help learn better representations. However, computing loss and updating model parameters with respect  
1 Our code is at github.com/luyug/GradCache .  
to a big batch require encoding all batch data and storing all activation, so batch size is limited by total available GPU memory. This limits application and research of contrastive learning methods under memory limited setup, e.g. academia. For example, Lee et al. (2019) pre-train a BERT (Devlin et al., 2019) passage encoder with a batch size of 4096 while a high-end commercial GPU RTX 2080ti can only fit a batch of 8. The gradient accumulation technique, splitting a large batch into chunks and summing gradients across several backwards, cannot emulate a large batch as each smaller chunk has fewer in-batch negatives.  
In this paper, we present a simple technique that thresholds peak memory usage for contrastive learning to almost constant regardless of the batch size. For deep contrastive learning, the memory bottlenecks are at the deep neural network based encoder. We observe that we can separate the backpropagation process of contrastive loss into two parts, from loss to representation, and from representation to model parameter, with the latter being independent across batch examples given the former, detailed in subsection 3.2. We then show in subsection 3.3 that by separately pre-computing the representations' gradient and store them in a cache, we can break the update of the encoder into multiple sub-updates that can fit into the GPU memory. This pre-computation of gradients allows our method to produce the exact same gradient update as training with large batch. Experiments show that with about 20% increase in runtime, our technique enables a single consumer-grade GPU to reproduce the state-of-the-art large batch trained models that used to require multiple professional GPUs.",1 Introduction,2,,,prose,551,bfe49823-a361-486a-9fd2-faad744f2c34,cd82e9a2-1c46-471c-8d8a-9c6cbf2821c7,2025-12-18T02:00:21.653241,80,main_content,OK,True
cd82e9a2-1c46-471c-8d8a-9c6cbf2821c7,312060dd-8950-458a-9680-47ea55c0345e,3,"## 2 Related Work  
Contrastive Learning First introduced for probablistic language modeling (Mnih and Teh, 2012),  
Noise Contrastive Estimation (NCE) was later used by Word2Vec (Mikolov et al., 2013) to learn word embedding. Recent works use contrastive learning to unsupervisedly pre-train (Lee et al., 2019; Chang et al., 2020) as well as supervisedly train dense retriever (Karpukhin et al., 2020), where contrastive loss is used to estimate retrieval probability over the entire corpus. Inspired by SimCLR (Chen et al., 2020), constrastive learning is used to learn better sentence representation (Giorgi et al., 2020) and pre-trained language model (Wu et al., 2020).  
Deep Network Memory Reduction Many existing techniques deal with large and deep models. The gradient checkpoint method attempts to emulate training deep networks by training shallower layers and connecting them with gradient checkpoints and re-computation (Chen et al., 2016). Some methods also use reversible activation functions, allowing internal activation in the network to be recovered throughout back propagation (Gomez et al., 2017; MacKay et al., 2018). However, their effectiveness as part of contrastive encoders has not been confirmed. Recent work also attempts to remove the redundancy in optimizer tracked parameters on each GPU (Rajbhandari et al., 2020). Compared with the aforementioned methods, our method is designed for scaling over the batch size dimension for contrastive learning.",2 Related Work,2,,,prose,336,c2da299e-99ce-4101-b5e5-8560d12124b6,11863e17-7a25-40b3-80b7-9e7f7141851a,2025-12-18T02:00:21.653244,90,main_content,OK,True
11863e17-7a25-40b3-80b7-9e7f7141851a,312060dd-8950-458a-9680-47ea55c0345e,4,"## 3 Methodologies  
In this section, we formally introduce the notations for contrastive loss and analyze the difficulties of using it on limited hardware. We then show how we can use a Gradient Cache technique to factor the loss so that large batch gradient update can be broken into several sub-updates.",3 Methodologies,2,,,prose,61,cd82e9a2-1c46-471c-8d8a-9c6cbf2821c7,17aa9637-cf1c-4433-b4b3-de84c7a8d4f1,2025-12-18T02:00:21.653248,100,main_content,OK,True
17aa9637-cf1c-4433-b4b3-de84c7a8d4f1,312060dd-8950-458a-9680-47ea55c0345e,5,"## 3.1 Preliminaries  
Under a general formulation, given two classes of data S , T , we want to learn encoders f and g for each such that, given s ∈ S , t ∈ T , encoded representations f ( s ) and g ( t ) are close if related and far apart if not related by some distance measurement. For large S and T and deep neural network based f and g , direct training is not tractable, so a common approach is to use a contrastive loss: sample anchors S ⊂ S and targets T ⊂ T as a training batch, where each element s i ∈ S has a related element t r i ∈ T as well as zero or more specially sampled hard negatives. The rest of the random samples in T will be used as in-batch negatives.  
Define loss based on dot product as follows:",3.1 Preliminaries,2,,,prose,176,11863e17-7a25-40b3-80b7-9e7f7141851a,fe572e0e-4aa3-44fa-b05c-ed8683da4f2a,2025-12-18T02:00:21.653252,95,main_content,OK,True
fe572e0e-4aa3-44fa-b05c-ed8683da4f2a,312060dd-8950-458a-9680-47ea55c0345e,6,$$\mathcal { L } = - \frac { 1 } { | S | } \sum _ { s _ { i } \in S } \log \frac { e x p ( f ( s _ { i } ) ^ { \top } g ( t _ { r _ { i } } ) / \tau ) } { \sum _ { t _ { j } \in T } e x p ( f ( s _ { i } ) ^ { \top } g ( t _ { j } ) / \tau ) } \\ \intertext { s i g n } \vartheta h o r a $ o h $ o w h o r a $ s u m m o t y $ o n t h o r a $ w h o r a $ o t h o r a $ w h o r a $ s u m m o t y$$,3.1 Preliminaries,2,,,math,187,17aa9637-cf1c-4433-b4b3-de84c7a8d4f1,b7639979-6ea4-48e0-9525-e9168fb0a200,2025-12-18T02:00:21.653255,80,main_content,OK,True
b7639979-6ea4-48e0-9525-e9168fb0a200,312060dd-8950-458a-9680-47ea55c0345e,7,"where each summation term depends on the entire set T and requires fitting all of them into memory.  
We set temperature τ = 1 in the following discussion for simplicity as in general it only adds a constant multiplier to the gradient.",3.1 Preliminaries,2,,,prose,47,fe572e0e-4aa3-44fa-b05c-ed8683da4f2a,562d3537-f182-4861-9fb7-331e92710a40,2025-12-18T02:00:21.653264,100,main_content,OK,True
562d3537-f182-4861-9fb7-331e92710a40,312060dd-8950-458a-9680-47ea55c0345e,8,"## 3.2 Analysis of Computation  
In this section, we give a mathematical analysis of contrastive loss computation and its gradient. We show that the back propagation process can be divided into two parts, from loss to representation, and from representation to encoder model. The separation then enables us to devise a technique that removes data dependency in encoder parameter update. Suppose the function f is parameterized with Θ and g is parameterized with Λ .",3.2 Analysis of Computation,2,,,prose,91,b7639979-6ea4-48e0-9525-e9168fb0a200,079827da-f5fc-408a-87a1-a83ff5ea9eca,2025-12-18T02:00:21.653270,100,main_content,OK,True
079827da-f5fc-408a-87a1-a83ff5ea9eca,312060dd-8950-458a-9680-47ea55c0345e,9,$$\frac { \partial \mathcal { L } } { \partial \Theta } = \sum _ { s _ { i } \in S } \frac { \partial \mathcal { L } } { \partial f ( s _ { i } ) } \frac { \partial f ( s _ { i } ) } { \partial \Theta } \\$$,3.2 Analysis of Computation,2,,,math,79,562d3537-f182-4861-9fb7-331e92710a40,0a60da20-64d8-442d-a890-ceb788f4df20,2025-12-18T02:00:21.653273,80,main_content,OK,True
0a60da20-64d8-442d-a890-ceb788f4df20,312060dd-8950-458a-9680-47ea55c0345e,10,$$\frac { \partial \mathcal { L } } { \partial \Lambda } = \sum _ { t _ { j } \in T } \frac { \partial \mathcal { L } } { \partial g ( t _ { j } ) } \frac { \partial g ( t _ { j } ) } { \partial \Lambda }$$,3.2 Analysis of Computation,2,,,math,78,079827da-f5fc-408a-87a1-a83ff5ea9eca,7711e789-0084-4839-80c5-8cddb09ca2e4,2025-12-18T02:00:21.653277,80,main_content,OK,True
7711e789-0084-4839-80c5-8cddb09ca2e4,312060dd-8950-458a-9680-47ea55c0345e,11,"As an extra notation, denote normalized similarity,

$$p _ { i j } = \frac { e x p ( f ( s _ { i } ) \tau _ { g ( t _ { j } ) } ) } { \sum _ { t \in T } e x p ( f ( s _ { i } ) \tau _ { g ( t ) } ) }$$",3.2 Analysis of Computation,2,,,math,81,0a60da20-64d8-442d-a890-ceb788f4df20,c2e01958-9c78-43ce-a1ac-38d39b9c068e,2025-12-18T02:00:21.653292,80,main_content,OK,True
c2e01958-9c78-43ce-a1ac-38d39b9c068e,312060dd-8950-458a-9680-47ea55c0345e,12,"We note that the summation term for a particular s i or t i is a function of the batch, as,

$$\begin{array} { r l } { \text {for} } & \frac { \partial \mathcal { L } } { \partial f ( s _ { i } ) } = - \frac { 1 } { | S | } \left ( g ( t _ { r _ { i } } ) - \sum _ { t _ { j } \in T } p _ { i j } g ( t _ { j } ) \right ) , \ ( 5 ) } \end{array}$$",3.2 Analysis of Computation,2,,,math,138,7711e789-0084-4839-80c5-8cddb09ca2e4,d56dab75-d723-4ae3-9f81-e53815e7f8cd,2025-12-18T02:00:21.653296,80,main_content,OK,True
d56dab75-d723-4ae3-9f81-e53815e7f8cd,312060dd-8950-458a-9680-47ea55c0345e,13,"$$\text {of} \quad \frac { \partial \mathcal { L } } { \partial g ( t _ { j } ) } = - \frac { 1 } { | S | } \left ( \epsilon _ { j } - \sum _ { s _ { i } \in S } p _ { i j } f ( s _ { i } ) \right ) ,$$",3.2 Analysis of Computation,2,,,math,87,c2e01958-9c78-43ce-a1ac-38d39b9c068e,532a7f77-3a6a-4362-8fbe-1ad11c69be43,2025-12-18T02:00:21.653299,80,main_content,OK,True
532a7f77-3a6a-4362-8fbe-1ad11c69be43,312060dd-8950-458a-9680-47ea55c0345e,14,"where

$$\epsilon _ { j } = \begin{cases} f ( s _ { k } ) & \text {if } \exists \, k \, \text { s.t. } r _ { k } = j \\ 0 & \text {otherwise} \end{cases}$$",3.2 Analysis of Computation,2,,,math,65,d56dab75-d723-4ae3-9f81-e53815e7f8cd,fa2f8558-6900-49d3-820b-19ac8b11c0d9,2025-12-18T02:00:21.653302,70,main_content,OK,True
fa2f8558-6900-49d3-820b-19ac8b11c0d9,312060dd-8950-458a-9680-47ea55c0345e,15,"which prohibits the use of gradient accumulation. We make two observations here:  
- The partial derivative ∂f ( s i ) ∂ Θ depends only on s i and Θ while ∂g ( t j ) ∂ Λ depends only on t j and Λ ; and
- Computing partial derivatives ∂ L ∂f ( s i ) and ∂ L ∂g ( t j ) requires only encoded representations, but not Θ or Λ .  
These observations mean back propagation of f ( s i ) for data s i can be run independently with its own computation graph and activation if the numerical value of the partial derivative ∂ L ∂s i is known. Meanwhile the derivation of ∂ L ∂s i requires only numerical values of two sets of representation vectors F = { f ( s 1 ) , f ( s 2 ) , .., f ( s | S | ) } and G = { g ( t 1 ) , g ( t 2 ) , ..., g ( t | T | ) } . A similar argument holds true for g , where we can use representation vectors to compute ∂ L ∂t j and back propagate for each g ( t j ) independently. In the next section, we will describe how to scale up batch size by precomputing these representation vectors.",3.2 Analysis of Computation,2,,,prose,277,532a7f77-3a6a-4362-8fbe-1ad11c69be43,991bf779-5bbb-4ea6-9298-b6bbbf5487cd,2025-12-18T02:00:21.653305,85,main_content,OK,True
991bf779-5bbb-4ea6-9298-b6bbbf5487cd,312060dd-8950-458a-9680-47ea55c0345e,16,"## 3.3 Gradient Cache Technique  
Given a large batch that does not fit into the available GPU memory for training, we first divide it into a set of sub-batches each of which can fit into memory for gradient computation, denoted as S = { ˆ S 1 , ˆ S 2 , .. } , T = { ˆ T 1 , ˆ T 2 , .. } . The full-batch gradient update is computed by the following steps.  
Step1: Graph-less Forward Before gradient computation, we first run an extra encoder forward pass for each batch instance to get its representation. Importantly, this forward pass runs without constructing the computation graph. We collect and store all representations computed.  
Step2: Representation Gradient Computation and Caching We then compute the contrastive loss for the batch based on the representation from Step1 and have a corresponding computation graph constructed. Despite the mathematical derivation, automatic differentiation system is used in actual implementation, which automatically supports variations of contrastive loss. A backward pass is then run to populate gradients for each representation. Note that the encoder is not included in this gradient computation. Let u i = ∂ L ∂f ( s i ) and v i = ∂ L ∂g ( t i ) , we take these gradient tensors and store them as a Representation Gradient Cache , [ u 1 , u 2 , .., v 1 , v 2 , .. ] .  
Step3: Sub-batch Gradient Accumulation We run encoder forward one sub-batch at a time to compute representations and build the corresponding computation graph. We take the sub-batch's representation gradients from the cache and run back propagation through the encoder. Gradients are accumulated for encoder parameters across all sub-batches. Effectively for f we have,",3.3 Gradient Cache Technique,2,,,prose,372,fa2f8558-6900-49d3-820b-19ac8b11c0d9,01b3605c-baed-4229-8bc5-ff09e4d98c51,2025-12-18T02:00:21.653309,85,main_content,OK,True
01b3605c-baed-4229-8bc5-ff09e4d98c51,312060dd-8950-458a-9680-47ea55c0345e,17,$$\frac { \partial \mathcal { L } } { \partial \Theta } & = \sum _ { \hat { S } _ { j } \in \mathbb { S } } \sum _ { s _ { i } \in \hat { S } _ { j } } \frac { \partial \mathcal { L } } { \partial f ( s _ { i } ) } \frac { \partial f ( s _ { i } ) } { \partial \Theta } \\ & = \sum _ { \hat { S } _ { j } \in \mathbb { S } } \sum _ { s _ { i } \in \hat { S } _ { j } } \underline { u } _ { i } \frac { \partial f ( s _ { i } ) } { \partial \Theta } \\$$,3.3 Gradient Cache Technique,2,,,math,185,991bf779-5bbb-4ea6-9298-b6bbbf5487cd,c4fadf73-f1ac-4d90-9b8a-8005ec70f792,2025-12-18T02:00:21.653312,80,main_content,OK,True
c4fadf73-f1ac-4d90-9b8a-8005ec70f792,312060dd-8950-458a-9680-47ea55c0345e,18,"where the outer summation enumerates each subbatch and the entire internal summation corresponds to one step of accumulation. Similarly, for g , gradients accumulate based on,

$$\frac { \partial \mathcal { L } } { \partial \Lambda } = \sum _ { \hat { T } _ { j } \in \mathbb { T } } \sum _ { t _ { i } \in \hat { T } _ { j } } v _ { i } \frac { \partial g ( t _ { i } ) } { \partial \Lambda }$$",3.3 Gradient Cache Technique,2,,,math,122,01b3605c-baed-4229-8bc5-ff09e4d98c51,e937a7b4-75f5-496e-b429-7c2dc43bc6ae,2025-12-18T02:00:21.653315,90,main_content,OK,True
e937a7b4-75f5-496e-b429-7c2dc43bc6ae,312060dd-8950-458a-9680-47ea55c0345e,19,"Here we can see the equivalence with direct large batch update by combining the two summations.  
Step4: Optimization When all sub-batches are processed, we can step the optimizer to update model parameters as if the full batch is processed in a single forward-backward pass.  
Compared to directly updating with the full batch, which requires memory linear to the number of examples, our method fixes the number of examples in each encoder gradient computation to be the size of sub-batch and therefore requires constant memory for encoder forward-backward pass. The extra data pieces introduced by our method that remain persistent across steps are the representations and their corresponding gradients with the former turned into the latter after representation gradient computation. Consequently, in a general case with data from S and T each represented with d dimension vectors, we only need to store ( | S | d + | T | d ) floating points in the cache on top of the computation graph. To remind our readers, this is several orders smaller than million-size model parameters.",3.3 Gradient Cache Technique,2,,,prose,203,c4fadf73-f1ac-4d90-9b8a-8005ec70f792,2d3dd364-d177-4df2-93d4-a03acbacf093,2025-12-18T02:00:21.653319,90,main_content,OK,True
2d3dd364-d177-4df2-93d4-a03acbacf093,312060dd-8950-458a-9680-47ea55c0345e,20,"## 3.4 Multi-GPU Training  
When training on multiple GPUs, we need to compute the gradients with all examples across all GPUs. This requires a single additional cross GPU communication after Step1 when all representations are computed. We use an all-gather operation to make all representations available on all GPUs. Denote F n , G n representations on n -th GPU and a total of N device. Step2 runs with gathered representations F all = F 1 ∪ .. ∪ F N and G all = G 1 ∪ .. ∪ G N . While F all and G all are used  
Table 1: Retrieval: We compare top-5/20/100 hit accuracy of small batch update (Sequential), accumulated small batch (Accumulation) and gradient cache (Cache) systems with DPR reference.  
| Method       | Top-5   |   Top-20 |   Top-100 |
|--------------|---------|----------|-----------|
| DPR          | -       |     78.4 |      85.4 |
| Sequential   | 59.3    |     71.9 |      80.9 |
| Accumulation | 64.3    |     77.2 |      84.9 |
| Cache        | 68.6    |     79.3 |      86   |
| - BSZ = 512  | 68.3    |     79.9 |      86.6 |  
to compute loss, the n -th GPU only computes gradient of its local representations F n , G n and stores them into cache. No communication happens in Step3 , when each GPU independently computes gradient for local representations. Step4 will then perform gradient reduction across GPUs as with standard parallel training.",3.4 Multi-GPU Training,2,,,prose,369,e937a7b4-75f5-496e-b429-7c2dc43bc6ae,c6483e06-baec-4b50-b95f-e217a6b7e598,2025-12-18T02:00:21.653322,85,main_content,OK,True
c6483e06-baec-4b50-b95f-e217a6b7e598,312060dd-8950-458a-9680-47ea55c0345e,21,"## 4 Experiments  
To examine the reliability and computation cost of our method, we implement our method into dense passage retriever (DPR; Karpukhin et al. (2020)) 2 . We use gradient cache to compute DPR's supervised contrastive loss on a single GPU. Following DPRpaper, we measure top hit accuracy on the Natural Question Dataset (Kwiatkowski et al., 2019) for different methods. We then examine the training speed of various batch sizes.",4 Experiments,2,,,prose,102,2d3dd364-d177-4df2-93d4-a03acbacf093,6efd8cb2-d4c5-426a-b967-e31c60070113,2025-12-18T02:00:21.653325,100,main_content,OK,True
6efd8cb2-d4c5-426a-b967-e31c60070113,312060dd-8950-458a-9680-47ea55c0345e,22,"## 4.1 Retrieval Accuracy  
Compared Systems 1) DPR : the reference number taken from the original paper trained on 8 GPUs, 2) Sequential : update with max batch size that fits into 1 GPU, 3) Accumulation : similar to Sequential but accumulate gradients and update until number of examples matches DPR setup, 4) Cache : training with DPR setup using our gradient cache on 1 GPU. We attempted to run with gradient checkpointing but found it cannot scale to standard DPR batch size on our hardware.  
Implementations All runs start with the same random seed and follow DPR training hyperparameters except batch size. Cache uses a batch size of 128 same as DPR and runs with a sub-batch size of 16 for questions and 8 for passages. We also run Cache with a batch size of 512 (BSZ=512) to  
2 Our implementation is at: https://github.com/ luyug/GC-DPR  
Figure 1: We compare training speed versus the number of examples per update for gradient cache (Cache) and gradient accumulation (Accumulation).  
<!-- image -->  
examine the behavior of even larger batches. Sequential uses a batch size of 8, the largest that fits into memory. Accumulation will accumulate 16 of size-8 batches. Each question is paired with a positive and a BM25 negative passage. All experiments use a single RTX 2080ti.  
Results Accuracy results are shown in Table 1. We observe that Cache performs better than DPR reference due to randomness in training. Further increasing batch size to 512 can bring in some advantage at top 20/100. Accumulation and Sequential results confirm the importance of a bigger batch and more negatives. For Accumulation which tries to match the batch size but has fewer negatives, we see a drop in performance which is larger towards the top. In the sequential case, a smaller batch incurs higher variance, and the performance further drops. In summary, our Cache method improves over standard methods and matches the performance of large batch training.",4.1 Retrieval Accuracy,2,,,prose,423,c6483e06-baec-4b50-b95f-e217a6b7e598,088999d5-6ae4-4974-9018-bc0ed0de10dd,2025-12-18T02:00:21.653329,90,main_content,OK,True
088999d5-6ae4-4974-9018-bc0ed0de10dd,312060dd-8950-458a-9680-47ea55c0345e,23,"## 4.2 Training Speed  
In Figure 1, we compare update speed of gradient cache and accumulation with per update example number of { 64 , 128 , 256 , 512 , 1024 , 2048 , 4096 } . We observe gradient cache method can steadily scale up to larger batch update and uses 20% more time for representation pre-computation. This extra cost enables it to create an update of a much larger batch critical for the best performance, as shown by previous experiments and many early works. While the original DPR reports a training time of roughly one day on 8 V100 GPUs, in practice, with improved data loading, our gradient cache code can train a dense retriever in a practical 31 hours on a single RTX2080ti. We also find gradient checkpoint only runs up to batch of 64 and consumes twice the amount of time than accumulation 3 .",4.2 Training Speed,2,,,prose,187,6efd8cb2-d4c5-426a-b967-e31c60070113,96948df5-e493-4d22-b34b-777f0f39ec74,2025-12-18T02:00:21.653332,100,main_content,OK,True
96948df5-e493-4d22-b34b-777f0f39ec74,312060dd-8950-458a-9680-47ea55c0345e,24,"## 5 Extend to Deep Distance Function  
Previous discussion assumes a simple parameterless dot product similarity. In general it can also be deep distance function Φ richly parameterized by Ω , formally,

$$d _ { i j } = d ( s _ { i } , t _ { j } ) = \Phi ( f ( s _ { i } ) , g ( t _ { j } ) ) \quad ( 1 0 ) \quad \mod { 1 } _ { i }$$",5 Extend to Deep Distance Function,2,,,math,106,088999d5-6ae4-4974-9018-bc0ed0de10dd,6770427c-d19a-4ca7-bb93-15faa74890cc,2025-12-18T02:00:21.653335,90,main_content,OK,True
6770427c-d19a-4ca7-bb93-15faa74890cc,312060dd-8950-458a-9680-47ea55c0345e,25,"This can still scale by introducing an extra Distance Gradient Cache . In the first forward we collect all representations as well as all distances. We compute loss with d ij s and back propagate to get w ij = ∂ L ∂d ij , and store them in Distance Gradient Cache, [ w 00 , w 01 , .., w 10 , .. ] . We can then update Ω in a sub-batch manner,",5 Extend to Deep Distance Function,2,,,prose,88,96948df5-e493-4d22-b34b-777f0f39ec74,6b64eec3-0be8-46cb-895f-92124ec332e7,2025-12-18T02:00:21.653338,95,main_content,OK,True
6b64eec3-0be8-46cb-895f-92124ec332e7,312060dd-8950-458a-9680-47ea55c0345e,26,"$$\frac { \partial \mathcal { L } } { \partial \Omega } = \sum _ { \hat { S } \in S \hat { T } \in \mathbb { T } } \sum _ { s _ { i } \in \hat { S } } \sum _ { t _ { j } \in \hat { T } } w _ { i j } \frac { \partial \Phi ( f ( s _ { i } ) , g _ { i } ( t _ { j } ) ) } { \partial \Omega } \\ \intertext { a d d i t i o n d l y w i l l }$$",5 Extend to Deep Distance Function,2,,,math,145,6770427c-d19a-4ca7-bb93-15faa74890cc,48aa005b-4d07-49ac-85a1-60b3d504d74f,2025-12-18T02:00:21.653341,80,main_content,OK,True
48aa005b-4d07-49ac-85a1-60b3d504d74f,312060dd-8950-458a-9680-47ea55c0345e,27,"Additionally, we simultaneously compute with the constructed computation graph ∂d ij ∂f ( s i ) and ∂d ij ∂g ( t j ) and accumulate across batches,

$$u _ { i } = \frac { \partial \mathcal { L } } { \partial f ( s _ { i } ) } = \sum _ { j } w _ { i j } \frac { \partial d _ { i j } } { \partial f ( s _ { i } ) }$$",5 Extend to Deep Distance Function,2,,,math,107,6b64eec3-0be8-46cb-895f-92124ec332e7,6cb04f7a-7ca1-4b93-b864-6af1eff9e704,2025-12-18T02:00:21.653344,90,main_content,OK,True
6cb04f7a-7ca1-4b93-b864-6af1eff9e704,312060dd-8950-458a-9680-47ea55c0345e,28,"and,

$$v _ { j } = \frac { \partial \mathcal { L } } { \partial g ( t _ { j } ) } = \sum _ { i } w _ { i j } \frac { \partial d _ { i j } } { \partial g ( t _ { j } ) }$$",5 Extend to Deep Distance Function,2,,,math,71,48aa005b-4d07-49ac-85a1-60b3d504d74f,ae8d2ece-43ca-4c1e-b0a1-e6ae195f5054,2025-12-18T02:00:21.653346,70,main_content,OK,True
ae8d2ece-43ca-4c1e-b0a1-e6ae195f5054,312060dd-8950-458a-9680-47ea55c0345e,29,"with which we can build up the Representation Gradient Cache. When all representations' gradients are computed and stored, encoder gradient can be computed with Step3 described in subsection 3.3. In philosophy this method links up two caches. Note this covers early interaction f ( s ) = s, g ( t ) = t as a special case.",5 Extend to Deep Distance Function,2,,,prose,69,6cb04f7a-7ca1-4b93-b864-6af1eff9e704,05279a22-e38b-4cf1-8950-45e2e8a2ec30,2025-12-18T02:00:21.653349,100,main_content,OK,True
05279a22-e38b-4cf1-8950-45e2e8a2ec30,312060dd-8950-458a-9680-47ea55c0345e,30,"## 6 Conclusion  
In this paper, we introduce a gradient cache technique that breaks GPU memory limitations for large batch contrastive learning. We propose to construct a representation gradient cache that removes in-batch data dependency in encoder optimization. Our method produces the exact same gradient update as training with a large batch. We show the  
3 We used the gradient checkpoint implemented in Huggingface transformers package  
method is efficient and capable of preserving accuracy on resource-limited hardware. We believe a critical contribution of our work is providing a large population in the NLP community with access to batch-wise contrastive learning. While many previous works come from people with industry-grade hardware, researchers with limited hardware can now use our technique to reproduce state-of-the-art models and further advance the research without being constrained by available GPU memory.",6 Conclusion,2,,,prose,161,ae8d2ece-43ca-4c1e-b0a1-e6ae195f5054,7b727282-ad9b-42a7-bf11-83a412a5f69c,2025-12-18T02:00:21.653352,100,main_content,OK,True
7b727282-ad9b-42a7-bf11-83a412a5f69c,312060dd-8950-458a-9680-47ea55c0345e,31,"## Acknowledgments  
The authors would like to thank Zhuyun Dai and Chenyan Xiong for comments on the paper, and the anonymous reviewers for their reviews.",Acknowledgments,2,,,prose,35,05279a22-e38b-4cf1-8950-45e2e8a2ec30,9148fbc5-7d0e-4ecc-a196-77e5056eccff,2025-12-18T02:00:21.653355,0,metadata,Header contains 'acknowledgment',False
9148fbc5-7d0e-4ecc-a196-77e5056eccff,312060dd-8950-458a-9680-47ea55c0345e,32,"## References  
- Wei-Cheng Chang, Felix X. Yu, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. 2020. Pre-training tasks for embedding-based large-scale retrieval. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net.
- T. Chen, B. Xu, C. Zhang, and Carlos Guestrin. 2016. Training deep nets with sublinear memory cost. ArXiv , abs/1604.06174.
- Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020. A simple framework for contrastive learning of visual representations. ArXiv , abs/2002.05709.
- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.
- John Michael Giorgi, Osvald Nitski, Gary D Bader, and Bo Wang. 2020. Declutr: Deep contrastive learning for unsupervised textual representations. ArXiv , abs/2006.03659.
- Aidan N. Gomez, Mengye Ren, R. Urtasun, and Roger B. Grosse. 2017. The reversible residual network: Backpropagation without storing activations. In NIPS .
- Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 67696781, Online. Association for Computational Linguistics.
- Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. 2020. Supervised contrastive learning. arXiv preprint arXiv:2004.11362 .
- T. Kwiatkowski, J. Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, C. Alberti, D. Epstein, Illia Polosukhin, J. Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Q. Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics , 7:453466.
- Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the
- 57th Annual Meeting of the Association for Computational Linguistics , pages 6086-6096, Florence, Italy. Association for Computational Linguistics.
- Matthew MacKay, Paul Vicol, Jimmy Ba, and Roger B. Grosse. 2018. Reversible recurrent neural networks. In NeurIPS .
- Tomas Mikolov, Ilya Sutskever, Kai Chen, G. Corrado, and J. Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS .
- A. Mnih and Y. Teh. 2012. A fast and simple algorithm for training neural probabilistic language models. In ICML .
- Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2020. Rocketqa: An optimized training approach to dense passage retrieval for opendomain question answering.
- Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models.
- Z. Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun, and Hao Ma. 2020. Clear: Contrastive learning for sentence representation. ArXiv , abs/2012.15466.",References,2,,,prose,978,7b727282-ad9b-42a7-bf11-83a412a5f69c,,2025-12-18T02:00:21.653358,0,metadata,Header contains 'references',False
e5fc4a28-d771-4e42-bd39-e7b6f1966208,655021bc-2dd9-4de8-a93a-85bc3dc612a1,0,"Publish AI, ML &amp; data-science insights to a global community of data professionals.  
LATEST EDITOR'S PICKS DEEP DIVES NEWSLETTER WRITE FOR TDS  
DATA SCIENCE",,,,,prose,42,,3846a551-e2e1-4995-bac9-4619fa5bfb57,2025-12-18T02:47:35.560635,75,main_content,OK,True
3846a551-e2e1-4995-bac9-4619fa5bfb57,655021bc-2dd9-4de8-a93a-85bc3dc612a1,1,"## A/B Testing Like a Pro: Master the Art of Statistical Test Selection  
Real-world data can be tricky! Guide to choosing and applying the Right Statistical Test (using Python)  
Thauri Dattadeen  
17 min read  
Dec 29, 2023  
<!-- image -->  
<!-- image -->  
A/B tests are powerful tools, but choosing the wrong statistical test can lead to misleading results. This guide will help you pick the perfect test for your data, to guarantee reliable analysis and make con fi dent recommendations.  
Just wrapped up your A/B test? The excitement doesn't stop there! The real magic happens when you dive into the data and  
<!-- image -->  
Submit an Article  
<!-- image -->  
<!-- image -->  
unearth valuable insights. This guide equips you, the Data Analyst or Data Scientist, with a systematic approach to analysing your A/ B test results. A big part of the analysis involves understanding the data and the intricate statistical underpinnings in choosing the right test for your data. This step is often overlooked, due to the allure of jumping straight to implementation and potentially missing crucial insights. Depending on what you're analysing, there are a di � erent set of assumptions to make and hence di � erent set of tests to choose. This article guides you through how to choose the 'correct' test for your data.",A/B Testing Like a Pro: Master the Art of Statistical Test Selection,2,,,prose,281,e5fc4a28-d771-4e42-bd39-e7b6f1966208,6192ae9b-418c-4ee8-8c06-1d5925e2ebda,2025-12-18T02:47:35.560653,90,main_content,OK,True
6192ae9b-418c-4ee8-8c06-1d5925e2ebda,655021bc-2dd9-4de8-a93a-85bc3dc612a1,2,"## GUIDE  
Getting right into it. This table focusses on typical Mobile Apps' metrics that a typical objective of an A/B test would focus on, though the principles and assumptions applies throughout. Skip ahead to the necessary section below where I describe each of the metric-types, how to decide the best test for them, and using Python to calculate them!",GUIDE,2,,,prose,71,3846a551-e2e1-4995-bac9-4619fa5bfb57,fcf9bb6f-ed2a-49b1-8dc7-be1e5fcd6b92,2025-12-18T02:47:35.560676,100,main_content,OK,True
fcf9bb6f-ed2a-49b1-8dc7-be1e5fcd6b92,655021bc-2dd9-4de8-a93a-85bc3dc612a1,3,## Section 1: Average per User Metrics Section 2: Categorical Metrics Section 3: Joint Metrics,Section 1: Average per User Metrics Section 2: Categorical Metrics Section 3: Joint Metrics,2,,,prose,22,6192ae9b-418c-4ee8-8c06-1d5925e2ebda,5cbc3a36-e49e-49aa-bf58-3fa3976790b1,2025-12-18T02:47:35.560683,75,main_content,OK,True
5cbc3a36-e49e-49aa-bf58-3fa3976790b1,655021bc-2dd9-4de8-a93a-85bc3dc612a1,4,"## Before we start, Basic De fi nitions:","Before we start, Basic De fi nitions:",2,,,prose,11,fcf9bb6f-ed2a-49b1-8dc7-be1e5fcd6b92,00046930-148e-4ecf-a27b-c5c98fccb935,2025-12-18T02:47:35.560691,75,main_content,OK,True
00046930-148e-4ecf-a27b-c5c98fccb935,655021bc-2dd9-4de8-a93a-85bc3dc612a1,5,"## Null Hypothesis:  
Each Hypothesis test is made up of a ""theory"" to test, this is called the Null Hypothesis (H ₀ ) . The goal of our analysis is to try to con fi dently prove whether this hypothesis is true or not. The Null Hypothesis (H ₀ ) assumes that there is no di � erence between the 2 groups, ie, the feature has no impact. E.g. H ₀ : μ 1 = μ 2 HA : μ 1 ≠ μ 2",Null Hypothesis:,2,,,prose,115,5cbc3a36-e49e-49aa-bf58-3fa3976790b1,052673ab-3819-4413-b621-70e6081261a5,2025-12-18T02:47:35.560698,90,main_content,OK,True
052673ab-3819-4413-b621-70e6081261a5,655021bc-2dd9-4de8-a93a-85bc3dc612a1,6,"## Signi fi cance Level (Alpha):  
The signi fi cance level, or alpha ( α ), is a measure used to decide if the results of the test are statistically signi fi cant or not. This forms part of the base assumptions made before running the test. In simple terms, the signi fi cance level helps you determine if your fi ndings are reliable and not just a fl uke. It acts as a threshold for saying, ""Hey, this is probably real, not just a coincidence.""",Signi fi cance Level (Alpha):,2,,,prose,108,00046930-148e-4ecf-a27b-c5c98fccb935,454869d7-6505-4fa0-acf1-d7479d994941,2025-12-18T02:47:35.560706,95,main_content,OK,True
454869d7-6505-4fa0-acf1-d7479d994941,655021bc-2dd9-4de8-a93a-85bc3dc612a1,7,"## P-Value:  
The p-value, or probability value, is a measure used in statistics to help determine the strength of evidence against a null hypothesis. In simple terms, the p-value is related to the probability of getting a false positive result, ie, the probability that the data we got is due to chance and we make a Type I Error. If the p-value is high, we cannot trust the data as the likelihood of landing on a false positive result is high. If the data is not reliable (ie, the p-value is high), we cannot con fi dently prove or disprove H ₀ . P-value &lt; Signi fi cance Level → Reject H ₀ . We have su � cient data to conclude that the two groups are signi fi cantly di � erent P-value &gt;Signi fi cance Level → Do not Reject H ₀ . We do not have su � cient data to conclude that the two groups are signi fi cantly di � erent",P-Value:,2,,,prose,208,052673ab-3819-4413-b621-70e6081261a5,9686685e-6099-48ce-8187-65d8fb5bbc02,2025-12-18T02:47:35.560720,95,main_content,OK,True
9686685e-6099-48ce-8187-65d8fb5bbc02,655021bc-2dd9-4de8-a93a-85bc3dc612a1,8,"## Selecting the metric to analyse  
Before we begin, it's important to understand what you're trying to analyse from your A/B test. For example, if you are revamping the fi rst-time user experience (FTUE)?, you'd likely be interested in metrics like user retention or conversion. These typically involve yes/no (1 or 0) outcomes, making the ""Two Proportion Z Test"" a good fi t. We'll explore di � erent types of metrics mentioned in the guide above, explaining why they might be chosen and how to ensure your data aligns with the test's requirements.",Selecting the metric to analyse,2,,,prose,124,454869d7-6505-4fa0-acf1-d7479d994941,d0f605e1-53ac-4757-b613-33fafdc19225,2025-12-18T02:47:35.560728,100,main_content,OK,True
d0f605e1-53ac-4757-b613-33fafdc19225,655021bc-2dd9-4de8-a93a-85bc3dc612a1,9,"## Setting up your data  
I have assumed that you already have your data and it is in the format of a table with 3 columns: Unique ID, Variant, Metric.

```
#Separate the data into 2 group_A = df[df['Variant'] == ""A""][""Metric""] group_B = df[df['Variant'] == ""B""][""Metric""] #Change ""A"" and ""B"" to the relevant groups in your dataset #Change ""Metric"" to the relevant column name with the values
```",Setting up your data,2,,,code,105,9686685e-6099-48ce-8187-65d8fb5bbc02,56f51155-5f57-4408-af5d-c245ec8daf33,2025-12-18T02:47:35.560736,90,main_content,OK,True
56f51155-5f57-4408-af5d-c245ec8daf33,655021bc-2dd9-4de8-a93a-85bc3dc612a1,10,"## Section 1: Average Per User Metrics  
This is the most common type of metrics to analyse. It involves independent samples of the data.  
In most real-world cases, Avg per User metrics, such as Average Revenue per User or Average Time Spent per User is very common to analyse. If you have a large enough sample size, you could just skip ahead and choose the Welch's t-test.  
I'll now go through each fl ow and describe the steps to decide which path to use -",Section 1: Average Per User Metrics,2,,,prose,102,d0f605e1-53ac-4757-b613-33fafdc19225,08c98c64-4136-4ff9-92cd-e93b27b98fca,2025-12-18T02:47:35.560742,95,main_content,OK,True
08c98c64-4136-4ff9-92cd-e93b27b98fca,655021bc-2dd9-4de8-a93a-85bc3dc612a1,11,"## Flow 1: Large Sample or Normally Distributed:  
My general assumption for a 'Large Sample' is typically where the number of individual samples is greater than 10,000, though this de fi nition is relative and may di � er based on various factors, such as the speci fi c fi eld of study, the type of analysis being performed, and the e � ect size that needs to be detected.  
If n is relatively small, then perform a Normality Test to determine whether you should select a T-Test. There are several ways to test for normality. The easiest way to check is by creating a histogram and visually inspecting the data. If this roughly shapes a Normal Distribution, then continue on. If still unsure, it's best to do a more statistical test, such as the Shapiro-Wilkes test for Normality. Here is a great article about the various ways to test your data for normality. Keep in mind that each statistical test usually makes various assumptions on your data, so keep this in mind before selecting the noramlity right test.  
See the code snippet below showcasing how to test your data for Normality through either visual inspection by creating a histogram or using a statistical test called the Shapiro-Wilkes test.",Flow 1: Large Sample or Normally Distributed:,2,,,prose,258,56f51155-5f57-4408-af5d-c245ec8daf33,fb445dd6-ca5c-4426-bd5f-7573cb986939,2025-12-18T02:47:35.560749,90,main_content,OK,True
fb445dd6-ca5c-4426-bd5f-7573cb986939,655021bc-2dd9-4de8-a93a-85bc3dc612a1,12,"```
import pandas as pd import matplotlib.pyplot as plt import scipy.stats as stats alpha=0.05 #We assume an alpha here of 0.05 Test for Normality
```

```
# Creating Histograms grouped = df.groupby('Variant') #Variant column identifies the group of the user - group_A / group_B etc # Plotting histograms for each group to visually inspect the shape of the data for name, group in grouped: plt.hist(group['Metric']) plt.title(name) plt.show() # Shapiro-Wilkes Test to statistically test for normality for name, group in grouped: shapiro_test = stats.shapiro(group['Value']) print(f""Shapiro-Wilk Test for Group {name}: W = {shapiro_test[0]}, p-value = {shapiro_test[1]}"") if p_value &lt; alpha: print(""Reject the null hypothesis. There is a significant difference between the groups."") else: print(""Fail to reject the null hypothesis. There is no significant difference between the groups."")
```",Flow 1: Large Sample or Normally Distributed:,2,,,code,211,08c98c64-4136-4ff9-92cd-e93b27b98fca,2fa28fa1-12f6-4de0-95e7-860380fe2eda,2025-12-18T02:47:35.560755,95,main_content,OK,True
2fa28fa1-12f6-4de0-95e7-860380fe2eda,655021bc-2dd9-4de8-a93a-85bc3dc612a1,13,## Flow 2a: Equal Variances?,Flow 2a: Equal Variances?,2,,,prose,10,fb445dd6-ca5c-4426-bd5f-7573cb986939,09b76ba1-c39d-496b-962a-194a468c1fa9,2025-12-18T02:47:35.560762,0,metadata,Empty or image-only,False
09b76ba1-c39d-496b-962a-194a468c1fa9,655021bc-2dd9-4de8-a93a-85bc3dc612a1,14,"## Following on from the Average Per User fl owchart above -  
If you have con fi rmed that your data conforms to Normality, then the next step is to check whether the datasets have equal variances. This determines whether you should use the Welch's ttest or the Student's t-test.  
The main di � erences between Welch's t-test and Student's t-test are around the degrees of freedom and sample variance estmates. Student's assumes that both samples have the same variance, while Welch's does not.  
When comparing large sample sizes (n &gt; 10,000) for a hypothesis test using either Welch's t-test or Student's t-test, the di � erence in signi fi cance levels between the two tests is typically negligible. This is because the Student's t-test assumption of equal variances has minimal impact on the test's accuracy when dealing with large sample sizes.  
Even if the assumption of equal variances is violated, the Student's t-test remains relatively robust, meaning it produces accurate p-values and maintains the desired type I error rate (the probability of rejecting a true null hypothesis). This robustness is due to the Central Limit Theorem, which states that as the sample size increases, the distribution of sample means approaches a normal distribution, regardless of the underlying population distribution.  
In contrast, Welch's t-test is speci fi cally designed to handle unequal variances, making it more appropriate when the assumption of equal variances is questionable. However, for large sample sizes, the di � erence in signi fi cance levels between  
Welch's t-test and Student's t-test is usually minimal.  
If you are concerned about the potential for unequal variances, Welch's t-test is a safer choice. However, if you want to maximise power and are con fi dent that the sample size is large enough, Student's t-test can be used.  
See the code snippet below showcasing how to test your data for equal variances using the Bartlett's test. Bartlett's test is a very robust test that requires the assumption of normality. If you would prefer a less robust test, then Levene's test may be more appropriate.",Following on from the Average Per User fl owchart above -,2,,,prose,453,2fa28fa1-12f6-4de0-95e7-860380fe2eda,5fd8b300-35c9-4b47-92ce-72cbf141690b,2025-12-18T02:47:35.560768,80,main_content,OK,True
5fd8b300-35c9-4b47-92ce-72cbf141690b,655021bc-2dd9-4de8-a93a-85bc3dc612a1,15,"## Test for Equal Variances

```
from scipy.stats import bartlett, levene # Perform Bartlett's test for equal variances (works best on data that conforms to normality) statistic, p_value = bartlett(group_A, group_B) # Perform Levene's test for equal variances (less sensitive to Normality assumption) statistic, p_value = levene(group_A, group_B) # Display test results print(f""Test statistic: {statistic}"") print(f""P-value: {p_value}"") if p_value &lt; alpha: print(""Reject the null hypothesis. There is a significant difference in variances between the groups."") else: print(""Fail to reject the null hypothesis. There is no significant difference in variances between the groups."")
```",Test for Equal Variances,2,,,code,158,09b76ba1-c39d-496b-962a-194a468c1fa9,68aa0c7f-aeef-46b6-b56e-61dfa1c0c778,2025-12-18T02:47:35.560775,95,main_content,OK,True
68aa0c7f-aeef-46b6-b56e-61dfa1c0c778,655021bc-2dd9-4de8-a93a-85bc3dc612a1,16,## Flow 2b: Median or Mean?,Flow 2b: Median or Mean?,2,,,prose,10,5fd8b300-35c9-4b47-92ce-72cbf141690b,4de08c66-d054-48ef-a252-3db8b7042719,2025-12-18T02:47:35.560781,0,metadata,Empty or image-only,False
4de08c66-d054-48ef-a252-3db8b7042719,655021bc-2dd9-4de8-a93a-85bc3dc612a1,17,"## Following on from the Average Per User fl owchart above -  
If you have reasonable doubts that your dataset conforms to Normality, then another statistical method might be more appropriate to analyse your dataset. The next step is to decide whether the Mean or Median of your data is more useful.  
Considering the median over the mean in an A/B test can be bene fi cial in speci fi c scenarios, primarily when dealing with data that might be a � ected by outliers or very skewed, non-normal distributions.  
- Communicating Results: Using the median can o � er a clearer and more intuitive interpretation of central tendency, especially when describing the typical or typical-per-user behavior. It might be more relatable for stakeholders or nontechnical audiences.
- Skewed Distributions: If your data is highly skewed or does
- not follow a normal distribution, the mean might not accurately represent the typical value. In such cases, the median provides a more robust estimate of central tendency as it's less in fl uenced by extreme values or the shape of the distribution.
- Outlier Sensitivity: The mean is highly sensitive to outliers or extreme values in a dataset. Even a few outliers can signi fi cantly skew the mean, impacting its representativeness of the central tendency. In contrast, the median is less a � ected by extreme values since it represents the middle value in the dataset when arranged in ascending order.  
Both measures have their merits, and choosing between them should align with the nature of your data, the e � ect of outliers, and the speci fi c insights you aim to derive from your A/B test. You should consider both the mean and median when coming to a conclusion.  
This is a very useful guide to the Mann-Whitney U Test. As always, it is best to do research to thoroughly understand each test before jumping into it!",Following on from the Average Per User fl owchart above -,2,,,prose,385,68aa0c7f-aeef-46b6-b56e-61dfa1c0c778,2735b551-cacf-4271-bb3b-1fe88d7a20e4,2025-12-18T02:47:35.560788,90,main_content,OK,True
2735b551-cacf-4271-bb3b-1fe88d7a20e4,655021bc-2dd9-4de8-a93a-85bc3dc612a1,18,"## Statistical Tests  
If you've followed the Average Per User fl owchart above, you would have now decided what's the best test to determine whether the 2 groups' metrics are statistically signi fi cant di � erent. See how to perform them below.  
Refer to the GUIDE and the Average Per User fl owchart above.",Statistical Tests,2,,,prose,69,4de08c66-d054-48ef-a252-3db8b7042719,6d9c8b8d-03be-472d-bb85-7b9b2564494f,2025-12-18T02:47:35.560794,100,main_content,OK,True
6d9c8b8d-03be-472d-bb85-7b9b2564494f,655021bc-2dd9-4de8-a93a-85bc3dc612a1,19,"## Student's t-test

```
import scipy.stats # Student's t-test - This test requires Normality and Equal Variances t_statistic, p_value = stats.ttest_ind(group_A, group_B) print(f""Student's t-test: t = {t_statistic}, p-value = {p_value}"") if p_value &lt; alpha: print(""Reject the null hypothesis. There is a significant difference between the groups."") else: print(""Fail to reject the null hypothesis. There is no significant difference between the groups."") t_statistic, p_value = stats.ttest_ind(group_A, group_B, equal_var=False) Welch's t-test
```",Student's t-test,2,,,code,136,2735b551-cacf-4271-bb3b-1fe88d7a20e4,a1959ae5-2f59-4c46-ad94-d0c6cad1690b,2025-12-18T02:47:35.560801,95,main_content,OK,True
a1959ae5-2f59-4c46-ad94-d0c6cad1690b,655021bc-2dd9-4de8-a93a-85bc3dc612a1,20,"```
import scipy.stats # Welch's t-test - This test requires Normality print(f""Welch's t-test: t = {t_statistic}, p-value = {p_value}"") if p_value &lt; alpha:
```

```
print(""Reject the null hypothesis. There is a significant difference between the groups."") else: print(""Fail to reject the null hypothesis. There is no significant difference between the groups."") # Mann-Whitney U-test - No statistical assumptions, Median preferred over Mean u_statistic, p_value = stats.mannwhitneyu(group_A, group_B) print(f""Mann-Whitney U-test: U = {u_statistic}, p-value = {p_value}"") if p_value &lt; alpha: print(""Reject the null hypothesis. There is a significant difference between the groups."") else: print(""Fail to reject the null hypothesis. There is no significant difference between the groups."") #Bootstrapping - forNon-Normal data/Small sample sizes, and Mean is preferred Mann-Whitney U-test Bootstrapping
```",Student's t-test,2,,,code,224,6d9c8b8d-03be-472d-bb85-7b9b2564494f,8a9de546-d021-4607-a224-1df0e8d4f088,2025-12-18T02:47:35.560807,95,main_content,OK,True
8a9de546-d021-4607-a224-1df0e8d4f088,655021bc-2dd9-4de8-a93a-85bc3dc612a1,21,"```
# Calculate observed difference in means observed_diff = np.mean(group_B) - np.mean(group_A) # Combined data combined_data = np.concatenate((group_A, group_B)) # Number of bootstrap samples num_samples = 10000  # You can adjust this number based on computational resources # Bootstrap resampling bootstrap_diffs = [] for _ in range(num_samples): # Resample with replacement bootstrap_sample = np.random.choice(combined_data, size=len(combined_data), replace=True) # Calculate difference in means for each bootstrap sample bootstrap_mean_A = np.mean(bootstrap_sample[len(group_A):]) bootstrap_mean_B = np.mean(bootstrap_sample[:len(group_A)]) bootstrap_diff = bootstrap_mean_B - bootstrap_mean_A bootstrap_diffs.append(bootstrap_diff) # Calculate p-value (significance level) p_value = np.mean(np.abs(bootstrap_diffs) &gt;= np.abs(observed_diff)) print(f""P-value: {p_value}"") if p_value &lt; alpha: print(""Reject the null hypothesis. There is a significant difference between the groups."") else: print(""Fail to reject the null hypothesis. There is no significant difference between the groups."")
```",Student's t-test,2,,,code,239,a1959ae5-2f59-4c46-ad94-d0c6cad1690b,ddcdccbc-cc14-49cd-b9f3-5703556f1d21,2025-12-18T02:47:35.560813,85,main_content,OK,True
ddcdccbc-cc14-49cd-b9f3-5703556f1d21,655021bc-2dd9-4de8-a93a-85bc3dc612a1,22,"## Section 2: Categorical Variables  
In this section, we will explore Metrics that are categorical. These metrics can be Discrete, such as Clicked/not Clicked, or Continuous, in the case of multivariate tests.  
Follow the Flowchart above to select the best test for your data.",Section 2: Categorical Variables,2,,,prose,62,8a9de546-d021-4607-a224-1df0e8d4f088,e0dd023b-941e-4b3a-be4b-976f1781d426,2025-12-18T02:47:35.560819,100,main_content,OK,True
e0dd023b-941e-4b3a-be4b-976f1781d426,655021bc-2dd9-4de8-a93a-85bc3dc612a1,23,## 2 Groups,2 Groups,2,,,prose,4,ddcdccbc-cc14-49cd-b9f3-5703556f1d21,72ef2abd-6b13-4a09-95f2-6778f8ff405f,2025-12-18T02:47:35.560825,0,metadata,Empty or image-only,False
72ef2abd-6b13-4a09-95f2-6778f8ff405f,655021bc-2dd9-4de8-a93a-85bc3dc612a1,24,"## Two Proportion Z-test (Binary Metrics)  
These are metrics like Retention, Conversion, Clicked etc.  
The two-sample z-test for binomial variables in A/B testing compares the proportions of binary outcomes between two groups. From a statistical viewpoint, a binomial distribution converges to a normal distribution as N gets large. This assumption generally holds quite well, so it makes sense to use a z-test for this.",Two Proportion Z-test (Binary Metrics),2,,,prose,87,e0dd023b-941e-4b3a-be4b-976f1781d426,206f4104-2ed4-47ec-95eb-7837df34478c,2025-12-18T02:47:35.560835,100,main_content,OK,True
206f4104-2ed4-47ec-95eb-7837df34478c,655021bc-2dd9-4de8-a93a-85bc3dc612a1,25,"```
H0: μ 1 μ 2 = 0 HA: μ 1 μ 2 ≠ 0 from statsmodels.stats.weightstats import ztest # Calculate the z-statistic and p-value. This assumes binomially distributed and i.i.d. variables. z_stat, p_value = ztest(group_A, group_B) print(f""Two Sample z-test: t = {z_stat}, p-value = {p_value}"") if p_value &lt; alpha: print(""Reject the null hypothesis. There is a significant difference between the groups."") else: print(""Fail to reject the null hypothesis. There is no significant difference between the groups."") Two Proportion Z-test
```",Two Proportion Z-test (Binary Metrics),2,,,code,143,72ef2abd-6b13-4a09-95f2-6778f8ff405f,074042d1-33c0-43ea-89f2-6bfd957003a9,2025-12-18T02:47:35.560842,95,main_content,OK,True
074042d1-33c0-43ea-89f2-6bfd957003a9,655021bc-2dd9-4de8-a93a-85bc3dc612a1,26,## 3+ Groups,3+ Groups,2,,,prose,5,206f4104-2ed4-47ec-95eb-7837df34478c,e969c1d3-0f12-41c3-aa86-87f6f3440710,2025-12-18T02:47:35.560848,0,metadata,Empty or image-only,False
e969c1d3-0f12-41c3-aa86-87f6f3440710,655021bc-2dd9-4de8-a93a-85bc3dc612a1,27,## Discrete Variables,Discrete Variables,2,,,prose,4,074042d1-33c0-43ea-89f2-6bfd957003a9,afb83a20-2397-456b-b5a5-cd740b8ad061,2025-12-18T02:47:35.560854,0,metadata,Empty or image-only,False
afb83a20-2397-456b-b5a5-cd740b8ad061,655021bc-2dd9-4de8-a93a-85bc3dc612a1,28,"## Pearson's Chi-Squared Test  
The chi-squared test is another powerful tool for analyzing A/B tests, particularly when you have multiple groups in addition to a control group.  
It allows you to compare multiple variants simultaneously, without assuming any distributional properties. This method will also work with similar binary variables as above, but using multiple groups instead of just 2. As these groups are likely splitting the sample size into smaller groups, it is important to ensure that the sample sizes for each group remain relatively large.  
Pearson's Chi Sq Test is utilised to determine if there's a signi fi cant association or di � erence between observed and expected frequencies of categorical data among multiple groups. Therefore, the Null Hypothesis assumes that there is no di � erence between the groups.  
As the data is discrete, we create a contingency table to sum the counts across each Variant. This is then interpreted by the stats.chi2\_contingency function.  
## Pearson's Chi-Squared Test",Pearson's Chi-Squared Test,2,,,prose,206,e969c1d3-0f12-41c3-aa86-87f6f3440710,b5d0a6f2-815d-4bee-a06f-68c6fd8d41e4,2025-12-18T02:47:35.560860,85,main_content,OK,True
b5d0a6f2-815d-4bee-a06f-68c6fd8d41e4,655021bc-2dd9-4de8-a93a-85bc3dc612a1,29,"```
# Create a contingency table contingency_table = pd.crosstab(df['Variant'], df[""metric""]) # Perform the chi-squared test chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table) if p_value &lt; alpha: print(""There is a statistically significant difference in the distribution of the metric across groups."") else: print(""There is no statistically significant difference in the distribution of the metric across groups."")
```",Pearson's Chi-Squared Test,2,,,code,96,afb83a20-2397-456b-b5a5-cd740b8ad061,919d703a-6ca4-45ba-84e2-2f61ef89a35e,2025-12-18T02:47:35.560866,95,main_content,OK,True
919d703a-6ca4-45ba-84e2-2f61ef89a35e,655021bc-2dd9-4de8-a93a-85bc3dc612a1,30,## Continuous Variables,Continuous Variables,2,,,prose,3,b5d0a6f2-815d-4bee-a06f-68c6fd8d41e4,e4b2ae3b-6eb5-4a87-8175-d23b78aac064,2025-12-18T02:47:35.560872,0,metadata,Empty or image-only,False
e4b2ae3b-6eb5-4a87-8175-d23b78aac064,655021bc-2dd9-4de8-a93a-85bc3dc612a1,31,"## ANOVA Test  
ANOVA is a statistical test used to compare the means of three or more groups. It assesses whether the observed di � erences between group means are likely due to chance or actually represent signi fi cant di � erences in their underlying populations. It is useful when releasing multiple di � erent variants and want to compare the results against each other to save time rather than deploying individual A/B tests.  
ANOVA is relatively robust to violations of normality and variance homogeneity assumptions, especially when sample sizes are reasonably large.  
## ANOVA Test",ANOVA Test,2,,,prose,119,919d703a-6ca4-45ba-84e2-2f61ef89a35e,e169e330-23c5-4d23-8b03-acd4176a5217,2025-12-18T02:47:35.560881,95,main_content,OK,True
e169e330-23c5-4d23-8b03-acd4176a5217,655021bc-2dd9-4de8-a93a-85bc3dc612a1,32,"```
# Group the counts for the various groups in the data grouped_data = [df[df['Variant'] == cat]['Metric'] for cat in df['Variant'].unique()] # Perform ANOVA test f_statistic, p_value = stats.f_oneway(*grouped_data) if p_value &lt; alpha: print(""There is a statistically significant difference in the means of the metric across groups."") else: print(""There is no statistically significant difference in the means of the metric across groups."")
```",ANOVA Test,2,,,code,103,e4b2ae3b-6eb5-4a87-8175-d23b78aac064,e05eb5a7-8967-4865-b209-52fdd441ea7a,2025-12-18T02:47:35.560889,95,main_content,OK,True
e05eb5a7-8967-4865-b209-52fdd441ea7a,655021bc-2dd9-4de8-a93a-85bc3dc612a1,33,"This test determines whether the groups are statistically signi fi cant. One problem we encounter here is trying to identify a particular group that signi fi cantly outperforms the rest. While the collective ANOVA test highlights general deviations, a deeper investigation is needed to identify speci fi c groups showcasing statistical di � erences over the other groups.  
Fortunately, there is a neat function which uses Tukey's range test o � ering a structured approach. It generates a comprehensive pairwise comparison table across all group combinations, unveiling statistically signi fi cant di � erences among them.  
However, exercising caution is imperative due to potential violations of underlying assumptions in the Turkey's range test. This test should primarily be used to identify distinct groups, utilizing it as a supportive tool only in group comparisons. This is really useful video showing how it is performed and used.  
See the code snippet below which should be used as an aid to the ANOVA test above to identify the particular groups that outperforms the rest.",ANOVA Test,2,,,prose,207,e169e330-23c5-4d23-8b03-acd4176a5217,2aa5dfd1-1e54-49e3-aea1-642f3416e5af,2025-12-18T02:47:35.560900,90,main_content,OK,True
2aa5dfd1-1e54-49e3-aea1-642f3416e5af,655021bc-2dd9-4de8-a93a-85bc3dc612a1,34,"## Turkey's HSD test

```
import statsmodels.api as sm from statsmodels.stats.multicomp import pairwise_tukeyhsd # Perform Tukey's HSD test for post-hoc analysis tukey = pairwise_tukeyhsd(df[""metric""], df['Variant']) # Print pairwise comparison results print(tukey.summary())
```",Turkey's HSD test,2,,,code,71,e05eb5a7-8967-4865-b209-52fdd441ea7a,7ef35be7-a76b-4958-b579-6de34d2a7797,2025-12-18T02:47:35.560911,95,main_content,OK,True
7ef35be7-a76b-4958-b579-6de34d2a7797,655021bc-2dd9-4de8-a93a-85bc3dc612a1,35,"## Section 3: Joint Metrics  
Following the GUIDE above, if you've determined that your metric is actually a joint metric of 2 or more variables, then you may need to take additional steps to e � ectively determine the statistical di � erences between your various groups. This is because the other tests above assume that the metrics you are testing are independent of one another.",Section 3: Joint Metrics,2,,,prose,77,2aa5dfd1-1e54-49e3-aea1-642f3416e5af,878adec1-fa9f-4b16-88f4-35f0911d2b5d,2025-12-18T02:47:35.560918,100,main_content,OK,True
878adec1-fa9f-4b16-88f4-35f0911d2b5d,655021bc-2dd9-4de8-a93a-85bc3dc612a1,36,"## Delta t-test  
The Delta t-test is a statistical method employed to assess the di � erence in means between two independent groups while considering the ratio of two random variables forming a joint distribution. In the realm of A/B testing, there are often scenarios where the metrics itself may not be independent.  
An example of this is Ad Click-Through-Rate. A person may view the same Advertisement multiple times, but only click on it once.  
The problem with using the standard t-tests here is that we're analysing 2 separate random variables and their ratio forms a joint distribution. The subjects themselves are independent, however, their joint ratio is not. This violates the assumptions of independence of the Student's and Welch's t-test.  
Instead, we use the Delta Method to estimate the Variance;  
See the code snippet below showing how to calculate this new Variance formula, as well as creating a t-test function using this new variance.  
## Delta t-test",Delta t-test,2,,,prose,198,7ef35be7-a76b-4958-b579-6de34d2a7797,247d4eac-0c8d-4317-98e5-f74899d7e82a,2025-12-18T02:47:35.560927,95,main_content,OK,True
247d4eac-0c8d-4317-98e5-f74899d7e82a,655021bc-2dd9-4de8-a93a-85bc3dc612a1,37,"```
# create the new Variance function as described above def var_ratio(metric1,metric2): mean_x = np.mean(metric1) mean_y = np.mean(metric2) var_x = np.var(metric1,ddof=1) var_y = np.var(metric2,ddof=1) cov_xy = np.cov(metric1,metric2,ddof=1)[0][1] result = (mean_x**2 / mean_y**2) * (var_x/mean_x**2 - 2*cov_xy/ (mean_x*mean_y) + var_y/mean_y**2) return result # create this new ttest function, using the new Variances above. This is a standard t-test function. def delta_ttest(mean_c,mean_t,var_c,var_t, alpha = 0.05): mean_diff = mean_t - mean_c var = var_c + var_t std_e = stats.norm.ppf(1 - alpha/2) * np.sqrt(var) lower_ci = mean_diff - std_e upper_ci = mean_diff + std_e z = mean_diff/np.sqrt(var) p_val = stats.norm.sf(abs(z))*2 return z, p_val, upper_ci, lower_ci #Eg. Here we calculate the significance of the CTR for a control and treatment group. var_c = var_ratio(control['click'],control['view']) #Calculates the delta variance for the control group var_t = var_ratio(treatment['click'],treatment['view']) #Calculates the delta variance for the treatment group mean_c = control['click'].sum()/control['view'].sum() mean_t= treatment['click'].sum()/treatment['view'].sum() z, p_value, upper_ci, lower_ci = delta_ttest(mean_c,mean_t,var_c,var_t,alpha) #Applies the ttestusing these new delta variances if p_value &lt; alpha: print(""Reject the null hypothesis. There is a significant difference between the groups."") else: print(""Fail to reject the null hypothesis. There is no significant difference between the groups."")
```",Delta t-test,2,,,code,436,878adec1-fa9f-4b16-88f4-35f0911d2b5d,23b0fc5f-86a8-420f-a898-63faff686e4e,2025-12-18T02:47:35.560934,80,main_content,OK,True
23b0fc5f-86a8-420f-a898-63faff686e4e,655021bc-2dd9-4de8-a93a-85bc3dc612a1,38,"## Conclusion:  
In summary, while A/B tests are invaluable for experimentation and optimisation, the choice of the right statistical test is essential. For robust and reliable results, data scientists should carefully consider the characteristics and test assumptions.  
Remember, A/B tests are powerful tools, but choosing the wrong statistical test can lead to misleading results!",Conclusion:,2,,,prose,68,247d4eac-0c8d-4317-98e5-f74899d7e82a,1d04aa14-6866-4a91-8394-c81c0ac30e1b,2025-12-18T02:47:35.560941,100,main_content,OK,True
1d04aa14-6866-4a91-8394-c81c0ac30e1b,655021bc-2dd9-4de8-a93a-85bc3dc612a1,39,"## References :  
1. Test for Normality by Sivasai Yadav Mudugandla
2. Levene's Test by Kyaw Saw Htoon
3. Mann Whitney U Test by Ricardo Lara Jácome
4. ��� Delta Method: https:/ /arxiv.org/pdf/1803.06336.pdf
5. Delta T-Test by Ahmad Nur Aziz  
·  
·  
WRITTEN BY  
<!-- image -->",References :,2,,,prose,95,23b0fc5f-86a8-420f-a898-63faff686e4e,7eba3b26-4810-44ea-8279-c72af4868377,2025-12-18T02:47:35.560947,0,metadata,Header contains 'references',False
7eba3b26-4810-44ea-8279-c72af4868377,655021bc-2dd9-4de8-a93a-85bc3dc612a1,40,"## Related Articles  
·  
<!-- image -->  
<!-- image -->  
<!-- image -->  
ARTIFICIAL INTELLIGENCE",Related Articles,2,,,prose,25,1d04aa14-6866-4a91-8394-c81c0ac30e1b,476392da-3d1e-4ef2-9a06-3b651084e44f,2025-12-18T02:47:35.560954,80,main_content,OK,True
476392da-3d1e-4ef2-9a06-3b651084e44f,655021bc-2dd9-4de8-a93a-85bc3dc612a1,41,"## Implementing Convolutional Neural Networks in TensorFlow  
Step-by-step code guide to building a Convolutional Neural Network  
Shreya Rao  
August 20, 2024 6 min read  
DATA SCIENCE",Implementing Convolutional Neural Networks in TensorFlow,2,,,prose,45,7eba3b26-4810-44ea-8279-c72af4868377,4e8ee281-b309-47da-bd36-fdf1df1db150,2025-12-18T02:47:35.560962,85,main_content,OK,True
4e8ee281-b309-47da-bd36-fdf1df1db150,655021bc-2dd9-4de8-a93a-85bc3dc612a1,42,"## Solving a Constrained Project Scheduling Problem with Quantum Annealing  
Solving the resource constrained project scheduling problem (RCPSP) with DWave's hybrid constrained quadratic model (CQM)  
Luis Fernando PÉREZ ARMAS, Ph.D.  
August 20, 2024  
29 min read  
DATA SCIENCE  
<!-- image -->",Solving a Constrained Project Scheduling Problem with Quantum Annealing,2,,,prose,74,476392da-3d1e-4ef2-9a06-3b651084e44f,0e56801c-b193-44f3-9b81-78b599863276,2025-12-18T02:47:35.560969,95,main_content,OK,True
0e56801c-b193-44f3-9b81-78b599863276,655021bc-2dd9-4de8-a93a-85bc3dc612a1,43,"## Must-Know in Statistics: The Bivariate Normal Projection Explained  
Derivation and practical examples of this powerful concept  
Luigi Battistoni  
August 14, 2024  
7 min read  
DATA SCIENCE  
<!-- image -->",Must-Know in Statistics: The Bivariate Normal Projection Explained,2,,,prose,50,4e8ee281-b309-47da-bd36-fdf1df1db150,d1b9de4a-88ad-457c-ba05-f88d75ac21a8,2025-12-18T02:47:35.560974,95,main_content,OK,True
d1b9de4a-88ad-457c-ba05-f88d75ac21a8,655021bc-2dd9-4de8-a93a-85bc3dc612a1,44,"## Hands-on Time Series Anomaly Detection using Autoencoders, with Python  
Here's how to use Autoencoders to detect signals with anomalies in a few lines of…  
Piero Paialunga  
August 21, 2024  
12 min read  
DATA SCIENCE  
<!-- image -->","Hands-on Time Series Anomaly Detection using Autoencoders, with Python",2,,,prose,61,0e56801c-b193-44f3-9b81-78b599863276,62fba649-8a38-4cb9-9bb8-b5fe7a50969b,2025-12-18T02:47:35.560980,95,main_content,OK,True
62fba649-8a38-4cb9-9bb8-b5fe7a50969b,655021bc-2dd9-4de8-a93a-85bc3dc612a1,45,"## Back To Basics, Part Uno: Linear Regression and Cost Function  
An illustrated guide on essential machine learning concepts  
Shreya Rao  
February 3, 2023  
6 min read  
DATA SCIENCE  
<!-- image -->","Back To Basics, Part Uno: Linear Regression and Cost Function",2,,,prose,47,d1b9de4a-88ad-457c-ba05-f88d75ac21a8,f53f4038-2eb5-4c2e-ab5b-97ca83e4da36,2025-12-18T02:47:35.560986,90,main_content,OK,True
f53f4038-2eb5-4c2e-ab5b-97ca83e4da36,655021bc-2dd9-4de8-a93a-85bc3dc612a1,46,"## Our Columns  
Columns on TDS are carefully curated collections of posts on a particular idea or category…  
TDS Editors  
November 14, 2020 4 min read  
<!-- image -->  
DATA SCIENCE",Our Columns,2,,,prose,45,62fba649-8a38-4cb9-9bb8-b5fe7a50969b,ff21cbb8-6cf6-4a6b-9515-1f5a4497a194,2025-12-18T02:47:35.560993,80,main_content,OK,True
ff21cbb8-6cf6-4a6b-9515-1f5a4497a194,655021bc-2dd9-4de8-a93a-85bc3dc612a1,47,"## Optimizing Marketing Campaigns with Budgeted Multi-Armed Bandits  
With demos, our new solution, and a video  
Vadim Arzamasov  
August 16, 2024 10 min read  
<!-- image -->  
<!-- image -->  
Your home for data science and Al. The world's leading publication for data science, data analytics, data engineering, machine learning, and arti fi cial intelligence professionals.  
© Insight Media Group, LLC 2025  
<!-- image -->  
<!-- image -->  
<!-- image -->  
Subscribe to Our Newsletter  
COOKIES SETTINGS WRITE FOR TDS · ABOUT · ADVERTISE PRIVACY POLICY · TERMS OF USE  
•",Optimizing Marketing Campaigns with Budgeted Multi-Armed Bandits,2,,,prose,141,f53f4038-2eb5-4c2e-ab5b-97ca83e4da36,,2025-12-18T02:47:35.561002,90,main_content,OK,True
