chunk_id,file_id,chunk_index,content,section_header,section_level,parent_section,content_type,estimated_tokens,prev_chunk_id,next_chunk_id,created_at
8336e511-2545-47fd-9391-713a7a16b42b,eacd5119-8805-41cb-bdfc-bbab89efdb71,0,"## AUDIO DEEPFAKE VERIFICATION

Li Wang, Junyi Ao, Linyong Gan, Yuancheng Wang, Xueyao Zhang, Zhizheng Wu

School of Data Science, Shenzhen Research Institute of Big Data, The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen), China",AUDIO DEEPFAKE VERIFICATION,2,,prose,67,,0f2e0087-e32a-4761-ad73-cf2cbacf618f,2025-12-07T17:14:21.247486
0f2e0087-e32a-4761-ad73-cf2cbacf618f,eacd5119-8805-41cb-bdfc-bbab89efdb71,1,"## ABSTRACT

With the rapid development of deepfake technology, simply making a binary judgment of true or false on audio is no longer sufficient to meet practical needs. Accurately determining the specific deepfake method has become crucial. This paper introduces the Audio Deepfake Verification (ADV) task, effectively addressing the limitations of existing deepfake source tracing methods in closed-set scenarios, aiming to achieve open-set deepfake source tracing. Meanwhile, the Audity dual-branch architecture is proposed, extracting deepfake features from two dimensions: audio structure and generation artifacts. Experimental results show that the dualbranch Audity architecture outperforms any single-branch configuration, and it can simultaneously achieve excellent performance in both deepfake detection and verification tasks.",ABSTRACT,2,,prose,146,8336e511-2545-47fd-9391-713a7a16b42b,6f2d5597-e73b-47d7-948e-43ebb0bc19cb,2025-12-07T17:14:21.247518
6f2d5597-e73b-47d7-948e-43ebb0bc19cb,eacd5119-8805-41cb-bdfc-bbab89efdb71,2,"Index Terms -Deepfake source tracing, audio deepfake verification, open set",ABSTRACT,2,,prose,15,0f2e0087-e32a-4761-ad73-cf2cbacf618f,29f84c9f-f34e-4caa-a28a-10e902820182,2025-12-07T17:14:21.247530
29f84c9f-f34e-4caa-a28a-10e902820182,eacd5119-8805-41cb-bdfc-bbab89efdb71,3,"## 1. INTRODUCTION

Artificially generated synthetic media, or deepfakes , have advanced rapidly over the past decade, enabling creative expression but also posing severe risks - from cybercrime to political disinformation [1]. In audio processing, while Audio Deepfake Detection (ADD) identifies whether an audio is synthetic via binary classification [2, 3], the emerging task of deepfake source tracing (or attribution [4], fingerprint [5], algorithm recognition [6]) aims to identify the specific generative model or tool used. This goes beyond detection, providing critical evidence for digital forensics and accountability.",1. INTRODUCTION,2,,prose,124,6f2d5597-e73b-47d7-948e-43ebb0bc19cb,af7b7633-edb1-48ed-985f-177e069858b2,2025-12-07T17:14:21.247538
af7b7633-edb1-48ed-985f-177e069858b2,eacd5119-8805-41cb-bdfc-bbab89efdb71,4,"Most existing audio deepfake tracers operate in closed-set scenarios , only recognizing predefined algorithms and treating unknowns as a generic 'unknown' class [7]. Two common strategies are threshold-based similarity scoring [4] and anomaly detection [8]; however, both approaches are unable to attribute audio generated by previously unseen methods.",1. INTRODUCTION,2,,prose,63,29f84c9f-f34e-4caa-a28a-10e902820182,b5c73c16-d820-428a-82cb-a46d31ef6564,2025-12-07T17:14:21.247549
b5c73c16-d820-428a-82cb-a46d31ef6564,eacd5119-8805-41cb-bdfc-bbab89efdb71,5,"In particular, Wang et al. [9] proposed an audio deepfake attribution enhancement strategy (ADAE) that decouples speaker identity from generation-style features via a speaker encoder and information disentangle block. While this approach effectively mitigates speaker interference in",1. INTRODUCTION,2,,prose,51,af7b7633-edb1-48ed-985f-177e069858b2,e79cdb37-a256-43a7-8249-d1d87258353d,2025-12-07T17:14:21.247560
e79cdb37-a256-43a7-8249-d1d87258353d,eacd5119-8805-41cb-bdfc-bbab89efdb71,6,"Thanks to XYZ agency for funding.

feature space, it primarily focuses on removing time-invariant speaker information to isolate algorithm-specific fingerprints (e.g., via differential rectification of speaker-parallel components). However, this strict disentanglement may inadvertently discard subtle speaker-deepfake interaction cues, including the transformation patterns of specific vocal characteristics induced by different synthesis algorithms. Such cues are critical for cross-speaker generalization in open-set scenarios, where the model must recognize unseen algorithms across diverse speaker identities.",1. INTRODUCTION,2,,prose,99,b5c73c16-d820-428a-82cb-a46d31ef6564,d51d8c17-f16c-4407-bd1c-d8da3364eafa,2025-12-07T17:14:21.247569
d51d8c17-f16c-4407-bd1c-d8da3364eafa,eacd5119-8805-41cb-bdfc-bbab89efdb71,7,"Inspired by paradigms in speaker verification, we introduce the Audio Deepfake Verification (ADV) task. ADV aims to determine whether two audio samples were generated using the same deep fake method, enabling open set source tracking without restricting to predefined algorithms .",1. INTRODUCTION,2,,prose,50,e79cdb37-a256-43a7-8249-d1d87258353d,fee3d61e-cf8b-44c8-b06e-aac16d193ab4,2025-12-07T17:14:21.247587
fee3d61e-cf8b-44c8-b06e-aac16d193ab4,eacd5119-8805-41cb-bdfc-bbab89efdb71,8,"Effective feature extraction is paramount for Audio Deepfake Verification (ADV), a task that extends beyond the binary authenticity classification of Audio Deepfake Detection (ADD). ADV necessitates the identification of discriminative features that not only confirm the artificial origin of an audio sample but also, crucially, reveal the characteristic signatures of the specific deepfake generation methodology employed.",1. INTRODUCTION,2,,prose,69,d51d8c17-f16c-4407-bd1c-d8da3364eafa,f514a190-6282-41b8-a72d-34727b0d4694,2025-12-07T17:14:21.247600
f514a190-6282-41b8-a72d-34727b0d4694,eacd5119-8805-41cb-bdfc-bbab89efdb71,9,"Currently, state-of-the-art audio deepfake feature extraction often leverages self-supervised pre-trained models such as Wav2Vec [10] and WavLM [11]. These models, typically trained through masked prediction objectives, are highly effective at learning robust representations of natural speech structures, including phoneme sequences, prosodic patterns, and linguistic temporal dependencies.",1. INTRODUCTION,2,,prose,72,fee3d61e-cf8b-44c8-b06e-aac16d193ab4,ec14648c-add7-4e6c-be31-ce30766994a3,2025-12-07T17:14:21.247633
ec14648c-add7-4e6c-be31-ce30766994a3,eacd5119-8805-41cb-bdfc-bbab89efdb71,10,"However, for the task of ADV, it is not sufficient to rely solely on speech structural features. In addition to capturing the natural speech structure, it is also crucial to extract generation-specific artifacts, such as discontinuous highfrequency energy distributions, anomalous patterns in silent segments, breath sounds, pauses, and spectral distortions introduced during synthesis. These artifacts can provide important cues for distinguishing between different deepfake generation methods.",1. INTRODUCTION,2,,prose,83,f514a190-6282-41b8-a72d-34727b0d4694,6991a6ec-5ecd-411a-8731-2789dfee1f71,2025-12-07T17:14:21.247645
6991a6ec-5ecd-411a-8731-2789dfee1f71,eacd5119-8805-41cb-bdfc-bbab89efdb71,11,"Therefore, in the context of ADV, it is essential to jointly consider both the structural features of natural speech and the generation-specific artifacts. To this end, we propose the dual-branch Audity architecture, which is specifically designed to extract and integrate these two complementary types of features.",1. INTRODUCTION,2,,prose,56,ec14648c-add7-4e6c-be31-ce30766994a3,5809a04a-f008-4f82-9ef3-9bbf9b017d27,2025-12-07T17:14:21.247652
5809a04a-f008-4f82-9ef3-9bbf9b017d27,eacd5119-8805-41cb-bdfc-bbab89efdb71,12,"Experimental results demonstrate the effectiveness of the proposed Audity architecture. Notably, Audity exhibits strong verification capabilities not only on commercial audio deepfake systems but also on state-of-the-art deepfake generation models. It is worth highlighting that Audity achieves high performance on both verification and detection tasks simultaneously, showcasing its versatility and robustness in practical audio deepfake scenarios.",1. INTRODUCTION,2,,prose,72,6991a6ec-5ecd-411a-8731-2789dfee1f71,056fc493-6cb2-4f05-9bd6-a26b0108386c,2025-12-07T17:14:21.247659
056fc493-6cb2-4f05-9bd6-a26b0108386c,eacd5119-8805-41cb-bdfc-bbab89efdb71,13,## 2. AUDIO DEEPFAKE VERIFICATION,2. AUDIO DEEPFAKE VERIFICATION,2,,prose,11,5809a04a-f008-4f82-9ef3-9bbf9b017d27,e87f1c43-e661-4bfc-b540-e2d2ff94b5f9,2025-12-07T17:14:21.247666
e87f1c43-e661-4bfc-b540-e2d2ff94b5f9,eacd5119-8805-41cb-bdfc-bbab89efdb71,14,"## 2.1. Definition

Currently, existing methods for deepfake source tracing are predominantly designed for closed-set scenarios or classify unseen deepfake methods as 'unknown'. To recognize new deepfake methods, retraining the models becomes necessary. Inspired by speaker verification paradigms, this paper introduces the Audio Deepfake Verification (ADV) task, aiming to achieve open-set deepfake source tracing.",2.1. Definition,2,,prose,79,056fc493-6cb2-4f05-9bd6-a26b0108386c,65f2a448-33ac-4c9a-b13e-56d94b2e5387,2025-12-07T17:14:21.247673
65f2a448-33ac-4c9a-b13e-56d94b2e5387,eacd5119-8805-41cb-bdfc-bbab89efdb71,15,"Fig. 1 . Block diagram of the Audio Deepfake Verification (ADV) system.

<!-- image -->

The basic block diagram of the ADV system is shown in Figure 1. In the training phase, data from known deepfake methods are utilized for closed-set training, with the objective of extracting deepfake features. During the testing phase, an unknown sample is compared with the features of known samples. If the similarity score exceeds the threshold, it is considered to be generated by the same deepfake method; otherwise, it is deemed to be generated by a different method.",2.1. Definition,2,,prose,114,e87f1c43-e661-4bfc-b540-e2d2ff94b5f9,2379f458-6f34-4f38-bcef-65337f9b937c,2025-12-07T17:14:21.247680
2379f458-6f34-4f38-bcef-65337f9b937c,eacd5119-8805-41cb-bdfc-bbab89efdb71,16,"## 2.2. Audity: A Dual-Branch Network for Audio Deepfake Verification

To address the complementary modeling of speech structure and generation artifacts, we propose Audity , a dual-branch architecture that integrates pre-trained speech structural representations with generation artifact modeling. As illustrated in Figure 2, the network consists of two parallel pathways: an Audio Structural Branch and a Generation Artifacts Branch , which are fused to produce discriminative deepfake embeddings.",2.2. Audity: A Dual-Branch Network for Audio Deepfake Verification,2,,prose,91,65f2a448-33ac-4c9a-b13e-56d94b2e5387,a7af3016-3376-49fa-9413-2629c1df0793,2025-12-07T17:14:21.247686
a7af3016-3376-49fa-9413-2629c1df0793,eacd5119-8805-41cb-bdfc-bbab89efdb71,17,"Fig. 2 . The overall architecture of Audity.

<!-- image -->",2.2. Audity: A Dual-Branch Network for Audio Deepfake Verification,2,,prose,15,2379f458-6f34-4f38-bcef-65337f9b937c,11edf67b-efea-4280-bde6-691f65edd681,2025-12-07T17:14:21.247692
11edf67b-efea-4280-bde6-691f65edd681,eacd5119-8805-41cb-bdfc-bbab89efdb71,18,"## 2.2.1. Audio Structural Branch

Leveraging the strengths of self-supervised pre-training, this branch employs w2v-BERT 2.0 [12] 1 to extract high-level structural representations of speech content. Unlike traditional 'semantic' modeling, which focuses on linguistic meaning, our design emphasizes capturing temporal dependencies of language units - including phoneme sequences, prosodic contours, and syllabic rhythms -that characterize natural speech consistency. Input as a mel-spectrogram, the backbone processes sequential features through masked prediction objectives, learning to reconstruct intact speech structures from corrupted inputs. This enables the branch to encode normative patterns of human speech production and detect structural anomalies introduced by deepfake algorithms (e.g., unnatural prosodic breaks or phoneme misalignments).",2.2.1. Audio Structural Branch,2,,prose,159,a7af3016-3376-49fa-9413-2629c1df0793,298f98c7-1d0c-4af5-b679-7fb267bd2b47,2025-12-07T17:14:21.247699
298f98c7-1d0c-4af5-b679-7fb267bd2b47,eacd5119-8805-41cb-bdfc-bbab89efdb71,19,"## 2.2.2. Generation Artifacts Branch

To model artifacts specific to synthetic audio generation, this branch is designed to flexibly accommodate various state-of-the-art acoustic modeling architectures. By taking spectrograms as input, the branch preserves comprehensive raw audio information, enabling the extraction of generationspecific artifacts without reliance on handcrafted features. This approach allows the model to capture technical fingerprints associated with different deepfake generation methods.",2.2.2. Generation Artifacts Branch,2,,prose,85,11edf67b-efea-4280-bde6-691f65edd681,dcafdead-5b3b-4a13-af28-2003249e2248,2025-12-07T17:14:21.247705
dcafdead-5b3b-4a13-af28-2003249e2248,eacd5119-8805-41cb-bdfc-bbab89efdb71,20,"## 2.2.3. Feature Fusion and Embedding Generation

The outputs of the two branches are concatenated and passed through a fusion layer, which integrates the audio structural features and generation artifact features. The resulting joint representation captures both the intrinsic patterns of natural speech and the distinctive artifacts introduced by synthetic generation. This comprehensive feature integration enables Audity to support the ADV task, facilitating not only the detection of deepfake audio but also the accurate attribution of its generative source.",2.2.3. Feature Fusion and Embedding Generation,2,,prose,95,298f98c7-1d0c-4af5-b679-7fb267bd2b47,48b44e51-b920-46d9-85ba-897675193c2d,2025-12-07T17:14:21.247711
48b44e51-b920-46d9-85ba-897675193c2d,eacd5119-8805-41cb-bdfc-bbab89efdb71,21,1 https://huggingface.co/facebook/w2v-bert-2.0,2.2.3. Feature Fusion and Embedding Generation,2,,prose,17,dcafdead-5b3b-4a13-af28-2003249e2248,18c1f1fd-f6bb-4b8f-b855-c76da9d56715,2025-12-07T17:14:21.247718
18c1f1fd-f6bb-4b8f-b855-c76da9d56715,eacd5119-8805-41cb-bdfc-bbab89efdb71,22,## 3. EXPERIMENTAL SETUPS,3. EXPERIMENTAL SETUPS,2,,prose,9,48b44e51-b920-46d9-85ba-897675193c2d,f520d73f-4498-42dd-8487-5499968e8201,2025-12-07T17:14:21.247723
f520d73f-4498-42dd-8487-5499968e8201,eacd5119-8805-41cb-bdfc-bbab89efdb71,23,## 3.1. Dataset,3.1. Dataset,2,,prose,7,18c1f1fd-f6bb-4b8f-b855-c76da9d56715,5ae491ec-5edd-43d2-9b11-cdc0540de26c,2025-12-07T17:14:21.247729
5ae491ec-5edd-43d2-9b11-cdc0540de26c,eacd5119-8805-41cb-bdfc-bbab89efdb71,24,"## 3.1.1. Training Dataset

In this study, we design and employ two distinct training settings to comprehensively evaluate the effectiveness of Audity.

First, to ensure rigorous benchmarking and comparability, we adopt the official protocol of MLAAD SourceTrace [13] 2 , where the training set consists of samples generated by 24 different audio deepfake methods.",3.1.1. Training Dataset,2,,prose,77,f520d73f-4498-42dd-8487-5499968e8201,31ba3ec0-8517-42ec-a8ec-43b27c14c9aa,2025-12-07T17:14:21.247735
31ba3ec0-8517-42ec-a8ec-43b27c14c9aa,eacd5119-8805-41cb-bdfc-bbab89efdb71,25,"Second, only through training with diverse deepfake methods can the model effectively extract the key features of various deepfake techniques. Therefore, this study has comprehensively collected 10 open-source datasets, and the detailed statistical information is presented in Table 1. All datasets are partitioned according to their official splits whenever available. For datasets without official training, validation, and test splits, the entire dataset is utilized as training data.",3.1.1. Training Dataset,2,,prose,84,5ae491ec-5edd-43d2-9b11-cdc0540de26c,4f0d6ca1-4f19-4ff8-b998-995187b7ecb1,2025-12-07T17:14:21.247742
4f0d6ca1-4f19-4ff8-b998-995187b7ecb1,eacd5119-8805-41cb-bdfc-bbab89efdb71,26,"Table 1 . Training dataset

| Dataset              | #Utterance   | Hours    |   #Types |
|----------------------|--------------|----------|----------|
| ASVspoof2019-LA [14] | 25,380       | 24.15    |        6 |
| CodecFake(UCAS) [15] | 740,747      | 706.43   |        6 |
| CodecFake+ [16]      | 1,417,845    | 1,341.30 |       31 |
| DFADD [17]           | 372,325      | 373.97   |        5 |
| GigaSpeech(M) [18]   | 885,397      | 973.44   |        0 |
| LibriSeVoc [19]      | 55,440       | 145.02   |        6 |
| MLAAD [20]           | 151,446      | 371.21   |       49 |
| SpoofCeleb [21]      | 2,540,421    | 1,982.23 |       10 |
| Wavefake [22]        | 110,870      | 197.91   |        9 |
| ADD2023 Track3 [23]  | 22,400       | 34.93    |        6 |
| Total                | 6,322,271    | 6,150.60 |      128 |",3.1.1. Training Dataset,2,,prose,323,31ba3ec0-8517-42ec-a8ec-43b27c14c9aa,c7064ca0-c3cb-4bf3-89e8-d88685b46366,2025-12-07T17:14:21.247748
c7064ca0-c3cb-4bf3-89e8-d88685b46366,eacd5119-8805-41cb-bdfc-bbab89efdb71,27,"The validation dataset is constructed based on the official validation splits provided by each source dataset. The detailed statistical information for the validation set is summarized in Table 2.

Regarding the annotation of deepfake methods, there is currently no consensus in the academic community on the factors associated with deepfake fingerprints. It is generally believed that they may be closely related to model architectures, optimization algorithms, and training datasets. In view of this, this study has conducted a rigorous examination of the deepfake methods in each dataset. Identical annotations are only assigned to models that are completely the same, thus maximizing the retention of rich meta-information and laying a foundation for subsequent research.",3.1.1. Training Dataset,2,,prose,130,4f0d6ca1-4f19-4ff8-b998-995187b7ecb1,b272b835-238a-4099-99da-3f98735cd507,2025-12-07T17:14:21.247755
b272b835-238a-4099-99da-3f98735cd507,eacd5119-8805-41cb-bdfc-bbab89efdb71,28,"2 https://deepfake-total.com/sourcetracing

Table 2 . Validate dataset

| Dataset              | #Utterance   |   Hours |   #Types |
|----------------------|--------------|---------|----------|
| ASVspoof2019-LA [14] | 24,844       |   24    |        6 |
| CodecFake(UCAS) [15] | 92,596       |   88.66 |        6 |
| CodecFake+ [16]      | 48,510       |   46.13 |       23 |
| DFADD [17]           | 6,675        |    6.78 |        5 |
| GigaSpeech(M) [18]   | 5,715        |   11.37 |        0 |
| LibriSeVoc [19]      | 18,480       |   49.03 |        6 |
| SpoofCeleb [21]      | 55,741       |   40.92 |        6 |
| ADD2023 Track3 [23]  | 8,400        |   13.9  |        6 |
| Total                | 260,961      |  280.78 |       58 |",3.1.1. Training Dataset,2,,prose,279,c7064ca0-c3cb-4bf3-89e8-d88685b46366,9e937112-63d0-4570-99ee-3650eb3992f3,2025-12-07T17:14:21.247761
9e937112-63d0-4570-99ee-3650eb3992f3,eacd5119-8805-41cb-bdfc-bbab89efdb71,29,"## 3.1.2. Test Dataset

We conducted evaluations using five different test sets, which are shown in Table 3. Especially, Demo Page datasets are collected from various demo pages and include samples generated by CosyVoice [24], CosyVoice2[25], E2TTS[26], FireRedTTS[27], Llasa[28], MaskGCT[29], SeedTTS[30], and SparkTTS[31], while Commercial TTS datasets consist of audio generated by commercial Text-to-Speech (TTS) systems, including the latest gpt-4o-mini-tts from OpenAI and audio produced by Microsoft Azure TTS.",3.1.2. Test Dataset,2,,prose,140,b272b835-238a-4099-99da-3f98735cd507,ee7ccae9-93df-414a-9da7-75af5b6f7426,2025-12-07T17:14:21.247767
ee7ccae9-93df-414a-9da7-75af5b6f7426,eacd5119-8805-41cb-bdfc-bbab89efdb71,30,"| Table 3 . Test dataset   | Table 3 . Test dataset   | Table 3 . Test dataset   | Table 3 . Test dataset   |
|--------------------------|--------------------------|--------------------------|--------------------------|
| Dataset                  | #Utterance               | Hours                    | #Types                   |
| ASVspoof2019-LA [14]     | 71,237                   | 61.49                    | 13                       |
| SpoofCeleb [21]          | 91,130                   | 66.08                    | 9                        |
| ADD2023 Track3 [23]      | 79,490                   | 119.43                   | 7                        |
| Demo Page dataset        | 604                      | 2.04                     | 8                        |
| Commercial TTS dataset   | 200                      | 0.25                     | 2                        |",3.1.2. Test Dataset,2,,prose,187,9e937112-63d0-4570-99ee-3650eb3992f3,91215ab1-308e-435d-9695-bce66c55bdf6,2025-12-07T17:14:21.247772
91215ab1-308e-435d-9695-bce66c55bdf6,eacd5119-8805-41cb-bdfc-bbab89efdb71,31,"## 3.2. Experimental Setups

Construction of test sample pairs: For the samples in each test set, samples of the same type and different types are randomly selected to form verification sample pairs.

Validation set and model selection: The validation set is shown in Table 2 and all come from the official division. Sample pairs are constructed using the same method as that for the test set. The model with the lowest Equal Error Rate (EER) on the validation set is selected as the optimal model. The threshold corresponds to the EER of the validation set.",3.2. Experimental Setups,2,,prose,114,ee7ccae9-93df-414a-9da7-75af5b6f7426,83fe275f-1f47-4ea4-a01d-30246c018e91,2025-12-07T17:14:21.247778
83fe275f-1f47-4ea4-a01d-30246c018e91,eacd5119-8805-41cb-bdfc-bbab89efdb71,32,"Sample type distribution balance: At the start of each epoch, randomly sample 1000 samples per type to ensure the balance of every type.

Implementation details of the generation artifacts branch : We implemented the generation artifacts branch using several alternative architectures, including CAM++ [32], ECAPA-TDNN [33], and ResNet293[34]. All implementations were developed based on the WeSpeaker toolkit[35, 36], ensuring consistency and reproducibility across different model variants.",3.2. Experimental Setups,2,,prose,95,91215ab1-308e-435d-9695-bce66c55bdf6,449f7340-1e4c-4855-bfab-a45de92d6fa5,2025-12-07T17:14:21.247783
449f7340-1e4c-4855-bfab-a45de92d6fa5,eacd5119-8805-41cb-bdfc-bbab89efdb71,33,"## 3.3. Metric

The test metrics consist of six items: Equal Error Rate (EER), Accuracy (Acc), False Acceptance Rate (FAR), False Rejection Rate (FRR), F1 score, and Area Under the Receiver Operating Characteristic curve (AUROC). Among them, FAR represents the sample pair with 'different' label that are misidentified as 'same'.",3.3. Metric,2,,prose,81,83fe275f-1f47-4ea4-a01d-30246c018e91,7e43db6c-491b-4466-8c46-1dc3f8bb9f59,2025-12-07T17:14:21.247790
7e43db6c-491b-4466-8c46-1dc3f8bb9f59,eacd5119-8805-41cb-bdfc-bbab89efdb71,34,"In addition, to provide a reference for practical applications, the FRR when the FAR is 1% and the FAR when the FRR is 1% are also reported.",3.3. Metric,2,,prose,36,449f7340-1e4c-4855-bfab-a45de92d6fa5,9da6f12e-8aa2-44ae-9d4b-9a945f099961,2025-12-07T17:14:21.247795
9da6f12e-8aa2-44ae-9d4b-9a945f099961,eacd5119-8805-41cb-bdfc-bbab89efdb71,35,## 4. EXPERIMENTAL RESULTS,4. EXPERIMENTAL RESULTS,2,,prose,8,7e43db6c-491b-4466-8c46-1dc3f8bb9f59,190fe329-48e8-43bb-905d-efbe70cfa66f,2025-12-07T17:14:21.247802
190fe329-48e8-43bb-905d-efbe70cfa66f,eacd5119-8805-41cb-bdfc-bbab89efdb71,36,"## 4.1. Performance of Different Audity Architectures

Table 4 . Results of different Audity branch configurations on the MLAAD SourceTrace dataset. GAB: Generation Artifacts Branch; ASB: Audio Structural Branch.

| GAB        | ASB          |   Acc (%) |   EER (%) |
|------------|--------------|-----------|-----------|
| ECAPA-TDNN | w2v-BERT 2.0 |     89.28 |     10.49 |
| ResNet239  | w2v-BERT 2.0 |     79.8  |     21.05 |
| CAM++      | w2v-BERT 2.0 |     89.87 |     10.02 |
| -          | w2v-BERT 2.0 |     86.69 |     13.29 |
| CAM++      | -            |     68.77 |     31.35 |",4.1. Performance of Different Audity Architectures,2,,prose,206,9da6f12e-8aa2-44ae-9d4b-9a945f099961,f83049df-734c-433b-a64c-42b693546bb5,2025-12-07T17:14:21.247808
f83049df-734c-433b-a64c-42b693546bb5,eacd5119-8805-41cb-bdfc-bbab89efdb71,37,"Integrating different Generation Artifacts Branches with the Audio Structural Branch reveals that CAM++ achieves the best overall performance. Table 4 presents the effectiveness of the Audity architecture with different branch configurations on the MLAAD SourceTrace dataset. The first three rows compare the impact of various Generation Artifacts Branch (GAB) implementations, including ECAPA-TDNN, ResNet239, and CAM++, each combined with the Audio Structural Branch (ASB, implemented as w2v-BERT 2.0). Among these configurations, the combination of CAM++ and w2v-BERT 2.0 achieves the best performance, with an accuracy of 89.87% and an EER of 10.02%.",4.1. Performance of Different Audity Architectures,2,,prose,146,190fe329-48e8-43bb-905d-efbe70cfa66f,d87bd3c6-c34d-4c8a-9744-0a5e7fad1ff8,2025-12-07T17:14:21.247814
d87bd3c6-c34d-4c8a-9744-0a5e7fad1ff8,eacd5119-8805-41cb-bdfc-bbab89efdb71,38,"The dual-branch architecture outperforms either single branch alone, highlighting the complementarity of structural and artifact features. The last two rows provide further insights by evaluating single-branch configurations. Using only the Audio Structural Branch (w2v-BERT 2.0) without a Generation Artifacts Branch yields an accuracy of 86.69% and an EER of 13.29%. In contrast, employing only the Generation Artifacts Branch (CAM++) without the Audio Structural Branch results in a significant performance drop, with an accuracy of 68.77% and an EER of 31.35%. These results demonstrate that the dual-branch architecture, which integrates both structural and generation artifact features, substantially outperforms either branch alone.",4.1. Performance of Different Audity Architectures,2,,prose,150,f83049df-734c-433b-a64c-42b693546bb5,e9a2bbd2-4304-40fe-9e8f-47374e798eeb,2025-12-07T17:14:21.247821
e9a2bbd2-4304-40fe-9e8f-47374e798eeb,eacd5119-8805-41cb-bdfc-bbab89efdb71,39,"## 4.2. Performance of Audity on Multiple Test Datasets

More advanced deepfake generation methods may be more difficult to distinguish due to their increased realism. The results of the Audity model trained on the datasets summarized in Table 1 are presented in Table 5. As shown, there is a considerable variation in performance across different test datasets. Notably, the ADD2023 Track3, Commercial System, and Demo Pages datasets present the greatest challenges. This can be attributed to the fact that ADD2023 Track3 contains a portion of commercial data, while the Demo Pages dataset consists exclusively of samples generated by the latest deepfake models. The presence of these advanced and diverse generation methods increases the difficulty of distinguishing between different types of audio deepfake methods.",4.2. Performance of Audity on Multiple Test Datasets,2,,prose,154,d87bd3c6-c34d-4c8a-9744-0a5e7fad1ff8,d9f74273-12ee-49b5-be81-e6ec5de3c522,2025-12-07T17:14:21.247827
d9f74273-12ee-49b5-be81-e6ec5de3c522,eacd5119-8805-41cb-bdfc-bbab89efdb71,40,"Using the average of embeddings from a larger number of enrollment samples to represent each deepfake method leads to improved performance. We conducted experiments on ADD2023 Track3 and Demo Pages by increasing both the number of enrollment and verification samples to 5, and using the mean of their embeddings as the representation for each method. As shown in Table 5, when both the enrollment and verification sample numbers are set to 5, the EER on ADD2023 Track3 drops significantly to 4.2%, and on Demo Pages to 17.0%. This demonstrates a substantial improvement compared to the single-sample setting. This improvement is reasonable, as incorporating more samples allows the model to capture more representative and robust features of each deepfake method.",4.2. Performance of Audity on Multiple Test Datasets,2,,prose,148,e9a2bbd2-4304-40fe-9e8f-47374e798eeb,8f9603f5-eebd-47e9-9926-97612427a368,2025-12-07T17:14:21.247834
8f9603f5-eebd-47e9-9926-97612427a368,eacd5119-8805-41cb-bdfc-bbab89efdb71,41,"## 4.3. Visualization of Deepfake Verification Embeddings

Figure 3 presents the visualization results for three datasets: the left panel corresponds to ADD2023 Track3, the middle panel to SpoofCeleb, and the right panel to the Demo Page dataset.",4.3. Visualization of Deepfake Verification Embeddings,2,,prose,55,d9f74273-12ee-49b5-be81-e6ec5de3c522,b2b37264-df07-46e2-b9b1-570347b1a5c7,2025-12-07T17:14:21.247841
b2b37264-df07-46e2-b9b1-570347b1a5c7,eacd5119-8805-41cb-bdfc-bbab89efdb71,42,"Models with similar architectures may exhibit similar deepfake feature distributions. For example, in the SpoofCeleb results (middle panel), both A19 and A20 are based on the Multi-scale Transformer architecture, while the waveform models for A18 and A23 are both NSF HiFiGAN. Similarly, in the Demo Page results (right panel), CosyVoice and CosyVoice2 display highly similar feature distributions. However, it is important to note that research on the factors associated with deepfake features is still in its early stages. The current consensus in the research community is that model architecture plays a significant role, but other factors related to data distribution, such as training data and optimization algorithms, have received relatively little attention.",4.3. Visualization of Deepfake Verification Embeddings,2,,prose,145,8f9603f5-eebd-47e9-9926-97612427a368,8295f9f3-50d7-42dc-96c8-6f2726c73397,2025-12-07T17:14:21.247848
8295f9f3-50d7-42dc-96c8-6f2726c73397,eacd5119-8805-41cb-bdfc-bbab89efdb71,43,"Commercial models are difficult to distinguish from each other. As shown in the visualization of ADD 2023 (left

Table 5 . Audio Deepfake Verification Results on Different Test Datasets using Audity with CAM++ as the Generation Artifacts Branch. #E denotes the number of enrollment samples (i.e., samples with known fake types), and #V denotes the number of verification samples (i.e., test samples).",4.3. Visualization of Deepfake Verification Embeddings,2,,prose,84,b2b37264-df07-46e2-b9b1-570347b1a5c7,397144d1-2a77-4f97-bc0b-ab42038b5b09,2025-12-07T17:14:21.247854
397144d1-2a77-4f97-bc0b-ab42038b5b09,eacd5119-8805-41cb-bdfc-bbab89efdb71,44,"| Test Dataset       |   #E |   #V | Acc   | FAR   | FRR   | EER   |   F1 |   AUROC | FRR@FAR1%   | FAR@FRR1%   |
|--------------------|------|------|-------|-------|-------|-------|------|---------|-------------|-------------|
| SpoofCeleb         |    1 |    1 | 87.3% | 19.9% | 5.6%  | 13.7% | 0.88 |    0.94 | 76.0%       | 26.6%       |
| ASVspoof2019       |    1 |    1 | 88.6% | 15.7% | 7.1%  | 11.6% | 0.89 |    0.95 | 67.3%       | 44.9%       |
| Commericial System |    1 |    1 | 73.5% | 9.5%  | 43.5% | 24.0% | 0.68 |    0.83 | 68.5%       | 92.0%       |
| ADD2023 Track3     |    1 |    1 | 76.4% | 14.2% | 33.1% | 25.0% | 0.74 |    0.83 | 65.3%       | 95.1%       |
| ADD2023 Track3     |    5 |    1 | 85.2% | 14.7% | 14.8% | 14.8% | 0.85 |    0.92 | 52.0%       | 75.6%       |
| ADD2023 Track3     |    5 |    5 | 89.1% | 21.4% | 0.5%  | 4.2%  | 0.9  |    0.99 | 31.2%       | 8.6%        |
| Demo Pages         |    1 |    1 | 62.8% | 17.6% | 57.0% | 37.2% | 0.54 |    0.68 | 91.1%       | 97.4%       |
| Demo Pages         |    5 |    1 | 67.8% | 47.0% | 17.4% | 30.8% | 0.72 |    0.76 | 89.1%       | 88.3%       |
| Demo Pages         |    5 |    5 | 57.9% | 84.1% | 0.2%  | 17.0% | 0.7  |    0.9  | 88.7%       | 54.8%       |",4.3. Visualization of Deepfake Verification Embeddings,2,,prose,658,8295f9f3-50d7-42dc-96c8-6f2726c73397,a92186d5-aa3c-43a3-acca-23dc156eb3b0,2025-12-07T17:14:21.247861
a92186d5-aa3c-43a3-acca-23dc156eb3b0,eacd5119-8805-41cb-bdfc-bbab89efdb71,45,"Fig. 3 . Visualization of deepfake audio embeddings using t-SNE. Each color represents a different deepfake generation method, and the star markers indicate the cluster centers for each method. The left panel corresponds to ADD2023 Track3, the middle panel to SpoofCeleb, and the right panel to our self-collected Demo Page dataset. For SpoofCeleb (middle), the authors use 'Acoustic model' and 'Waveform model' to denote different deepfake methods [21].",4.3. Visualization of Deepfake Verification Embeddings,2,,prose,102,397144d1-2a77-4f97-bc0b-ab42038b5b09,344205f0-7592-4a22-997b-3c963b55811b,2025-12-07T17:14:21.247869
344205f0-7592-4a22-997b-3c963b55811b,eacd5119-8805-41cb-bdfc-bbab89efdb71,46,"<!-- image -->

panel), the feature distributions of commercial systems such as Baidu and Aispeech are highly concentrated and challenging to separate. This may be attributed to the fact that commercial systems often adopt the most robust solutions in the industry, leading to similarities in model architectures and optimization strategies. Additionally, these systems may collect their own large-scale datasets, and as the amount of data increases, the data distributions may also become more similar.",4.3. Visualization of Deepfake Verification Embeddings,2,,prose,86,a92186d5-aa3c-43a3-acca-23dc156eb3b0,f3c0f8d2-2226-48ba-a8b4-0d0a3c628312,2025-12-07T17:14:21.247876
f3c0f8d2-2226-48ba-a8b4-0d0a3c628312,eacd5119-8805-41cb-bdfc-bbab89efdb71,47,"## 4.4. Can the Audio Deepfake Verification Model be directly used for Audio Deepfake Detection?

As shown in Figure 3, Bonafide samples are well separated from various deepfake methods. Motivated by this observation, we further explored using the posterior probability of Bonafide as the detection score for deepfake detection (i.e., binary classification). All test sets follow the official evalua- tion protocols. The results are summarized in Table 6.",4.4. Can the Audio Deepfake Verification Model be directly used for Audio Deepfake Detection?,2,,prose,95,344205f0-7592-4a22-997b-3c963b55811b,9d9f6452-e1c3-4c34-9f3c-62fd60032595,2025-12-07T17:14:21.247882
9d9f6452-e1c3-4c34-9f3c-62fd60032595,eacd5119-8805-41cb-bdfc-bbab89efdb71,48,"Learning to distinguish features of different deepfake methods enhances the model's understanding of deepfakes. Unlike simple binary classification, which treats all deepfakes as a single 'spoof' class, differentiating between various deepfake methods allows the model to capture the unique 'fingerprints' of each technique. This richer understanding enables the model to identify more subtle and discriminative patterns, resulting in a more comprehensive representation of deepfake characteristics and improved performance in both verification and detection tasks across diverse scenarios.",4.4. Can the Audio Deepfake Verification Model be directly used for Audio Deepfake Detection?,2,,prose,99,f3c0f8d2-2226-48ba-a8b4-0d0a3c628312,3e590428-c7d7-45a3-81ce-f934236762c8,2025-12-07T17:14:21.247888
3e590428-c7d7-45a3-81ce-f934236762c8,eacd5119-8805-41cb-bdfc-bbab89efdb71,49,"Data diversity is more important than quantity. As shown in Table 1, we utilized GigaSpeech [18], a highly diverse dataset composed entirely of bonafide samples. This diversity helps the model better understand the natural variability of human speech, including different accents, speak-",4.4. Can the Audio Deepfake Verification Model be directly used for Audio Deepfake Detection?,2,,prose,55,9d9f6452-e1c3-4c34-9f3c-62fd60032595,e565ed4d-4b70-41e9-a822-4c5c2dac2a7b,2025-12-07T17:14:21.247895
e565ed4d-4b70-41e9-a822-4c5c2dac2a7b,eacd5119-8805-41cb-bdfc-bbab89efdb71,50,"Table 6 . Audio Deepfake Detection Results compared with previous state-of-the-art (SOTA) methods.

| Test Dataset   |   EER (Ours) (%) |   EER (SOTA) (%) |
|----------------|------------------|------------------|
| In-the-Wild    |             7.14 |             7.68 |
| SpoofCeleb     |             0.07 |             1.12 |
| ASVspoof2019   |             0.03 |             0.22 |",4.4. Can the Audio Deepfake Verification Model be directly used for Audio Deepfake Detection?,2,,prose,113,3e590428-c7d7-45a3-81ce-f934236762c8,6e1dc0cc-5292-4632-aa1a-e9262ae6227d,2025-12-07T17:14:21.247901
6e1dc0cc-5292-4632-aa1a-e9262ae6227d,eacd5119-8805-41cb-bdfc-bbab89efdb71,51,"ing styles, and recording conditions. Exposure to such a wide range of real-world speech enables the model to establish a robust baseline for authentic audio, making it more effective at detecting subtle deviations caused by deepfake manipulations and thereby improving performance in both verification and detection tasks.",4.4. Can the Audio Deepfake Verification Model be directly used for Audio Deepfake Detection?,2,,prose,54,e565ed4d-4b70-41e9-a822-4c5c2dac2a7b,53873157-d3e6-4691-bc70-a24b720eea98,2025-12-07T17:14:21.247906
53873157-d3e6-4691-bc70-a24b720eea98,eacd5119-8805-41cb-bdfc-bbab89efdb71,52,"## 5. CONCLUSIONS

In this paper, we introduced the Audio Deepfake Verification (ADV) task to address the limitations of existing closed-set deepfake source tracing methods, enabling open-set source attribution by determining whether two audio samples are generated by the same deepfake method. We proposed Audity, a dual-branch architecture comprising an Audio Structural Branch, which encodes the temporal consistency of natural speech, and a Generation Artifacts Branch, which captures unique artifacts specific to synthetic audio. Experiments across diverse datasets demonstrated the effectiveness of Audity in distinguishing between different deepfake generation methods, although more advanced methods remain challenging. Remarkably, our model also achieved excellent performance in Audio Deepfake Detection on several datasets, suggesting that Audity can simultaneously perform both audio deepfake detection and verification tasks.",5. CONCLUSIONS,2,,prose,160,6e1dc0cc-5292-4632-aa1a-e9262ae6227d,8957ed13-0dec-4869-b1c6-c933c6905de5,2025-12-07T17:14:21.247912
8957ed13-0dec-4869-b1c6-c933c6905de5,eacd5119-8805-41cb-bdfc-bbab89efdb71,53,## 6. REFERENCES,6. REFERENCES,2,,prose,5,53873157-d3e6-4691-bc70-a24b720eea98,30ee915c-14ee-44ee-82f7-6a07a0a048dc,2025-12-07T17:14:21.247919
30ee915c-14ee-44ee-82f7-6a07a0a048dc,eacd5119-8805-41cb-bdfc-bbab89efdb71,54,"- [1] Mika Westerlund, 'The emergence of deepfake technology: A review,' Technology innovation management review , vol. 9, no. 11, 2019.
- [2] Menglu Li, Yasaman Ahmadiadli, and Xiao-Ping Zhang, 'Audio anti-spoofing detection: A survey,' 2024.
- [3] Jiangyan Yi, Chenglong Wang, Jianhua Tao, Xiaohui Zhang, Chu Yuan Zhang, and Yan Zhao, 'Audio deepfake detection: A survey,' arXiv preprint arXiv:2308.14970 , 2023.
- [4] Xinrui Yan, Jiangyan Yi, Jianhua Tao, and Jie Chen, 'Audio deepfake attribution: An initial dataset and investigation,' 2024.
- [5] Xinrui Yan, Jiangyan Yi, Jianhua Tao, Chenglong Wang, Haoxin Ma, Tao Wang, Shiming Wang, and Ruibo Fu, 'An initial investigation for detecting vocoder fingerprints of fake audio,' in Proceedings of the 1st International Workshop on Deepfake Detection for Audio Multimedia , 2022, pp. 61-68.
- [6] Jiangyan Yi, Jianhua Tao, Ruibo Fu, Xinrui Yan, Chenglong Wang, Tao Wang, Chu Yuan Zhang, Xiaohui Zhang, Yan Zhao, Yong Ren, et al., 'Add 2023: the second audio deepfake detection challenge,' arXiv preprint arXiv:2305.13774 , 2023.
- [7] Nicholas Klein, Tianxiang Chen, Hemlata Tak, Ricardo Casal, and Elie Khoury, 'Source tracing of audio deepfake systems,' in Interspeech 2024 .",6. REFERENCES,2,,prose,375,8957ed13-0dec-4869-b1c6-c933c6905de5,cc463846-ed2a-4a7a-8be1-4325105fde09,2025-12-07T17:14:21.247926
cc463846-ed2a-4a7a-8be1-4325105fde09,eacd5119-8805-41cb-bdfc-bbab89efdb71,55,"Sept. 2024, p. 1100-1104, ISCA.
- [8] Yuankun Xie, Ruibo Fu, Zhengqi Wen, Zhiyong Wang, Xiaopeng Wang, Haonnan Cheng, Long Ye, and Jianhua Tao, 'Generalized source tracing: Detecting novel audio deepfake algorithm with real emphasis and fake dispersion strategy,' in Interspeech 2024 , 2024, pp. 48334837.
- [9] Zhigang Wang, Dengpan Ye, Jingyang Li, and Jiacheng Deng, 'Generalize audio deepfake algorithm recognition via attribution enhancement,' in ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) .",6. REFERENCES,2,,prose,163,30ee915c-14ee-44ee-82f7-6a07a0a048dc,57afeb3b-a839-4b99-a497-51f0fbed5fbf,2025-12-07T17:14:21.247933
57afeb3b-a839-4b99-a497-51f0fbed5fbf,eacd5119-8805-41cb-bdfc-bbab89efdb71,56,"IEEE, 2025, pp. 1-5.
- [10] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli, 'wav2vec 2.0: A framework for self-supervised learning of speech representations,' Advances in neural information processing systems , vol. 33, pp. 12449-12460, 2020.
- [11] Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al., 'Wavlm: Large-scale self-supervised pre-training for full stack speech processing,' IEEE Journal of Selected Topics in Signal Processing , vol. 16, no. 6, pp. 1505-1518, 2022.
- [12] Lo¨ ıc Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong, Mark Duppenthaler, PaulAmbroise Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim, et al., 'Seamless: Multilingual expressive and streaming speech translation,' arXiv preprint arXiv:2312.05187 , 2023.
- [13] Nicolas M¨ uller, 'Using mlaad for source tracing of audio deepfakes,' https://deepfake-total. com/sourcetracing , 11 2024.
- [14] Andreas Nautsch, Xin Wang, Nicholas Evans, Tomi H Kinnunen, Ville Vestman, Massimiliano Todisco, H´ ector Delgado, Md Sahidullah, Junichi Yamagishi, and Kong Aik Lee, 'Asvspoof 2019: spoofing countermeasures for the detection of synthesized, converted and replayed speech,' IEEE Transactions on Biometrics, Behavior, and Identity Science , vol. 3, no. 2, pp. 252-265, 2021.
- [15] Yuankun Xie, Yi Lu, Ruibo Fu, Zhengqi Wen, Zhiyong Wang, Jianhua Tao, Xin Qi, Xiaopeng Wang, Yukun Liu, Haonan Cheng, et al., 'The codecfake dataset and countermeasures for the universally detection of deepfake audio,' IEEE Transactions on Audio, Speech and Language Processing , 2025.
- [16] Xuanjun Chen, Jiawei Du, Haibin Wu, Lin Zhang, IMing Lin, I-Hsiang Chiu, Wenze Ren, Yuan Tseng, Yu Tsao, Jyh-Shing Roger Jang, and Hung yi Lee, 'Codecfake+: A large-scale neural audio codec-based deepfake speech dataset,' 2025.
- [17] Jiawei Du, I-Ming Lin, I-Hsiang Chiu, Xuanjun Chen, Haibin Wu, Wenze Ren, Yu Tsao, Hung-yi Lee, and Jyh-Shing Roger Jang, 'Dfadd: The diffusion and flowmatching based audio deepfake dataset,' in 2024 IEEE Spoken Language Technology Workshop (SLT) .",6. REFERENCES,2,,prose,684,cc463846-ed2a-4a7a-8be1-4325105fde09,2b5d51c4-0e09-4276-a8d6-a2d86d4032f4,2025-12-07T17:14:21.247939
2b5d51c4-0e09-4276-a8d6-a2d86d4032f4,eacd5119-8805-41cb-bdfc-bbab89efdb71,57,"IEEE, 2024, pp. 921-928.
- [18] Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel Povey, Jan Trmal, Junbo Zhang, et al., 'Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio,' arXiv preprint arXiv:2106.06909 , 2021.
- [19] Chengzhe Sun, Shan Jia, Shuwei Hou, and Siwei Lyu, 'Ai-synthesized voice detection using neural vocoder artifacts,' in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2023, pp. 904-912.
- [20] Nicolas M M¨ uller, Piotr Kawa, Wei Herng Choong, Edresson Casanova, Eren G¨ olge, Thorsten M¨ uller, Piotr Syga, Philip Sperl, and Konstantin B¨ ottinger, 'Mlaad: The multi-language audio anti-spoofing dataset,' International Joint Conference on Neural Networks (IJCNN) , 2024.
- [21] Jee-weon Jung, Yihan Wu, Xin Wang, Ji-Hoon Kim, Soumi Maiti, Yuta Matsunaga, Hye-jin Shim, Jinchuan Tian, Nicholas Evans, Joon Son Chung, et al., 'Spoofceleb: Speech deepfake detection and sasv in the wild,' IEEE Open Journal of Signal Processing , 2025.
- [22] Joel Frank and Lea Sch¨ onherr, 'Wavefake: A data set to facilitate audio deepfake detection,' arXiv preprint arXiv:2111.02813 , 2021.
- [23] Jiangyan Yi, Chu Yuan Zhang, Jianhua Tao, Chenglong Wang, Xinrui Yan, Yong Ren, Hao Gu, and Junzuo Zhou, 'Add 2023: Towards audio deepfake detection and analysis in the wild,' arXiv preprint arXiv:2408.04967 , 2024.
- [24] Zhihao Du, Qian Chen, Shiliang Zhang, Kai Hu, Heng Lu, Yexin Yang, Hangrui Hu, Siqi Zheng, Yue Gu, Ziyang Ma, et al., 'Cosyvoice: A scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens,' arXiv preprint arXiv:2407.05407 , 2024.
- [25] Zhihao Du, Yuxuan Wang, Qian Chen, Xian Shi, Xiang Lv, Tianyu Zhao, Zhifu Gao, Yexin Yang, Changfeng Gao, Hui Wang, et al., 'Cosyvoice 2: Scalable streaming speech synthesis with large language models,' arXiv preprint arXiv:2412.10117 , 2024.
- [26] Sefik Emre Eskimez, Xiaofei Wang, Manthan Thakker, Canrun Li, Chung-Hsien Tsai, Zhen Xiao, Hemin Yang, Zirun Zhu, Min Tang, Xu Tan, et al., 'E2 tts: Embarrassingly easy fully non-autoregressive zero-shot tts,' in 2024 IEEE Spoken Language Technology Workshop (SLT) .",6. REFERENCES,2,,prose,747,57afeb3b-a839-4b99-a497-51f0fbed5fbf,12113023-ecde-427e-bdc2-b51752b31ab6,2025-12-07T17:14:21.247946
12113023-ecde-427e-bdc2-b51752b31ab6,eacd5119-8805-41cb-bdfc-bbab89efdb71,58,"IEEE, 2024, pp. 682-689.
- [27] Hao-Han Guo, Yao Hu, Kun Liu, Fei-Yu Shen, Xu Tang, Yi-Chen Wu, Feng-Long Xie, Kun Xie, and Kai-Tuo Xu, 'Fireredtts: A foundation text-to-speech framework for industry-level generative speech applications,' arXiv preprint arXiv:2409.03283 , 2024.
- [28] Zhen Ye, Xinfa Zhu, Chi-Min Chan, Xinsheng Wang, Xu Tan, Jiahe Lei, Yi Peng, Haohe Liu, Yizhu Jin, Zheqi DAI, et al., 'Llasa: Scaling train-time and inferencetime compute for llama-based speech synthesis,' arXiv preprint arXiv:2502.04128 , 2025.
- [29] Yuancheng Wang, Haoyue Zhan, Liwei Liu, Ruihong Zeng, Haotian Guo, Jiachen Zheng, Qiang Zhang, Xueyao Zhang, Shunsi Zhang, and Zhizheng Wu, 'Maskgct: Zero-shot text-to-speech with masked generative codec transformer,' arXiv preprint arXiv:2409.00750 , 2024.
- [30] Philip Anastassiou, Jiawei Chen, Jitong Chen, Yuanzhe Chen, Zhuo Chen, Ziyi Chen, Jian Cong, Lelai Deng, Chuang Ding, Lu Gao, et al., 'Seed-tts: A family of high-quality versatile speech generation models,' arXiv preprint arXiv:2406.02430 , 2024.
- [31] Xinsheng Wang, Mingqi Jiang, Ziyang Ma, Ziyu Zhang, Songxiang Liu, Linqin Li, Zheng Liang, Qixi Zheng, Rui Wang, Xiaoqin Feng, et al., 'Spark-tts: An efficient llm-based text-to-speech model with singlestream decoupled speech tokens,' arXiv preprint arXiv:2503.01710 , 2025.
- [32] Hui Wang, Siqi Zheng, Yafeng Chen, Luyao Cheng, and Qian Chen, 'Cam++: A fast and efficient network for speaker verification using context-aware masking,' arXiv preprint arXiv:2303.00332 , 2023.
- [33] Brecht Desplanques, Jenthe Thienpondt, and Kris Demuynck, 'Ecapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification,' arXiv preprint arXiv:2005.07143 , 2020.
- [34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, 'Deep residual learning for image recognition,' in Proceedings of the IEEE conference on computer vision and pattern recognition , 2016, pp. 770-778.
- [35] Shuai Wang, Zhengyang Chen, Bing Han, Hongji Wang, Chengdong Liang, Binbin Zhang, Xu Xiang, Wen Ding, Johan Rohdin, Anna Silnova, et al., 'Advancing speaker embedding learning: Wespeaker toolkit for research and production,' Speech Communication , vol. 162, pp. 103104, 2024.
- [36] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang Chen, Binbin Zhang, Xu Xiang, Yanlei Deng, and Yanmin Qian, 'Wespeaker: A research and production oriented speaker embedding learning toolkit,' in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) .",6. REFERENCES,2,,prose,804,2b5d51c4-0e09-4276-a8d6-a2d86d4032f4,bc95b142-906a-46c2-b725-9e1bf084a5e0,2025-12-07T17:14:21.247953
bc95b142-906a-46c2-b725-9e1bf084a5e0,eacd5119-8805-41cb-bdfc-bbab89efdb71,59,"IEEE, 2023, pp. 1-5.",6. REFERENCES,2,,prose,13,12113023-ecde-427e-bdc2-b51752b31ab6,,2025-12-07T17:14:21.247960
