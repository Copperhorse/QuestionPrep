chunk_id,file_id,chunk_index,content,section_header,section_level,parent_section,top_header,content_type,estimated_tokens,prev_chunk_id,next_chunk_id,created_at,quality_score,eval_content_type,eval_reason,should_use
36d712b3-e625-441b-bece-adf0e855e1d9,05da8492-f54b-4bb9-9d7e-023141262257,0,"## Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup

Luyu Gao 1 , Yunyi Zhang 2 , Jiawei Han 2 , Jamie Callan 1",,,,,prose,38,,a42bd688-f9a9-4930-973a-81b7be0f6922,2025-12-08T02:39:20.019572,75,main_content,OK,True
a42bd688-f9a9-4930-973a-81b7be0f6922,05da8492-f54b-4bb9-9d7e-023141262257,1,"1 Language Technologies Institute, Carnegie Mellon University","1. Language Technologies Institute, Carnegie Mellon University",1,,"1. Language Technologies Institute, Carnegie Mellon University",prose,8,36d712b3-e625-441b-bece-adf0e855e1d9,6c43bc23-92b9-4bfd-a8dc-6f803cb1fba5,2025-12-08T02:39:20.019594,75,main_content,OK,True
6c43bc23-92b9-4bfd-a8dc-6f803cb1fba5,05da8492-f54b-4bb9-9d7e-023141262257,2,"2 Department of Computer Science, University of Illinois Urbana-Champaign","2. Department of Computer Science, University of Illinois Urbana-Champaign",1,,"2. Department of Computer Science, University of Illinois Urbana-Champaign",prose,13,a42bd688-f9a9-4930-973a-81b7be0f6922,15f24d1a-94e6-4e78-b87b-d4a20fbb7ab7,2025-12-08T02:39:20.019624,75,main_content,OK,True
15f24d1a-94e6-4e78-b87b-d4a20fbb7ab7,05da8492-f54b-4bb9-9d7e-023141262257,3,"1 { luyug, callan } @cs.cmu.edu 2 { yzhan238, hanj } @illionis.edu

## Abstract

Contrastive learning has been applied successfully to learn vector representations of text. Previous research demonstrated that learning high-quality representations benefits from batch-wise contrastive loss with a large number of negatives. In practice, the technique of in-batch negative is used, where for each example in a batch, other batch examples' positives will be taken as its negatives, avoiding encoding extra negatives. This, however, still conditions each example's loss on all batch examples and requires fitting the entire large batch into GPU memory. This paper introduces a gradient caching technique that decouples backpropagation between contrastive loss and the encoder, removing encoder backward pass data dependency along the batch dimension. As a result, gradients can be computed for one subset of the batch at a time, leading to almost constant memory usage. 1","1. { luyug, callan } @cs.cmu.edu 2 { yzhan238, hanj } @illionis.edu",1,,"1. { luyug, callan } @cs.cmu.edu 2 { yzhan238, hanj } @illionis.edu",prose,192,6c43bc23-92b9-4bfd-a8dc-6f803cb1fba5,bc43ad2a-d48e-4252-abe6-4cce841c8b0d,2025-12-08T02:39:20.019637,95,main_content,OK,True
bc43ad2a-d48e-4252-abe6-4cce841c8b0d,05da8492-f54b-4bb9-9d7e-023141262257,4,"## 1 Introduction

Contrastive learning learns to encode data into an embedding space such that related data points have closer representations and unrelated ones have further apart ones. Recent works in NLP adopt deep neural nets as encoders and use unsupervised contrastive learning on sentence representation (Giorgi et al., 2020), text retrieval (Lee et al., 2019), and language model pre-training tasks (Wu et al., 2020). Supervised contrastive learning (Khosla et al., 2020) has also been shown effective in training dense retrievers (Karpukhin et al., 2020; Qu et al., 2020). These works typically use batch-wise contrastive loss, sharing target texts as in-batch negatives. With such a technique, previous works have empirically shown that larger batches help learn better representations. However, computing loss and updating model parameters with respect","1. { luyug, callan } @cs.cmu.edu 2 { yzhan238, hanj } @illionis.edu",1,,"1. { luyug, callan } @cs.cmu.edu 2 { yzhan238, hanj } @illionis.edu",prose,188,15f24d1a-94e6-4e78-b87b-d4a20fbb7ab7,804e8dd4-b2ab-443e-9232-1eb4a8210d07,2025-12-08T02:39:20.019647,95,main_content,OK,True
804e8dd4-b2ab-443e-9232-1eb4a8210d07,05da8492-f54b-4bb9-9d7e-023141262257,5,"1 Our code is at github.com/luyug/GradCache .

to a big batch require encoding all batch data and storing all activation, so batch size is limited by total available GPU memory. This limits application and research of contrastive learning methods under memory limited setup, e.g. academia. For example, Lee et al. (2019) pre-train a BERT (Devlin et al., 2019) passage encoder with a batch size of 4096 while a high-end commercial GPU RTX 2080ti can only fit a batch of 8. The gradient accumulation technique, splitting a large batch into chunks and summing gradients across several backwards, cannot emulate a large batch as each smaller chunk has fewer in-batch negatives.",1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,prose,152,bc43ad2a-d48e-4252-abe6-4cce841c8b0d,f8992b82-a6b7-4e4f-bc2b-13cfd7ecf281,2025-12-08T02:39:20.019655,100,main_content,OK,True
f8992b82-a6b7-4e4f-bc2b-13cfd7ecf281,05da8492-f54b-4bb9-9d7e-023141262257,6,"In this paper, we present a simple technique that thresholds peak memory usage for contrastive learning to almost constant regardless of the batch size. For deep contrastive learning, the memory bottlenecks are at the deep neural network based encoder. We observe that we can separate the backpropagation process of contrastive loss into two parts, from loss to representation, and from representation to model parameter, with the latter being independent across batch examples given the former, detailed in subsection 3.2. We then show in subsection 3.3 that by separately pre-computing the representations' gradient and store them in a cache, we can break the update of the encoder into multiple sub-updates that can fit into the GPU memory. This pre-computation of gradients allows our method to produce the exact same gradient update as training with large batch. Experiments show that with about 20% increase in runtime, our technique enables a single consumer-grade GPU to reproduce the state-of-the-art large batch trained models that used to require multiple professional GPUs.",1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,prose,208,804e8dd4-b2ab-443e-9232-1eb4a8210d07,53a4d775-e945-4fc1-a049-f2362da6b141,2025-12-08T02:39:20.019662,90,main_content,OK,True
53a4d775-e945-4fc1-a049-f2362da6b141,05da8492-f54b-4bb9-9d7e-023141262257,7,"## 2 Related Work

Contrastive Learning First introduced for probablistic language modeling (Mnih and Teh, 2012),

Noise Contrastive Estimation (NCE) was later used by Word2Vec (Mikolov et al., 2013) to learn word embedding. Recent works use contrastive learning to unsupervisedly pre-train (Lee et al., 2019; Chang et al., 2020) as well as supervisedly train dense retriever (Karpukhin et al., 2020), where contrastive loss is used to estimate retrieval probability over the entire corpus. Inspired by SimCLR (Chen et al., 2020), constrastive learning is used to learn better sentence representation (Giorgi et al., 2020) and pre-trained language model (Wu et al., 2020).",1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,prose,180,f8992b82-a6b7-4e4f-bc2b-13cfd7ecf281,bde02e0c-20e1-4685-965f-26dff8441951,2025-12-08T02:39:20.019678,100,main_content,OK,True
bde02e0c-20e1-4685-965f-26dff8441951,05da8492-f54b-4bb9-9d7e-023141262257,8,"Deep Network Memory Reduction Many existing techniques deal with large and deep models. The gradient checkpoint method attempts to emulate training deep networks by training shallower layers and connecting them with gradient checkpoints and re-computation (Chen et al., 2016). Some methods also use reversible activation functions, allowing internal activation in the network to be recovered throughout back propagation (Gomez et al., 2017; MacKay et al., 2018). However, their effectiveness as part of contrastive encoders has not been confirmed. Recent work also attempts to remove the redundancy in optimizer tracked parameters on each GPU (Rajbhandari et al., 2020). Compared with the aforementioned methods, our method is designed for scaling over the batch size dimension for contrastive learning.",1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,prose,154,53a4d775-e945-4fc1-a049-f2362da6b141,daa9e924-ac35-499f-b172-0e74341f8173,2025-12-08T02:39:20.019687,100,main_content,OK,True
daa9e924-ac35-499f-b172-0e74341f8173,05da8492-f54b-4bb9-9d7e-023141262257,9,"## 3 Methodologies

In this section, we formally introduce the notations for contrastive loss and analyze the difficulties of using it on limited hardware. We then show how we can use a Gradient Cache technique to factor the loss so that large batch gradient update can be broken into several sub-updates.",1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,prose,61,bde02e0c-20e1-4685-965f-26dff8441951,ec193ae5-c5f7-4b97-a76d-89233cf5d722,2025-12-08T02:39:20.019694,100,main_content,OK,True
ec193ae5-c5f7-4b97-a76d-89233cf5d722,05da8492-f54b-4bb9-9d7e-023141262257,10,"## 3.1 Preliminaries

Under a general formulation, given two classes of data S , T , we want to learn encoders f and g for each such that, given s ∈ S , t ∈ T , encoded representations f ( s ) and g ( t ) are close if related and far apart if not related by some distance measurement. For large S and T and deep neural network based f and g , direct training is not tractable, so a common approach is to use a contrastive loss: sample anchors S ⊂ S and targets T ⊂ T as a training batch, where each element s i ∈ S has a related element t r i ∈ T as well as zero or more specially sampled hard negatives. The rest of the random samples in T will be used as in-batch negatives.",1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,prose,166,daa9e924-ac35-499f-b172-0e74341f8173,4c52522f-86bf-4083-a9fb-d5156b90eb1c,2025-12-08T02:39:20.019701,100,main_content,OK,True
4c52522f-86bf-4083-a9fb-d5156b90eb1c,05da8492-f54b-4bb9-9d7e-023141262257,11,"Define loss based on dot product as follows:

$$\mathcal { L } = - \frac { 1 } { | S | } \sum _ { s _ { i } \in S } \log \frac { e x p ( f ( s _ { i } ) ^ { \top } g ( t _ { r _ { i } } ) / \tau ) } { \sum _ { t _ { j } \in T } e x p ( f ( s _ { i } ) ^ { \top } g ( t _ { j } ) / \tau ) } \\ \intertext { s i g n } \vartheta h o r a $ o h $ o w h o r a $ s u m m o t y $ o n t h o r a $ w h o r a $ o t h o r a $ w h o r a $ s u m m o t y$$",1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,math,196,ec193ae5-c5f7-4b97-a76d-89233cf5d722,9960911f-c119-4861-8472-cd73f8ec0b4c,2025-12-08T02:39:20.019708,80,main_content,OK,True
9960911f-c119-4861-8472-cd73f8ec0b4c,05da8492-f54b-4bb9-9d7e-023141262257,12,"where each summation term depends on the entire set T and requires fitting all of them into memory.

We set temperature τ = 1 in the following discussion for simplicity as in general it only adds a constant multiplier to the gradient.

## 3.2 Analysis of Computation",1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,prose,55,4c52522f-86bf-4083-a9fb-d5156b90eb1c,e86bbc28-b1f5-40e5-be36-92e3416a8054,2025-12-08T02:39:20.019714,95,main_content,OK,True
e86bbc28-b1f5-40e5-be36-92e3416a8054,05da8492-f54b-4bb9-9d7e-023141262257,13,"In this section, we give a mathematical analysis of contrastive loss computation and its gradient. We show that the back propagation process can be divided into two parts, from loss to representation, and from representation to encoder model. The separation then enables us to devise a technique that removes data dependency in encoder parameter update. Suppose the function f is parameterized with Θ and g is parameterized with Λ .",1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,prose,81,9960911f-c119-4861-8472-cd73f8ec0b4c,8f62c2df-128d-4240-8986-1639893e20a6,2025-12-08T02:39:20.019721,100,main_content,OK,True
8f62c2df-128d-4240-8986-1639893e20a6,05da8492-f54b-4bb9-9d7e-023141262257,14,$$\frac { \partial \mathcal { L } } { \partial \Theta } = \sum _ { s _ { i } \in S } \frac { \partial \mathcal { L } } { \partial f ( s _ { i } ) } \frac { \partial f ( s _ { i } ) } { \partial \Theta } \\$$,1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,math,79,e86bbc28-b1f5-40e5-be36-92e3416a8054,1001c700-c4da-45e6-8f8d-0e7ace496576,2025-12-08T02:39:20.019728,80,main_content,OK,True
1001c700-c4da-45e6-8f8d-0e7ace496576,05da8492-f54b-4bb9-9d7e-023141262257,15,$$\frac { \partial \mathcal { L } } { \partial \Lambda } = \sum _ { t _ { j } \in T } \frac { \partial \mathcal { L } } { \partial g ( t _ { j } ) } \frac { \partial g ( t _ { j } ) } { \partial \Lambda }$$,1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,math,78,8f62c2df-128d-4240-8986-1639893e20a6,0cb2a572-fdf9-4720-9835-af8a8f20136d,2025-12-08T02:39:20.019736,80,main_content,OK,True
0cb2a572-fdf9-4720-9835-af8a8f20136d,05da8492-f54b-4bb9-9d7e-023141262257,16,"As an extra notation, denote normalized similarity,

$$p _ { i j } = \frac { e x p ( f ( s _ { i } ) \tau _ { g ( t _ { j } ) } ) } { \sum _ { t \in T } e x p ( f ( s _ { i } ) \tau _ { g ( t ) } ) }$$",1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,math,81,1001c700-c4da-45e6-8f8d-0e7ace496576,15547ddd-f195-4929-aeda-db406375b040,2025-12-08T02:39:20.019742,80,main_content,OK,True
15547ddd-f195-4929-aeda-db406375b040,05da8492-f54b-4bb9-9d7e-023141262257,17,"We note that the summation term for a particular s i or t i is a function of the batch, as,

$$\begin{array} { r l } { \text {for} } & \frac { \partial \mathcal { L } } { \partial f ( s _ { i } ) } = - \frac { 1 } { | S | } \left ( g ( t _ { r _ { i } } ) - \sum _ { t _ { j } \in T } p _ { i j } g ( t _ { j } ) \right ) , \ ( 5 ) } \end{array}$$",1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,math,138,0cb2a572-fdf9-4720-9835-af8a8f20136d,03b1076d-0545-4e6c-9baf-c0d919eb44fb,2025-12-08T02:39:20.019749,80,main_content,OK,True
03b1076d-0545-4e6c-9baf-c0d919eb44fb,05da8492-f54b-4bb9-9d7e-023141262257,18,"$$\text {of} \quad \frac { \partial \mathcal { L } } { \partial g ( t _ { j } ) } = - \frac { 1 } { | S | } \left ( \epsilon _ { j } - \sum _ { s _ { i } \in S } p _ { i j } f ( s _ { i } ) \right ) ,$$",1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,math,87,15547ddd-f195-4929-aeda-db406375b040,4540b091-638c-4f5b-99bd-ba7697cfe43a,2025-12-08T02:39:20.019754,80,main_content,OK,True
4540b091-638c-4f5b-99bd-ba7697cfe43a,05da8492-f54b-4bb9-9d7e-023141262257,19,"where

$$\epsilon _ { j } = \begin{cases} f ( s _ { k } ) & \text {if } \exists \, k \, \text { s.t. } r _ { k } = j \\ 0 & \text {otherwise} \end{cases}$$",1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,math,65,03b1076d-0545-4e6c-9baf-c0d919eb44fb,37aa1fdc-4a86-4308-940d-4374fdfe91e8,2025-12-08T02:39:20.019761,70,main_content,OK,True
37aa1fdc-4a86-4308-940d-4374fdfe91e8,05da8492-f54b-4bb9-9d7e-023141262257,20,"which prohibits the use of gradient accumulation. We make two observations here:

- The partial derivative ∂f ( s i ) ∂ Θ depends only on s i and Θ while ∂g ( t j ) ∂ Λ depends only on t j and Λ ; and
- Computing partial derivatives ∂ L ∂f ( s i ) and ∂ L ∂g ( t j ) requires only encoded representations, but not Θ or Λ .",1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,prose,98,4540b091-638c-4f5b-99bd-ba7697cfe43a,ea3f46b7-6a3b-4ad9-a1ba-f2f117433b0b,2025-12-08T02:39:20.019769,95,main_content,OK,True
ea3f46b7-6a3b-4ad9-a1ba-f2f117433b0b,05da8492-f54b-4bb9-9d7e-023141262257,21,"These observations mean back propagation of f ( s i ) for data s i can be run independently with its own computation graph and activation if the numerical value of the partial derivative ∂ L ∂s i is known. Meanwhile the derivation of ∂ L ∂s i requires only numerical values of two sets of representation vectors F = { f ( s 1 ) , f ( s 2 ) , .., f ( s | S | ) } and G = { g ( t 1 ) , g ( t 2 ) , ..., g ( t | T | ) } . A similar argument holds true for g , where we can use representation vectors to compute ∂ L ∂t j and back propagate for each g ( t j ) independently. In the next section, we will describe how to scale up batch size by precomputing these representation vectors.",1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,prose,177,37aa1fdc-4a86-4308-940d-4374fdfe91e8,7b8497ee-6329-4fbf-9d6f-d2780c8cc543,2025-12-08T02:39:20.019775,95,main_content,OK,True
7b8497ee-6329-4fbf-9d6f-d2780c8cc543,05da8492-f54b-4bb9-9d7e-023141262257,22,"## 3.3 Gradient Cache Technique

Given a large batch that does not fit into the available GPU memory for training, we first divide it into a set of sub-batches each of which can fit into memory for gradient computation, denoted as S = { ˆ S 1 , ˆ S 2 , .. } , T = { ˆ T 1 , ˆ T 2 , .. } . The full-batch gradient update is computed by the following steps.",1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,prose,103,ea3f46b7-6a3b-4ad9-a1ba-f2f117433b0b,05231b5d-c3cd-4154-badb-5898268957af,2025-12-08T02:39:20.019781,95,main_content,OK,True
05231b5d-c3cd-4154-badb-5898268957af,05da8492-f54b-4bb9-9d7e-023141262257,23,"Step1: Graph-less Forward Before gradient computation, we first run an extra encoder forward pass for each batch instance to get its representation. Importantly, this forward pass runs without constructing the computation graph. We collect and store all representations computed.

Step2: Representation Gradient Computation and Caching We then compute the contrastive loss for the batch based on the representation from Step1 and have a corresponding computation graph constructed. Despite the mathematical derivation, automatic differentiation system is used in actual implementation, which automatically supports variations of contrastive loss. A backward pass is then run to populate gradients for each representation. Note that the encoder is not included in this gradient computation. Let u i = ∂ L ∂f ( s i ) and v i = ∂ L ∂g ( t i ) , we take these gradient tensors and store them as a Representation Gradient Cache , [ u 1 , u 2 , .., v 1 , v 2 , .. ] .",1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,prose,196,7b8497ee-6329-4fbf-9d6f-d2780c8cc543,ac32612a-f160-4aaf-92bb-f3134cd852f6,2025-12-08T02:39:20.019788,100,main_content,OK,True
ac32612a-f160-4aaf-92bb-f3134cd852f6,05da8492-f54b-4bb9-9d7e-023141262257,24,"Step3: Sub-batch Gradient Accumulation We run encoder forward one sub-batch at a time to compute representations and build the corresponding computation graph. We take the sub-batch's representation gradients from the cache and run back propagation through the encoder. Gradients are accumulated for encoder parameters across all sub-batches. Effectively for f we have,",1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,prose,70,05231b5d-c3cd-4154-badb-5898268957af,e35e883e-01bf-4211-9098-f841e07fd811,2025-12-08T02:39:20.019794,95,main_content,OK,True
e35e883e-01bf-4211-9098-f841e07fd811,05da8492-f54b-4bb9-9d7e-023141262257,25,$$\frac { \partial \mathcal { L } } { \partial \Theta } & = \sum _ { \hat { S } _ { j } \in \mathbb { S } } \sum _ { s _ { i } \in \hat { S } _ { j } } \frac { \partial \mathcal { L } } { \partial f ( s _ { i } ) } \frac { \partial f ( s _ { i } ) } { \partial \Theta } \\ & = \sum _ { \hat { S } _ { j } \in \mathbb { S } } \sum _ { s _ { i } \in \hat { S } _ { j } } \underline { u } _ { i } \frac { \partial f ( s _ { i } ) } { \partial \Theta } \\$$,1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,math,185,ac32612a-f160-4aaf-92bb-f3134cd852f6,db13b09f-8214-40a7-ae3c-fdf1dc4cb88d,2025-12-08T02:39:20.019800,80,main_content,OK,True
db13b09f-8214-40a7-ae3c-fdf1dc4cb88d,05da8492-f54b-4bb9-9d7e-023141262257,26,"where the outer summation enumerates each subbatch and the entire internal summation corresponds to one step of accumulation. Similarly, for g , gradients accumulate based on,

$$\frac { \partial \mathcal { L } } { \partial \Lambda } = \sum _ { \hat { T } _ { j } \in \mathbb { T } } \sum _ { t _ { i } \in \hat { T } _ { j } } v _ { i } \frac { \partial g ( t _ { i } ) } { \partial \Lambda }$$",1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,math,122,e35e883e-01bf-4211-9098-f841e07fd811,341a8d25-62fe-471f-913a-5873d301e653,2025-12-08T02:39:20.019805,90,main_content,OK,True
341a8d25-62fe-471f-913a-5873d301e653,05da8492-f54b-4bb9-9d7e-023141262257,27,"Here we can see the equivalence with direct large batch update by combining the two summations.

Step4: Optimization When all sub-batches are processed, we can step the optimizer to update model parameters as if the full batch is processed in a single forward-backward pass.",1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,prose,54,db13b09f-8214-40a7-ae3c-fdf1dc4cb88d,d9e195dc-c5b2-4c40-8bf6-d744a0e011c3,2025-12-08T02:39:20.019810,100,main_content,OK,True
d9e195dc-c5b2-4c40-8bf6-d744a0e011c3,05da8492-f54b-4bb9-9d7e-023141262257,28,"Compared to directly updating with the full batch, which requires memory linear to the number of examples, our method fixes the number of examples in each encoder gradient computation to be the size of sub-batch and therefore requires constant memory for encoder forward-backward pass. The extra data pieces introduced by our method that remain persistent across steps are the representations and their corresponding gradients with the former turned into the latter after representation gradient computation. Consequently, in a general case with data from S and T each represented with d dimension vectors, we only need to store ( | S | d + | T | d ) floating points in the cache on top of the computation graph. To remind our readers, this is several orders smaller than million-size model parameters.",1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,prose,147,341a8d25-62fe-471f-913a-5873d301e653,48d4f4e4-5f70-41ef-ad25-21bc8416ca0b,2025-12-08T02:39:20.019816,100,main_content,OK,True
48d4f4e4-5f70-41ef-ad25-21bc8416ca0b,05da8492-f54b-4bb9-9d7e-023141262257,29,"## 3.4 Multi-GPU Training

When training on multiple GPUs, we need to compute the gradients with all examples across all GPUs. This requires a single additional cross GPU communication after Step1 when all representations are computed. We use an all-gather operation to make all representations available on all GPUs. Denote F n , G n representations on n -th GPU and a total of N device. Step2 runs with gathered representations F all = F 1 ∪ .. ∪ F N and G all = G 1 ∪ .. ∪ G N . While F all and G all are used",1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,prose,124,d9e195dc-c5b2-4c40-8bf6-d744a0e011c3,a5cf68a1-6d18-4ba2-b24c-8c0a4c13aaee,2025-12-08T02:39:20.019821,95,main_content,OK,True
a5cf68a1-6d18-4ba2-b24c-8c0a4c13aaee,05da8492-f54b-4bb9-9d7e-023141262257,30,"Table 1: Retrieval: We compare top-5/20/100 hit accuracy of small batch update (Sequential), accumulated small batch (Accumulation) and gradient cache (Cache) systems with DPR reference.

| Method       | Top-5   |   Top-20 |   Top-100 |
|--------------|---------|----------|-----------|
| DPR          | -       |     78.4 |      85.4 |
| Sequential   | 59.3    |     71.9 |      80.9 |
| Accumulation | 64.3    |     77.2 |      84.9 |
| Cache        | 68.6    |     79.3 |      86   |
| - BSZ = 512  | 68.3    |     79.9 |      86.6 |",1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,prose,183,48d4f4e4-5f70-41ef-ad25-21bc8416ca0b,52b94ab1-2f67-4062-81b0-33d75efea323,2025-12-08T02:39:20.019828,80,main_content,OK,True
52b94ab1-2f67-4062-81b0-33d75efea323,05da8492-f54b-4bb9-9d7e-023141262257,31,"to compute loss, the n -th GPU only computes gradient of its local representations F n , G n and stores them into cache. No communication happens in Step3 , when each GPU independently computes gradient for local representations. Step4 will then perform gradient reduction across GPUs as with standard parallel training.",1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,prose,59,a5cf68a1-6d18-4ba2-b24c-8c0a4c13aaee,0e41a9de-9df5-426f-8e04-8a8c13215030,2025-12-08T02:39:20.019834,100,main_content,OK,True
0e41a9de-9df5-426f-8e04-8a8c13215030,05da8492-f54b-4bb9-9d7e-023141262257,32,"## 4 Experiments

To examine the reliability and computation cost of our method, we implement our method into dense passage retriever (DPR; Karpukhin et al. (2020)) 2 . We use gradient cache to compute DPR's supervised contrastive loss on a single GPU. Following DPRpaper, we measure top hit accuracy on the Natural Question Dataset (Kwiatkowski et al., 2019) for different methods. We then examine the training speed of various batch sizes.",1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,prose,102,52b94ab1-2f67-4062-81b0-33d75efea323,89702ad2-ab91-4d03-be19-660293c45452,2025-12-08T02:39:20.019839,100,main_content,OK,True
89702ad2-ab91-4d03-be19-660293c45452,05da8492-f54b-4bb9-9d7e-023141262257,33,"## 4.1 Retrieval Accuracy

Compared Systems 1) DPR : the reference number taken from the original paper trained on 8 GPUs, 2) Sequential : update with max batch size that fits into 1 GPU, 3) Accumulation : similar to Sequential but accumulate gradients and update until number of examples matches DPR setup, 4) Cache : training with DPR setup using our gradient cache on 1 GPU. We attempted to run with gradient checkpointing but found it cannot scale to standard DPR batch size on our hardware.",1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,prose,110,0e41a9de-9df5-426f-8e04-8a8c13215030,eac622fe-4315-4f92-bdcf-643399e128ad,2025-12-08T02:39:20.019845,100,main_content,OK,True
eac622fe-4315-4f92-bdcf-643399e128ad,05da8492-f54b-4bb9-9d7e-023141262257,34,Implementations All runs start with the same random seed and follow DPR training hyperparameters except batch size. Cache uses a batch size of 128 same as DPR and runs with a sub-batch size of 16 for questions and 8 for passages. We also run Cache with a batch size of 512 (BSZ=512) to,1. Our code is at github.com/luyug/GradCache .,1,,1. Our code is at github.com/luyug/GradCache .,prose,68,89702ad2-ab91-4d03-be19-660293c45452,08a8356e-b686-43e6-841d-99d69ca9bf26,2025-12-08T02:39:20.019850,95,main_content,OK,True
08a8356e-b686-43e6-841d-99d69ca9bf26,05da8492-f54b-4bb9-9d7e-023141262257,35,"2 Our implementation is at: https://github.com/ luyug/GC-DPR

Figure 1: We compare training speed versus the number of examples per update for gradient cache (Cache) and gradient accumulation (Accumulation).

<!-- image -->",2. Our implementation is at: https://github.com/ luyug/GC-DPR,1,,2. Our implementation is at: https://github.com/ luyug/GC-DPR,prose,51,eac622fe-4315-4f92-bdcf-643399e128ad,9f7d67d2-439c-4599-a8d9-3d9a32e97644,2025-12-08T02:39:20.019857,0,metadata,Contains 'https://',False
9f7d67d2-439c-4599-a8d9-3d9a32e97644,05da8492-f54b-4bb9-9d7e-023141262257,36,"examine the behavior of even larger batches. Sequential uses a batch size of 8, the largest that fits into memory. Accumulation will accumulate 16 of size-8 batches. Each question is paired with a positive and a BM25 negative passage. All experiments use a single RTX 2080ti.",2. Our implementation is at: https://github.com/ luyug/GC-DPR,1,,2. Our implementation is at: https://github.com/ luyug/GC-DPR,prose,63,08a8356e-b686-43e6-841d-99d69ca9bf26,92fca3ee-928e-4e07-a253-ec5a26fdf5f4,2025-12-08T02:39:20.019867,100,main_content,OK,True
92fca3ee-928e-4e07-a253-ec5a26fdf5f4,05da8492-f54b-4bb9-9d7e-023141262257,37,"Results Accuracy results are shown in Table 1. We observe that Cache performs better than DPR reference due to randomness in training. Further increasing batch size to 512 can bring in some advantage at top 20/100. Accumulation and Sequential results confirm the importance of a bigger batch and more negatives. For Accumulation which tries to match the batch size but has fewer negatives, we see a drop in performance which is larger towards the top. In the sequential case, a smaller batch incurs higher variance, and the performance further drops. In summary, our Cache method improves over standard methods and matches the performance of large batch training.",2. Our implementation is at: https://github.com/ luyug/GC-DPR,1,,2. Our implementation is at: https://github.com/ luyug/GC-DPR,prose,126,9f7d67d2-439c-4599-a8d9-3d9a32e97644,0eb73ff3-5722-4f6c-b872-9796a8cf75fd,2025-12-08T02:39:20.019875,100,main_content,OK,True
0eb73ff3-5722-4f6c-b872-9796a8cf75fd,05da8492-f54b-4bb9-9d7e-023141262257,38,"## 4.2 Training Speed

In Figure 1, we compare update speed of gradient cache and accumulation with per update example number of { 64 , 128 , 256 , 512 , 1024 , 2048 , 4096 } . We observe gradient cache method can steadily scale up to larger batch update and uses 20% more time for representation pre-computation. This extra cost enables it to create an update of a much larger batch critical for the best performance, as shown by previous experiments and many early works. While the original DPR reports a training time of roughly one day on 8 V100 GPUs, in practice, with improved data loading, our gradient cache code can train a dense retriever in a practical 31 hours on a single RTX2080ti. We also find gradient checkpoint only runs up to batch of 64 and consumes twice the amount of time than accumulation 3 .",2. Our implementation is at: https://github.com/ luyug/GC-DPR,1,,2. Our implementation is at: https://github.com/ luyug/GC-DPR,prose,187,92fca3ee-928e-4e07-a253-ec5a26fdf5f4,23d56e12-98e1-4173-a271-99b2905db30f,2025-12-08T02:39:20.019884,100,main_content,OK,True
23d56e12-98e1-4173-a271-99b2905db30f,05da8492-f54b-4bb9-9d7e-023141262257,39,"## 5 Extend to Deep Distance Function

Previous discussion assumes a simple parameterless dot product similarity. In general it can also be deep distance function Φ richly parameterized by Ω , formally,

$$d _ { i j } = d ( s _ { i } , t _ { j } ) = \Phi ( f ( s _ { i } ) , g ( t _ { j } ) ) \quad ( 1 0 ) \quad \mod { 1 } _ { i }$$",2. Our implementation is at: https://github.com/ luyug/GC-DPR,1,,2. Our implementation is at: https://github.com/ luyug/GC-DPR,math,106,0eb73ff3-5722-4f6c-b872-9796a8cf75fd,9b957353-c888-4be2-8b33-762de83f82e1,2025-12-08T02:39:20.019895,90,main_content,OK,True
9b957353-c888-4be2-8b33-762de83f82e1,05da8492-f54b-4bb9-9d7e-023141262257,40,"This can still scale by introducing an extra Distance Gradient Cache . In the first forward we collect all representations as well as all distances. We compute loss with d ij s and back propagate to get w ij = ∂ L ∂d ij , and store them in Distance Gradient Cache, [ w 00 , w 01 , .., w 10 , .. ] . We can then update Ω in a sub-batch manner,",2. Our implementation is at: https://github.com/ luyug/GC-DPR,1,,2. Our implementation is at: https://github.com/ luyug/GC-DPR,prose,88,23d56e12-98e1-4173-a271-99b2905db30f,6c86ad92-eb42-485e-809e-f19e57cd49d2,2025-12-08T02:39:20.019906,95,main_content,OK,True
6c86ad92-eb42-485e-809e-f19e57cd49d2,05da8492-f54b-4bb9-9d7e-023141262257,41,"$$\frac { \partial \mathcal { L } } { \partial \Omega } = \sum _ { \hat { S } \in S \hat { T } \in \mathbb { T } } \sum _ { s _ { i } \in \hat { S } } \sum _ { t _ { j } \in \hat { T } } w _ { i j } \frac { \partial \Phi ( f ( s _ { i } ) , g _ { i } ( t _ { j } ) ) } { \partial \Omega } \\ \intertext { a d d i t i o n d l y w i l l }$$",2. Our implementation is at: https://github.com/ luyug/GC-DPR,1,,2. Our implementation is at: https://github.com/ luyug/GC-DPR,math,145,9b957353-c888-4be2-8b33-762de83f82e1,eea6fa94-b05a-4edc-aff8-96bdffc33279,2025-12-08T02:39:20.019912,80,main_content,OK,True
eea6fa94-b05a-4edc-aff8-96bdffc33279,05da8492-f54b-4bb9-9d7e-023141262257,42,"Additionally, we simultaneously compute with the constructed computation graph ∂d ij ∂f ( s i ) and ∂d ij ∂g ( t j ) and accumulate across batches,

$$u _ { i } = \frac { \partial \mathcal { L } } { \partial f ( s _ { i } ) } = \sum _ { j } w _ { i j } \frac { \partial d _ { i j } } { \partial f ( s _ { i } ) }$$",2. Our implementation is at: https://github.com/ luyug/GC-DPR,1,,2. Our implementation is at: https://github.com/ luyug/GC-DPR,math,107,6c86ad92-eb42-485e-809e-f19e57cd49d2,74d3010f-12a0-4c83-a019-cce21332b036,2025-12-08T02:39:20.019916,90,main_content,OK,True
74d3010f-12a0-4c83-a019-cce21332b036,05da8492-f54b-4bb9-9d7e-023141262257,43,"and,

$$v _ { j } = \frac { \partial \mathcal { L } } { \partial g ( t _ { j } ) } = \sum _ { i } w _ { i j } \frac { \partial d _ { i j } } { \partial g ( t _ { j } ) }$$",2. Our implementation is at: https://github.com/ luyug/GC-DPR,1,,2. Our implementation is at: https://github.com/ luyug/GC-DPR,math,71,eea6fa94-b05a-4edc-aff8-96bdffc33279,653ff633-6afa-4eaf-94e2-40e4c78bb6dc,2025-12-08T02:39:20.019919,70,main_content,OK,True
653ff633-6afa-4eaf-94e2-40e4c78bb6dc,05da8492-f54b-4bb9-9d7e-023141262257,44,"with which we can build up the Representation Gradient Cache. When all representations' gradients are computed and stored, encoder gradient can be computed with Step3 described in subsection 3.3. In philosophy this method links up two caches. Note this covers early interaction f ( s ) = s, g ( t ) = t as a special case.",2. Our implementation is at: https://github.com/ luyug/GC-DPR,1,,2. Our implementation is at: https://github.com/ luyug/GC-DPR,prose,69,74d3010f-12a0-4c83-a019-cce21332b036,decfdf3a-98f6-4c1a-9914-02977149bc40,2025-12-08T02:39:20.019923,100,main_content,OK,True
decfdf3a-98f6-4c1a-9914-02977149bc40,05da8492-f54b-4bb9-9d7e-023141262257,45,"## 6 Conclusion

In this paper, we introduce a gradient cache technique that breaks GPU memory limitations for large batch contrastive learning. We propose to construct a representation gradient cache that removes in-batch data dependency in encoder optimization. Our method produces the exact same gradient update as training with a large batch. We show the",2. Our implementation is at: https://github.com/ luyug/GC-DPR,1,,2. Our implementation is at: https://github.com/ luyug/GC-DPR,prose,64,653ff633-6afa-4eaf-94e2-40e4c78bb6dc,1793d18f-4cf2-4d44-a725-de3807466bc8,2025-12-08T02:39:20.019927,95,main_content,OK,True
1793d18f-4cf2-4d44-a725-de3807466bc8,05da8492-f54b-4bb9-9d7e-023141262257,46,"3 We used the gradient checkpoint implemented in Huggingface transformers package

method is efficient and capable of preserving accuracy on resource-limited hardware. We believe a critical contribution of our work is providing a large population in the NLP community with access to batch-wise contrastive learning. While many previous works come from people with industry-grade hardware, researchers with limited hardware can now use our technique to reproduce state-of-the-art models and further advance the research without being constrained by available GPU memory.",3. We used the gradient checkpoint implemented in Huggingface transformers package,1,,3. We used the gradient checkpoint implemented in Huggingface transformers package,prose,96,decfdf3a-98f6-4c1a-9914-02977149bc40,6e36ae04-3aad-430b-8e17-519268c5802a,2025-12-08T02:39:20.019930,100,main_content,OK,True
6e36ae04-3aad-430b-8e17-519268c5802a,05da8492-f54b-4bb9-9d7e-023141262257,47,"## Acknowledgments

The authors would like to thank Zhuyun Dai and Chenyan Xiong for comments on the paper, and the anonymous reviewers for their reviews.

## References

- Wei-Cheng Chang, Felix X. Yu, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. 2020. Pre-training tasks for embedding-based large-scale retrieval. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net.
- T. Chen, B. Xu, C. Zhang, and Carlos Guestrin. 2016. Training deep nets with sublinear memory cost. ArXiv , abs/1604.06174.
- Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020. A simple framework for contrastive learning of visual representations. ArXiv , abs/2002.05709.
- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.
- John Michael Giorgi, Osvald Nitski, Gary D Bader, and Bo Wang. 2020. Declutr: Deep contrastive learning for unsupervised textual representations. ArXiv , abs/2006.03659.
- Aidan N. Gomez, Mengye Ren, R. Urtasun, and Roger B. Grosse. 2017. The reversible residual network: Backpropagation without storing activations. In NIPS .
- Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 67696781, Online. Association for Computational Linguistics.
- Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. 2020.",3. We used the gradient checkpoint implemented in Huggingface transformers package,1,,3. We used the gradient checkpoint implemented in Huggingface transformers package,prose,536,1793d18f-4cf2-4d44-a725-de3807466bc8,16ea19c6-ad89-42c5-b651-414391fd5913,2025-12-08T02:39:20.019934,90,main_content,OK,True
16ea19c6-ad89-42c5-b651-414391fd5913,05da8492-f54b-4bb9-9d7e-023141262257,48,"Supervised contrastive learning. arXiv preprint arXiv:2004.11362 .
- T. Kwiatkowski, J. Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, C. Alberti, D. Epstein, Illia Polosukhin, J. Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Q. Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics , 7:453466.
- Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the
- 57th Annual Meeting of the Association for Computational Linguistics , pages 6086-6096, Florence, Italy. Association for Computational Linguistics.
- Matthew MacKay, Paul Vicol, Jimmy Ba, and Roger B. Grosse. 2018. Reversible recurrent neural networks. In NeurIPS .
- Tomas Mikolov, Ilya Sutskever, Kai Chen, G. Corrado, and J. Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS .
- A. Mnih and Y. Teh. 2012. A fast and simple algorithm for training neural probabilistic language models. In ICML .
- Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2020. Rocketqa: An optimized training approach to dense passage retrieval for opendomain question answering.
- Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models.
- Z. Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun, and Hao Ma. 2020. Clear: Contrastive learning for sentence representation. ArXiv , abs/2012.15466.",3. We used the gradient checkpoint implemented in Huggingface transformers package,1,,3. We used the gradient checkpoint implemented in Huggingface transformers package,prose,477,6e36ae04-3aad-430b-8e17-519268c5802a,,2025-12-08T02:39:20.019937,90,main_content,OK,True
